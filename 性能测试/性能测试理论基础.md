# 性能测试理论基础



## 什么样的系统需要做性能测试



- 用户量大，PV比较高的系统
- 系统核心模块/接口
- 业务逻辑 / 算法比较复杂
- 促销 / 活动推广计划
- 新技术选型



## 性能测试分类

![image-20260108131336152](D:\git_repository\cyber_security_learning\markdown_img\image-20260108131336152.png)



客户端性能：测试APP自身性能，如CPU、内存消耗；web页面元素渲染的速度

服务端性能：测试服务端项目程序的支持并发、处理能力、响应时间等，主要通过接口来做性能测试；目前服务端的性能测试是主流，一般说到的性能测试，都是指的服务端性能测试。客户端相对较少一些。



## 性能测试指标

![image-20260108131900794](D:\git_repository\cyber_security_learning\markdown_img\image-20260108131900794.png)

### 性能测试指标 — 并发

#### 并发（Concurrency）在性能测试里的**严格含义**

> **并发 = 在同一时间窗口内，系统中“同时处于未完成状态的请求/任务数量”**

注意三个关键词：

1. **同一时间窗口**
2. **同时存在**
3. **尚未完成**



##### 并发 ≠ QPS ≠ TPS ≠ 活跃用户

| 概念      | 本质                             |
| --------- | -------------------------------- |
| QPS / TPS | 单位时间内完成的请求数（吞吐量） |
| 并发数    | 某一时刻正在处理中的请求数量     |
| 活跃用户  | 有行为的用户（不等于同时发请求） |



##### 一个最容易理解的例子

- 每个请求平均耗时 **100ms**
- 系统 QPS = **100**

那么并发是多少？

> **并发 ≈ QPS × 平均响应时间**

```
并发 ≈ 100 × 0.1 = 10
```

**只有 10 个请求“同时在系统里”**



#### 并发的本质：不是“人多”，而是“请求重叠”

很多人误以为：

> “1000 个用户在线 = 1000 并发”

这是**错误的**。



**正确理解方式：**

> **并发取决于：请求到达速率 × 请求在系统中停留的时间**



这在排队论中叫：

> **Little’s Law（小法则）**

```
L = λ × W
```

| 符号 | 含义                   |
| ---- | ---------------------- |
| L    | 并发数（系统中请求数） |
| λ    | 到达率（QPS）          |
| W    | 平均响应时间           |



### 性能测试指标 — TPS/QPS

Transaction Pre Second，每秒钟处理的事务数

#### Transaction 在性能测试中到底是什么意思？

**原始定义（工具 / 理论层面）**

> **Transaction = 一个“有业务语义的完整操作”**

而不是：

> “随便一次 HTTP 请求”



**在不同场景下，Transaction 含义不同**

1️⃣ 最简单场景（你描述的情况）

- 一个 HTTP 接口
- 无依赖
- 无下游
- 无事务

✅ 此时：

```
1 Transaction ≈ 1 接口调用
TPS ≈ 接口 QPS
```



2️⃣ 真实生产环境（常态）

一个 Transaction 可能包含：

- 多个接口
- 多次 DB 操作
- RPC / MQ / Cache
- 事务提交

例如：

> “下单”事务：

```
下单接口
 ├── 库存校验
 ├── 扣库存
 ├── 写订单表
 ├── 写流水
 ├── 发 MQ
```

📌 这是 **1 个 Transaction，但 ≠ 1 个接口**

​    

#### 为什么“TPS 越高 = 性能越好”是错误的？

##### ❌ 错误点 1：TPS 不考虑延迟

举例：

| 系统 | TPS  | P99 延迟 |
| ---- | ---- | -------- |
| A    | 2000 | 5s       |
| B    | 800  | 100ms    |

请问哪个性能好？

**B 明显更好**

> **吞吐高但慢 = 拥堵系统**



##### ❌ 错误点 2：TPS 可以靠“牺牲正确性”换来

高 TPS 可能来自：

- 关闭校验
- 异步丢数据
- 降低一致性
- 超时直接返回成功

这不是“性能好”，这是“质量下降”。



##### ❌ 错误点 3：TPS 是负载条件下的结果，不是能力本身

TPS 永远依赖于：

```
并发 × 响应时间 × 资源
```

你可以：

- 提高并发 → TPS 上升（但延迟暴涨）
- 放宽 SLA → TPS 上升



#### TPS、QPS、并发的正确关系

```nginx
TPS / QPS = 并发 / 平均响应时间
```


所以：

- TPS 高，可能是：

  - 并发高

  - 响应慢

- TPS 低，可能是：

  - 并发低

  - 但系统很快

> TPS 只是“结果指标”，不是质量指标



#### 什么情况下“TPS 高”才有意义？

必须同时满足：

1. **延迟在 SLA 内**
2. **错误率可接受**
3. **资源使用合理**
4. **数据正确性不被破坏**

此时才能说：

> **在该 SLA 下，系统吞吐能力强**



