1. 说说Nginx和apache有什么区别

```bat
# 架构设计不同
Nginx
事件驱动架构：
使用异步非阻塞 I/O 和事件循环，单线程可以处理成千上万的连接。
每个工作进程使用事件模型处理多个请求，资源占用低。

Apache
进程/线程驱动架构：
默认使用多进程模型（prefork 模式），每个请求由一个独立的进程处理。
也支持多线程模式（worker 模式）或事件模式（event 模式），但线程和进程间的切换开销较高。

# 内存与资源占用
Nginx
内存占用低：
单线程模型更节省内存，适合大规模并发场景。
适合高负载环境。

Apache
内存占用高：
每个连接需要一个独立的线程/进程，资源开销大。
适合小流量环境。

# 负载均衡和反向代理
Nginx
内置强大的负载均衡和反向代理功能，支持多种均衡算法（如轮询、最小连接）。
直接支持健康检查、请求缓存等高级功能。

Apache
虽然可以通过模块（如 mod_proxy、mod_lb）实现负载均衡和反向代理，但配置复杂，性能不如 Nginx。

# HTTPS 和 HTTP/2
Nginx
原生支持 HTTP/2 和 SSL/TLS，性能优化较好。
配置简单，广泛用于 HTTPS 网站。
Apache
支持 HTTP/2 和 SSL/TLS，但需要加载额外模块（如 mod_ssl）。
性能不如 Nginx。
```







2. 当有攻击者使用未注册的域名访问服务器时，默认的 Nginx 配置可能会处理这些请求。如何解决？

```nginx
server {
    listen 80 default_server;
    server_name _;
    location / {
        root /var/www/error_pages;
        index 404.html;
    }
}
```





3. 什么是正向代理和反向代理？

```bat
# 正向代理
正向代理是客户端的代理，位于客户端和服务器之间，帮助客户端向服务器发出请求。服务器并不知道客户端的真实身份，只知道请求是来自代理服务器。

# 反向代理
反向代理是服务器的代理，位于客户端和服务器之间，帮助服务器接收客户端的请求并转发给内部的真实服务器。客户端并不知道真实服务器的存在，只与反向代理服务器通信。
```







4. Nginx负载均衡的算法怎么实现的?策略有哪些?

**轮询与加权轮询**

```bash
#round-robin
upstream rrups {
    server 127.0.0.1:8011;
    server 127.0.0.1:8012;
    keepalive 32;
}

#加权round-robin
upstream rrups {
    server 127.0.0.1:8011 weight=2 max_conns=2 max_fails=2 fail_timeout=5;
    server 127.0.0.1:8012;
    keepalive 32;
}
```



**源地址hash**

```bash
#源ip地址hash
#ip_hash算法只使用ipv4的前24位做hash运算，如果客户端前24位一致，则会被调度 到同一台后端服务器
# Proxy_Server 配置
upstream group1 {
    ip_hash;         # 只要加上ip_hash即可
    server 10.0.0.210;
    server 10.0.0.159;
}
```



**使用自行指定的key做hash调度**

```bash
#使用自行指定的key做hash调度
# 三台server权重一样，调度算法hash($remote_addr)%3,值为0,1,2根据不同的值调度到不同server
upstream group1 {
    hash $remote_addr;
    server 10.0.0.210;
    server 10.0.0.159;
    server 10.0.0.213;
}

# 三台server权重不一样，调度算法hash($remote_addr)%(1+2+3),值为0,1,2,3,4,5
# 0 调度到210
# 1，2调度到159
# 3,4,5调度到213
upstream group1 {
    hash $remote_addr;
    server 10.0.0.210 weight=1;
    server 10.0.0.159 weight=2;
    server 10.0.0.213 weight=3;
}

# 也可以根据request_uri进行调度，不同客户端访问同一个资源会被调度到同一个后端服务器上
upstream group1 {
    hash $request_uri
    server 10.0.0.210;
    server 10.0.0.159;
    server 10.0.0.213;
}
```



**最少连接调度算法**

```bash
# 根据哪台服务器上的连接数少，就调度到哪里，如果多台服务器连接数一致，则这几台服务器按照轮询的方式进行调度
upstream group1 {
    least_conn;
    server 10.0.0.210;
    server 10.0.0.159;
}
```



**一致性hash**

```bash
upstream group1 {
    hash $remote_addr consistent;
    server 10.0.0.210;
    server 10.0.0.159;
    server 10.0.0.213;
}
```









5. 你对nginx做过哪些优化

**根据CPU核数使用进程数**

```bash
worker_processes auto
```



**在主跑nginx的服务器上将worker进程优先级调高**

```bash
# -10起步，如果基本只跑nginx，可以调为-5
worker_priority -10
```



**调整 worker_rlimit_nofile** 

```bash
#nginx启动后，每个worker能够使用的文件描述符上限
#但因为文件描述符（FD）不仅用于连接，在 Nginx worker 进程里，FD 消耗来源主要有：
#- 网络连接（socket），每个客户端连接占用 1 个 FD（有时长连接、反向代理还会占更多 FD）。
#- 日志文件（access.log、error.log），每个打开的日志文件占用 1 个 FD。
#- 静态文件（HTML、图片等），传输过程中会打开文件 FD。
#- 反向代理/上游连接，如果启用了 upstream，一个请求可能占 2 个 FD（客户端 + upstream 服务器）。
#- 临时文件（大文件缓存、proxy_temp 等），占用临时文件 FD。

# 因此 worker_rlimit_nofile 必须大于 worker_connection

# 该数据受内核参数`ulimit -n`数量限制，来需要该外面的其他设置
# 如果是nginx直接启动，更改/etc/security/limits.conf 
# 更改limits.conf
# 添加 * hard nofile 10000
# * soft nofile 10000
# root hard nofile 10000
# root soft nofile 10000

# 如果是systemctl启动，则更改service文件中的参数
# LimitNOFILE=100000

# 百万并发建议
worker_rlimit_nofile 3200000

```



**通过亲源机制，把CPU和进程进行绑定**

```bash
# worker_cpu_affinity 不支持 auto，需要手动配置绑定掩码 
worker_cpu_affinity 00000001 00000010 00000100 00001000

# 通过脚本生成 worker_cpu_affinity 配置，实现细粒度的 CPU 绑定
CORES=$(nproc) # CPU 核数

for ((i=0; i<$CORES; i++)); do
    # 生成二进制掩码（1 左移 i 位）
    MASK=$(printf "%0${CORES}d" "$(echo "obase=2; $((1 << i))" | bc)")
    echo -n "$MASK "
done
```



**调整 worker_connection**

```bash
worker_connection 65535 # 设置单个工作进程的最大并发连接数,默认512，建议加大，否则，只能接受最大指定连接数，超出则会丢弃请求，不会处理。
```



**开启防惊群**

```bash
accept mutex on    # 惊群，推荐on，on为同一时刻一个请求轮流由worker进程处理，而防治被同时唤醒所有worker
                   # 默认为off，新请求会唤醒所有的worker进程
```



**开启多路复用**

```bash
multi_accept on  # on时Nginx服务器的每个工作进程可以同时接受多个新的网络连接，此指令默认为off，即默认为一个工作进程只能一次接受一个新的网络连接，打开后可以同时接受多个，建议设置on，充分利用多路复用的理念
```



**优化多路复用开启，导致连接在各 worker 间分布不均**

```bash
#用 SO_REUSEPORT 让每个 worker 拥有各自独立的监听队列，由内核做均衡分发，天然避免一个 worker 吸走所有连接。
#做法（Linux ≥3.9，Nginx ≥1.9.1）
# 每个 worker 拥有独立的 listen socket 与 backlog
worker_processes auto;

http {
  server {
    listen 80 reuseport;   # 关键
    # listen [::]:80 reuseport;  # IPv6 也要配
  }
}
```



**启用后端异步更新缓存**

当 Nginx 发现缓存过期时，不会立刻等待后端返回新的数据，而是先返回旧缓存给客户端，同时在后台向后端发起请求更新缓存**。这样可以**减少对后端服务器的压力，提高 Nginx 吞吐量

```bash
proxy_cache_use_stale updating error timeout http_500 http_502 http_503 http_504;
proxy_cache_background_update on;
```

1. 客户端请求到达。
2. **Nginx 发现缓存过期**，但**仍然返回旧的缓存数据**（减少客户端延迟）。
3. **同时在后台向后端请求最新数据**，更新缓存。
4. **下次有新的客户端请求时，Nginx 直接使用最新缓存**。



**`proxy_cache_use_stale` 参数详解**

| 参数                        | 作用                                   |
| --------------------------- | -------------------------------------- |
| **`updating`**              | 旧缓存过期时，仍然返回它，后台异步更新 |
| **`error`**                 | 如果后端请求失败，仍然返回旧缓存       |
| **`timeout`**               | 如果后端超时，仍然返回旧缓存           |
| **`http_500` ~ `http_504`** | 如果后端返回 5xx 错误，仍然返回旧缓存  |



**对上游服务使用keepalive长连接**

通过复用连接，降低nginx与上游服务建立，关闭连接的消耗，提升吞吐量的同时降低时延

```bash
upstream rrups {
    server 127.0.0.1:8011 weight=2 max_conns=2 max_fails=2 fail_timeout=5;
    server 127.0.0.1:8012;
    keepalive 512;
    keepalive_requests 10000;
    keepalive_time  30m;   # 限制连接寿命，避免“永久脏连接”
}

server {
    server_name rrups.feng.tech;
    error_log myerror.log info;

    location / {
        proxy_pass http://rrups;
        proxy_http_version 1.1;
        proxy_set_header Connection "";
    }
}
```



**开启 gzip 压缩**

```bash
gzip on    # 确保cpu资源富裕
```



**三次握手中的参数优化**

**SYN_SENT状态（nginx作为反向代理，对于上游是客户端）**

```bash
net.ipv4.tcp_syn_retries=6               # 主动建立连接时，发SYN的重试次数
```

> `net.ipv4.tcp_syn_retries` 决定 **主动发起连接（connect()）时**，内核在**没有收到 SYN+ACK** 时，SYN 报文会重试的次数。
>
> 默认值在很多 Linux 发行版上是 **6**，这意味着：
>
> - 第一次发 SYN，没收到回应 → 等一段时间再发一次；
> - 这个时间是**指数退避**（RTO 倍增），总共发 **1 次初始 SYN + 6 次重试**；
> - 每次等待时间大致按 RFC6298 算，6 次下来大约 **63 秒左右才放弃连接**（不同内核略有差异）。
>
> 这个逻辑在 Nginx 场景里对应：
>
> - Nginx 作为**反向代理**或**主动健康检查**时，worker 进程调用 `connect()` 到上游（upstream）；
> - 如果上游 IP 可达但端口未响应、链路丢包、或者 SYN 被防火墙丢弃 → 就会触发内核的 SYN 重试机制。
>
> 在**高并发、低延迟要求**的场景下：
>
> - **默认 6 次重试（>60 秒）太长**，会让“已不可用”的上游占着连接资源很久；
> - 对反代来说，这种连接会卡住一个 worker 的上游连接槽位（占资源）、延迟响应；
> - 特别是在**健康检查**或**短超时 API**里，长时间等 SYN 超时毫无意义，因为：
>   - 业务逻辑已经要求 1~3 秒内返回失败；
>   - 上游挂了就应该尽快 failover 或走备机；
>   - 长重试反而浪费连接、拖慢回退机制。
>
> 所以优化建议之一是：
>
> - **降低 `tcp_syn_retries` 到 2~3**（总超时 3~7 秒左右），让 connect 尽快失败；
> - 这样 Nginx 可以更快把请求切换到下一个可用 upstream



```bash
net.ipv4.ip_local_range=32768 60999      # 建立连接时的本地端口可用范围
```

> 对于反向代理的场景，nginx的上游ip和Port是确定的，本地IP也是确定的，因此端口范围就成了并发连接的瓶颈。
>
> 因此在高并发场景下，端口范围要放大。



**主动建立连接时应用层超时时间**

```nginx
Syntax: proxy_connect_timeout time;
Default: proxy_connect_timeout 60s;
Context: http,server,location
```

```nginx
Syntax: proxy_connect_timeout time;
Default: proxy_connect_timeout 60s;
Context: stream,server
```

> 优化原因和上面一样，**可以概括为**：**加快失败检测，释放资源，提升故障切换速度**。



**优先级**

如果 **`proxy_connect_timeout` < 内核 SYN 超时** → Nginx 提前放弃连接（Nginx 优先）。

如果 **`proxy_connect_timeout` ≥ 内核 SYN 超时** → 等内核超时，Nginx 才得到失败结果（内核优先）。



**SYN_RCVD状态（服务器端）**

```nginx
net.ipv4.tcp_max_syn_backlog                 # SYN_REVD状态连接的最大个数
```

```nginx
net.ipv4.tcp_synack_retries                  # 被动建立连接时，发SYN/ACK的重试次数
```



**基于三次握手在生产过程中的优化配置**

```bash
# 半连接重试次数
net.ipv4.tcp_synack_retries = 2        # 2 次，大约 6 秒左右释放

# 打开 SYN cookies（队列满时抗 flood）
net.ipv4.tcp_syncookies = 1

# 增大半连接队列长度
net.ipv4.tcp_max_syn_backlog = 262144

# 提高已完成连接的 accept 队列长度
net.core.somaxconn = 65535
```

**nginx端**

```bash
listen 443 reuseport backlog=65535
```





**高并发场景下优化**

**开启TCP keepalive**

**内核全局（对所有 TCP 连接生效）**

```nginx
# 开启 TCP keepalive
sysctl -w net.ipv4.tcp_keepalive_time=600      # 空闲 600 秒后发探测包
sysctl -w net.ipv4.tcp_keepalive_intvl=30      # 每隔 30 秒重试一次
sysctl -w net.ipv4.tcp_keepalive_probes=5      # 连续 5 次失败就判死
```

> 这意味着：一个连接空闲 10 分钟 → 开始发探测 → 2.5 分钟内没回应就关闭。

**Nginx 后端长连接（`keepalive` 指令）**

在 upstream 配置中打开后端 TCP keepalive：

```nginx
upstream backend {
    server 10.0.0.1:8080;
    keepalive 1000;
    keepalive_timeout 60s;
    keepalive_requests 1000;
}
```

配合：

```nginx
proxy_socket_keepalive on;   # 启用内核 TCP keepalive 探测
```





**对齐超时**

- Nginx `proxy_read_timeout` 和 `keepalive_timeout` ≤ 上游空闲超时
- 常见做法是统一 60s，或者按业务压到 15~30s



**磁盘IO优化**

**直接IO绕过磁盘高速缓存**

![image-20250815003417107](D:\git_repository\cyber_security_learning\markdown_img\image-20250815003417107.png)





正常默认情况下，进程在用户空间调用read()方法，会将磁盘中的数据先读到内核缓冲区中，然后再讲内核缓冲区的数据拷贝到用户缓冲区。此时做了两次数据拷贝。

而写的时候调用write()方法，此时会将用户缓冲区的数据拷贝到内核缓冲区，这里会有一些参数对这个内核缓冲区做一个策略，比如，多少数据，多少时间统一落盘。也就是脏页写回。写的过程，也发生了两次拷贝。



而在直接IO的过程中，省略了拷贝到缓冲区的过程，而是用户缓存区直接和磁盘进行数据拷贝。



在正产模式下，将数据写入缓冲区是由好处的，也为如果下次访问磁盘命中缓存，则可以直接使用缓冲区的数据，而不用访问磁盘。



**适用于大文件：直接IO**

当磁盘上的文件大小超过SIZE后，启用directIO功能，避免BufferedIO模式下，磁盘页缓存中的拷贝消耗。

```bash
Syntax:    directio size | off
Default:   directio off;
Context:   http;server;location
```

```bash
# 直接IO的数据对齐方式，通常不用改
Syntax:    directio_alignment size;
Default:   directio_alignment 512;
Context:   http;server;location
```



**异步IO**

![image-20250815004938055](D:\git_repository\cyber_security_learning\markdown_img\image-20250815004938055.png)



在传统方式中，进程在用户空间调用read()方法，read()在读数据的时候，会阻塞，等到数据加载到用户缓冲区后，在唤醒进程，继续执行。

而在异步IO中，用户进程在读取数据，阻塞的过程中，用户程序可以处理其他任务。



```nginx
Syntax: aio on | off | threads[=pool]
Default: aio off;
Context: http,server,location
```



```nginx
# 通常情况下，aio_write是没有必要打开的，因为写入磁盘高速缓存很快
# 所以通常，仅在一种情况下，会使用aio_write，我们接受上游服务发来的响应，而且我们把上游服务的响应打开了proxy_buffer,把它记录到临时文件中，只有在这种场景下，我们才使用aio_write
Syntax: aio_write on|off
Default: aio_write off;
Context: http,server,location
```





**异步读IO线程池**

![image-20250815005850812](D:\git_repository\cyber_security_learning\markdown_img\image-20250815005850812.png)





要启用线程池，必须编译的时候加`--with-threads`



正常情况下，任务会在事件驱动框架下执行，但是对于一些有的任务的执行，我们认为它可能会阻塞住，既然它可能阻塞，就要避免让它阻塞到进程里。所以派生出一堆线程。

派生出一堆线程后，比如如图派生出三个线程。派生出三个线程后，使用一个生产者消费者模型，生产者是就是worker processes，生产一个NEW TASK，并将该TASK放到一个TASK QUEUE，这样一个先入先出的模型。线程会从队列中抢出一个任务去执行。执行过程中可能会阻塞住线程，执行完后就会继续往下走，最后扔到worker Processes进行非阻塞操作。



首先为什么会存在Tread Pool场景？

我们知道nginx中，官方的模块都是非阻塞的，为什么会出现阻塞呢？

这是因为当我们的nginx做静态资源的时候，如果处理了太多的文件，这些文件由于特别多，导致缓存inode失效（因为内存不够大），所以，大部分操作在正常情况下，会命中缓存。不会阻塞，但在这种场景下，会可能出现阻塞。

在nginx的一篇很有名的博客中，介绍了这里会有一个9倍的性能提升。



**定义线程池**

```bash
# 仅用于在处理静态资源的时候，读取文件
Syntax:    tread_pool name threads=number [max_queue=number];
Default:   tread_pool default_threads=32 max_queue=65536;
Context    main
```



**异步IO中的缓存**

将磁盘文件读入缓存中等待处理，例如gzip模块会使用。

```bash
Syntax: output_buffers number size;
Default: output_buffers 2 32k;
Context: http;server;location
```



**推荐落地配置**

```nginx
# 全局：线程池
thread_pool default threads=32 max_queue=65536;

http {
    # 小/热文件路径（默认）：页缓存 + sendfile
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    aio threads;                 # 命中 miss 或需文件读时用线程池做，避免卡 worker

    # 静态小文件优化（元信息）
    open_file_cache           max=200000 inactive=60s;
    open_file_cache_valid     120s;
    open_file_cache_min_uses  2;

    # 大/冷文件（下载、视频等）位置
    server {
        listen 80 reuseport;

        location /download/ {
            root /data/www;
            # 1) 大文件才直读，避免污染页缓存
            directio 4m;                     # 超过 4MB 启用 O_DIRECT
            directio_alignment 4096;         # 4K 对齐更通用
            # 2) 必配：线程池异步化
            aio threads=default;
            # 3) 输出缓冲（对齐且足够大）
            output_buffers 2 1m;
            # 4) 顺序预读 & 分块发送（更平滑）
            read_ahead 1m;
            sendfile_max_chunk 4m;           # 仅影响非 directio 的场景；留着也无害
            # 注：本 location 内，sendfile 自动失效（因为 directio 生效）
        }
    }
}
```

小热文件走 sendfile+页缓存；大/冷文件用 directio；所有可能阻塞的文件 I/O 交给 `aio threads` 的线程池处理







6. 说一下nginx的location中匹配优先级

```bat
# 语法规则
location [ = | ~ | ~* | ^~ ] uri {...}

=  # 用于标准uri前，表示请求字符串和uri精确匹配，大小敏感
^~ # ^~ 是用于 匹配 URL 路径的前缀匹配 的关键字。它表示在所有其他正则表达式匹配之前，优先匹配指定的前缀路径。如果一个请求的 URL 匹配到带有 ^~ 的路径规则，Nginx 会立即使用该规则并停止进一步匹配，即使后续正则表达式可能更精确。
~  # 用于标准uri前，表示包含正则，区分大小写
~* # 用于标准uri前，表示包含正则，不区分大小写
不带符号 # 匹配起始于此uri的所有uri
\  # 用于标准uri前，表示包含正则表达式并且转义字符，可以将.*?等转译为普通符号

# 匹配优先级：
=， ^~, ~/~*, 不带符号
```







7. 你用过的nginx的常用模块有哪些

```bat
Postread阶段：realip模块
Server rewrite阶段：rewrite模块
preaccess阶段：limit_req模块, limit_conn模块
access阶段：access模块，auth_basic模块，auth_request模块
precontect阶段：try_files模块
content阶段：index模块，autoindex模块，反向代理模块（upsteam）


过滤模块：在log阶段之前，content阶段之后进行处理的，比如：gzip模块，image_filter模块（缩略图）等
变量相关模块：referer模块

log阶段：log模块
```







8. 实验题：

公司开发了一个网站：

- 客户端访问proxyServer10.0.0.150, www.magedu.com

- 动态内容由后端服务10.0.0.151处理（比如：www.magedu.com/api/下的文件是动态资源）
- 静态资源有后端服务10.0.0.152处理（比如：www.magedu.com/static/下的文件是静态资源）

为了优化性能，要求：

1. 对静态资源启用缓存，缓存到 `/var/cache/nginx`，缓存有效期为 5分钟，并且每1分钟访问一次后端静态服务看是否有更新。



验证：客户端访问ProxyServer，如果是动态资源，转发到10.0.0.151,如果是静态资源，转发到10.0.0.152

提示：IP地址和域名任选，上述仅供参考

```bash
# proxyServer
proxy_cache_path /apps/nginx/proxycache levels=1:2 keys_zone=proxycache:20m inactive=5m max_size=1g;


server {
    server_name feng.magedu.org;
    root /apps/nginx/html;

    location /api/ {
            proxy_pass http://10.0.0.151;
            proxy_set_header Host "api.feng.org";
        }

    location /static/ {  
            proxy_pass http://10.0.0.152;
            proxy_set_header Host "static.feng.org";
    	    proxy_cache proxycache;
            proxy_cache_key $request_uri;
     	    proxy_cache_valid 200 302 301 60s;
            proxy_cache_valid any 2m;
            add_header X-Cache $upstream_cache_status;
    }
}
```

```nginx
# api
vim /etc/nginx/sites-enable/api.conf

server {
    server_name api.feng.org;
    root /var/www/html;
}

# static
vim /etc/nginx/sites-enable/static.conf

server {
    server_name static.feng.org;
    root /var/www/html;
}
```

