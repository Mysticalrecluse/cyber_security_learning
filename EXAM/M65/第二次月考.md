# 第二次月考



**姓名：**



## 什么是OOM，Java程序如何解决OOM问题？



**OOM**：在 **Java 程序中**，OOM 通常指 **JVM 在堆或非堆内存中无法再分配对象空间**，导致运行时崩溃。通常系统级OOM发生的情况很少。



**生产环境中的 Java OOM 故障定位流程**

**业务告警触发（通常从前端开始）**

- 大量 500 错误、接口响应超时、服务不可用

- 日志/监控平台告警、Prometheus 报警、APM 探针（如 Skywalking）预警



 **临时恢复业务**

- 重启服务！

- 临时提升 `-Xmx` 或元空间大小
- 如在容器中，临时调高 `memory limit`



**运维介入排查协助开发分析堆快照并推进根因修复**

**在上游服务中发现 Java 报错，根据报错内容判断地址的OOM原因**

- **JVM OOM**
- **OS OOM**（通常很少）



**JVM OOM 最常见的三种类型**



**一、堆内存 OOM（Heap Space OOM）**

```ABAP
# 错误信息
java.lang.OutOfMemoryError: Java heap space
```

**典型成因：**

- 对象创建过多或生命周期过长（如：大缓存、无限集合）
- 内存泄漏（对象不再使用但仍被引用）
- GC 策略不当，无法及时回收无用对象
- 堆配置太小（如默认 `-Xmx256m`）

**解决思路：**

- 增加堆大小：`-Xmx1g -Xms1g`
- 配置合适的 GC 策略（如 CMS/G1）

> **快速止损：重启服务 + 增加堆内存（临时)**
>
> **收集现场信息（关键）**
>
> `-XX:+PrintGCDetails -Xloggc:/tmp/gc.log`
>
> 这是 **开启 GC 日志记录并输出到指定文件** 的方式，用于分析 JVM 内部垃圾回收行为
>
> ```bash
> java -XX:+PrintGCDetails -Xloggc:/tmp/gc.log ...
> ```
>
> - `-XX:+PrintGCDetails`：输出详细的 GC 日志，包括 Minor GC、Full GC 次数、时间、回收大小等
> - `-Xloggc=/tmp/gc.log`：把这些信息写入日志文件中（不会输出到 stdout）
>
> **用于分析是否存在：**
>
> - Full GC 频繁
> - 每次 GC 回收内存太少
> - GC 时间是否过长
> - 是否因为 GC 耗时导致 OOM
>
> `-XX:+HeapDumpOnOutOfMemoryError`
>
> 这是 **发生 OOM 时自动生成堆内存快照（heap dump）** 的方式：
>
> ```bash
> java -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/heap.hprof ...
> ```
>
> - `.hprof` 是 JVM 堆转储文件（包含内存中所有对象、引用、类等信息）
> - 文件很大，注意磁盘空间（通常几十 MB ~ 几百 MB）
>
> 使用 VisualVM 分析 `.hprof`



**二、元空间 OOM（Metaspace OOM）**

```ABAP
java.lang.OutOfMemoryError: Metaspace
```

**典型成因：**

- 大量类加载（如反射、动态代理、热部署）
- 第三方类加载器未释放（如 Tomcat 中部署多 war 包）
- 开发代码逻辑问题：重复加载类、频繁动态生成类

**解决思路：**

- 增加元空间大小：`-XX:MaxMetaspaceSize=256m`
- 优化代码结构，避免频繁加载类
- 热部署使用专业框架（Spring Boot Devtools、JRebel）

> 临时解决：**扩大 Metaspace 限制**
>
> 通过增加启动参数：
>
> ```bash
> -XX:MaxMetaspaceSize=512m
> ```
>
> （默认是无限制，直到系统内存用完）注意：这个方法只是延迟 OOM 的发生，**不是根本解决方案**。
>
> ![image-20250808185357670](D:\git_repository\cyber_security_learning\markdown_img\image-20250808185357670.png)
>
> Metaspace OOM 的本质是：**类加载太多且无法卸载**，具体诱因通常有：
>
> | 诱因                                | 你可以通过 Probe 观察到的现象                       |
> | ----------------------------------- | --------------------------------------------------- |
> | 动态类大量生成（如 CGLIB 动态代理） | 某些 Servlet 或 Filter 的类数异常高，请求数也异常高 |
> | 类加载器泄漏（如热部署失败）        | 应用重启后类仍旧残留（可结合 JMX 查看类加载器）     |
> | 某些业务请求路径导致类爆炸增长      | 某些 Servlet 请求数暴增，但代码无明显业务量支持     |
>
> ```bat
> 注意：不要仅凭请求数就“断言”代码有问题,你发给开发的应该是“观察+建议”，而不是“定性指责”。
> ```





**三、本地线程 OOM（Native Thread OOM）**

```ABAP
java.lang.OutOfMemoryError: unable to create new native thread
```

**典型成因：**

- 创建线程数太多，超过了系统线程限制（ulimit -u）

- 每个线程栈大小太大：默认 `-Xss1m`，成千上万线程就会耗尽内存

- 使用了线程池但没设置上限（如无限队列、无限提交）



**解决思路：**

- 限制线程数：使用线程池 + 合理的最大线程配置
- 减小线程栈大小：`-Xss256k`

>  JVM 线程栈是在 **本地内存（native memory）** 中分配的，**不是在 Java 堆中！**
>
> 通常线程数越多，推荐内存越小

- 查看系统线程上限：`ulimit -u`，或 `/proc/sys/kernel/threads-max`

```bash
# 修改Tomcat 的配置：maxThreads
<Connector
    ...
    maxThreads="300"
    minSpareThreads="20"
/>

# 注意：maxThreads 一定要小于 ulimit -u
# 建议示例
ulimit -u = 4096
JVM 和系统基础线程消耗约 200～500
那么你 Tomcat 的 maxThreads 应该设在 2000～3000 左右是比较合理的

# 注意：根据业务的实际情况对 maxThreads 和 ulimit -u，通常CPU核数决定了maxThreads的值，否则maxThreads过多，会导致CPU抢占，从而导致响应变慢
# 建议：maxThreads 控制在 2～8 × CPU核数范围内，ulimit -u 设置为高于 maxThreads + 500，但不可无限大
```



 **总结：发给开发的信息建议包括**

```bat
OOM时间点及频次  
当时服务是否出现请求堆积或响应慢（结合 Probe 线程或请求数）  
导出的 heap dump（如 hprof）  
`jmap -histo:live` 中前 20 占用内存最多的类  
是否有 FullGC 频繁、GC回收率低
```













## 你在工作中监控过Java程序的哪些指标



**probe重点关注数据总结**

- **请求数**，分析是按个站点的请求内容比较多，如果出问题的话是哪个类引起的，这些都可以从下图看出，虽然运维看的不是特别明确但是开发是看的懂的，哪个类在消耗，开发自身应该很清楚
- ![alt text](D:\git_repository\cyber_security_learning\markdown_img\image98.png)
- 通过线程池数据分析线程数，看线程够不够用，如果线程用满了就即使去增大，防止出现排队，出现排队超时
- ![alt text](D:\git_repository\cyber_security_learning\markdown_img\image94.png)
- 观察内存中内存分布的情况（青年态，老年态）
  - （可以在catalina.sh的文件中指定内存的大小），后期根据实际情况去调整内存的分布,防止内存溢出和产生频发回收机制（比如年轻态过小，就会频繁出发回收，而频繁的GC会消耗系统资源）
    ![alt text](D:\git_repository\cyber_security_learning\markdown_img\image99.png)
  - 通过看系统资源的情况分析，比如由于内存过小，导致频繁出发GC，导致CPU使用率增高
  - JMP CPU UTILIZATION过高，就会导致CPU负载变高
  - FILE DESCRIPTIONS变高，又或者发生频繁的读取文件，频繁导致磁盘IO变高
    ![alt text](D:\git_repository\cyber_security_learning\markdown_img\image100.png)
- 观察（http）连接器里的点击率和吞吐量
  ![alt text](D:\git_repository\cyber_security_learning\markdown_img\image112.png)









## Nginx负载均衡的算法怎么实现的?策略有哪些?

**轮询与加权轮询**

```bash
#round-robin
upstream rrups {
    server 127.0.0.1:8011;
    server 127.0.0.1:8012;
    keepalive 32;
}

#加权round-robin
upstream rrups {
    server 127.0.0.1:8011 weight=2 max_conns=2 max_fails=2 fail_timeout=5;
    server 127.0.0.1:8012;
    keepalive 32;
}
```



**源地址hash**

```bash
#源ip地址hash
#ip_hash算法只使用ipv4的前24位做hash运算，如果客户端前24位一致，则会被调度 到同一台后端服务器
# Proxy_Server 配置
upstream group1 {
    ip_hash;         # 只要加上ip_hash即可
    server 10.0.0.210;
    server 10.0.0.159;
}
```



**使用自行指定的key做hash调度**

```bash
#使用自行指定的key做hash调度
# 三台server权重一样，调度算法hash($remote_addr)%3,值为0,1,2根据不同的值调度到不同server
upstream group1 {
    hash $remote_addr;
    server 10.0.0.210;
    server 10.0.0.159;
    server 10.0.0.213;
}

# 三台server权重不一样，调度算法hash($remote_addr)%(1+2+3),值为0,1,2,3,4,5
# 0 调度到210
# 1，2调度到159
# 3,4,5调度到213
upstream group1 {
    hash $remote_addr;
    server 10.0.0.210 weight=1;
    server 10.0.0.159 weight=2;
    server 10.0.0.213 weight=3;
}

# 也可以根据request_uri进行调度，不同客户端访问同一个资源会被调度到同一个后端服务器上
upstream group1 {
    hash $request_uri
    server 10.0.0.210;
    server 10.0.0.159;
    server 10.0.0.213;
}
```



**最少连接调度算法**

```bash
# 根据哪台服务器上的连接数少，就调度到哪里，如果多台服务器连接数一致，则这几台服务器按照轮询的方式进行调度
upstream group1 {
    least_conn;
    server 10.0.0.210;
    server 10.0.0.159;
}
```



**一致性hash**

```bash
upstream group1 {
    hash $remote_addr consistent;
    server 10.0.0.210;
    server 10.0.0.159;
    server 10.0.0.213;
}
```





## 你对nginx做过哪些优化

**根据CPU核数使用进程数**

```bash
worker_processes auto
```



**在主跑nginx的服务器上将worker进程优先级调高**

```bash
# -10起步，如果基本只跑nginx，可以调为-5
worker_priority -10
```



**调整 worker_rlimit_nofile** 

```bash
#nginx启动后，每个worker能够使用的文件描述符上限
#但因为文件描述符（FD）不仅用于连接，在 Nginx worker 进程里，FD 消耗来源主要有：
#- 网络连接（socket），每个客户端连接占用 1 个 FD（有时长连接、反向代理还会占更多 FD）。
#- 日志文件（access.log、error.log），每个打开的日志文件占用 1 个 FD。
#- 静态文件（HTML、图片等），传输过程中会打开文件 FD。
#- 反向代理/上游连接，如果启用了 upstream，一个请求可能占 2 个 FD（客户端 + upstream 服务器）。
#- 临时文件（大文件缓存、proxy_temp 等），占用临时文件 FD。

# 因此 worker_rlimit_nofile 必须大于 worker_connection

# 该数据受内核参数`ulimit -n`数量限制，来需要该外面的其他设置
# 如果是nginx直接启动，更改/etc/security/limits.conf 
# 更改limits.conf
# 添加 * hard nofile 10000
# * soft nofile 10000
# root hard nofile 10000
# root soft nofile 10000

# 如果是systemctl启动，则更改service文件中的参数
# LimitNOFILE=100000

# 百万并发建议
worker_rlimit_nofile 3200000

```



**通过亲源机制，把CPU和进程进行绑定**

```bash
# worker_cpu_affinity 不支持 auto，需要手动配置绑定掩码 
worker_cpu_affinity 00000001 00000010 00000100 00001000

# 通过脚本生成 worker_cpu_affinity 配置，实现细粒度的 CPU 绑定
CORES=$(nproc) # CPU 核数

for ((i=0; i<$CORES; i++)); do
    # 生成二进制掩码（1 左移 i 位）
    MASK=$(printf "%0${CORES}d" "$(echo "obase=2; $((1 << i))" | bc)")
    echo -n "$MASK "
done
```



**调整 worker_connection**

```bash
worker_connection 65535 # 设置单个工作进程的最大并发连接数,默认512，建议加大，否则，只能接受最大指定连接数，超出则会丢弃请求，不会处理。
```



**开启防惊群**

```bash
accept mutex on    # 惊群，推荐on，on为同一时刻一个请求轮流由worker进程处理，而防治被同时唤醒所有worker
                   # 默认为off，新请求会唤醒所有的worker进程
```



**开启多路复用**

```bash
multi_accept on  # on时Nginx服务器的每个工作进程可以同时接受多个新的网络连接，此指令默认为off，即默认为一个工作进程只能一次接受一个新的网络连接，打开后可以同时接受多个，建议设置on，充分利用多路复用的理念
```



**优化多路复用开启，导致连接在各 worker 间分布不均**

```bash
#用 SO_REUSEPORT 让每个 worker 拥有各自独立的监听队列，由内核做均衡分发，天然避免一个 worker 吸走所有连接。
#做法（Linux ≥3.9，Nginx ≥1.9.1）
# 每个 worker 拥有独立的 listen socket 与 backlog
worker_processes auto;

http {
  server {
    listen 80 reuseport;   # 关键
    # listen [::]:80 reuseport;  # IPv6 也要配
  }
}
```



**启用后端异步更新缓存**

当 Nginx 发现缓存过期时，不会立刻等待后端返回新的数据，而是先返回旧缓存给客户端，同时在后台向后端发起请求更新缓存**。这样可以**减少对后端服务器的压力，提高 Nginx 吞吐量

```bash
proxy_cache_use_stale updating error timeout http_500 http_502 http_503 http_504;
proxy_cache_background_update on;
```

1. 客户端请求到达。
2. **Nginx 发现缓存过期**，但**仍然返回旧的缓存数据**（减少客户端延迟）。
3. **同时在后台向后端请求最新数据**，更新缓存。
4. **下次有新的客户端请求时，Nginx 直接使用最新缓存**。



**`proxy_cache_use_stale` 参数详解**

| 参数                        | 作用                                   |
| --------------------------- | -------------------------------------- |
| **`updating`**              | 旧缓存过期时，仍然返回它，后台异步更新 |
| **`error`**                 | 如果后端请求失败，仍然返回旧缓存       |
| **`timeout`**               | 如果后端超时，仍然返回旧缓存           |
| **`http_500` ~ `http_504`** | 如果后端返回 5xx 错误，仍然返回旧缓存  |



**对上游服务使用keepalive长连接**

通过复用连接，降低nginx与上游服务建立，关闭连接的消耗，提升吞吐量的同时降低时延

```bash
upstream rrups {
    server 127.0.0.1:8011 weight=2 max_conns=2 max_fails=2 fail_timeout=5;
    server 127.0.0.1:8012;
    keepalive 512;
    keepalive_requests 10000;
    keepalive_time  30m;   # 限制连接寿命，避免“永久脏连接”
}

server {
    server_name rrups.feng.tech;
    error_log myerror.log info;

    location / {
        proxy_pass http://rrups;
        proxy_http_version 1.1;
        proxy_set_header Connection "";
    }
}
```



**开启 gzip 压缩**

```bash
gzip on    # 确保cpu资源富裕
```



**三次握手中的参数优化**

**SYN_SENT状态（nginx作为反向代理，对于上游是客户端）**

```bash
net.ipv4.tcp_syn_retries=6               # 主动建立连接时，发SYN的重试次数
```

> `net.ipv4.tcp_syn_retries` 决定 **主动发起连接（connect()）时**，内核在**没有收到 SYN+ACK** 时，SYN 报文会重试的次数。
>
> 默认值在很多 Linux 发行版上是 **6**，这意味着：
>
> - 第一次发 SYN，没收到回应 → 等一段时间再发一次；
> - 这个时间是**指数退避**（RTO 倍增），总共发 **1 次初始 SYN + 6 次重试**；
> - 每次等待时间大致按 RFC6298 算，6 次下来大约 **63 秒左右才放弃连接**（不同内核略有差异）。
>
> 这个逻辑在 Nginx 场景里对应：
>
> - Nginx 作为**反向代理**或**主动健康检查**时，worker 进程调用 `connect()` 到上游（upstream）；
> - 如果上游 IP 可达但端口未响应、链路丢包、或者 SYN 被防火墙丢弃 → 就会触发内核的 SYN 重试机制。
>
> 在**高并发、低延迟要求**的场景下：
>
> - **默认 6 次重试（>60 秒）太长**，会让“已不可用”的上游占着连接资源很久；
> - 对反代来说，这种连接会卡住一个 worker 的上游连接槽位（占资源）、延迟响应；
> - 特别是在**健康检查**或**短超时 API**里，长时间等 SYN 超时毫无意义，因为：
>   - 业务逻辑已经要求 1~3 秒内返回失败；
>   - 上游挂了就应该尽快 failover 或走备机；
>   - 长重试反而浪费连接、拖慢回退机制。
>
> 所以优化建议之一是：
>
> - **降低 `tcp_syn_retries` 到 2~3**（总超时 3~7 秒左右），让 connect 尽快失败；
> - 这样 Nginx 可以更快把请求切换到下一个可用 upstream



```bash
net.ipv4.ip_local_range=32768 60999      # 建立连接时的本地端口可用范围
```

> 对于反向代理的场景，nginx的上游ip和Port是确定的，本地IP也是确定的，因此端口范围就成了并发连接的瓶颈。
>
> 因此在高并发场景下，端口范围要放大。



**主动建立连接时应用层超时时间**

```nginx
Syntax: proxy_connect_timeout time;
Default: proxy_connect_timeout 60s;
Context: http,server,location
```

```nginx
Syntax: proxy_connect_timeout time;
Default: proxy_connect_timeout 60s;
Context: stream,server
```

> 优化原因和上面一样，**可以概括为**：**加快失败检测，释放资源，提升故障切换速度**。



**优先级**

如果 **`proxy_connect_timeout` < 内核 SYN 超时** → Nginx 提前放弃连接（Nginx 优先）。

如果 **`proxy_connect_timeout` ≥ 内核 SYN 超时** → 等内核超时，Nginx 才得到失败结果（内核优先）。



**SYN_RCVD状态（服务器端）**

```nginx
net.ipv4.tcp_max_syn_backlog                 # SYN_REVD状态连接的最大个数
```

```nginx
net.ipv4.tcp_synack_retries                  # 被动建立连接时，发SYN/ACK的重试次数
```



**基于三次握手在生产过程中的优化配置**

```bash
# 半连接重试次数
net.ipv4.tcp_synack_retries = 2        # 2 次，大约 6 秒左右释放

# 打开 SYN cookies（队列满时抗 flood）
net.ipv4.tcp_syncookies = 1

# 增大半连接队列长度
net.ipv4.tcp_max_syn_backlog = 262144

# 提高已完成连接的 accept 队列长度
net.core.somaxconn = 65535
```

**nginx端**

```bash
listen 443 reuseport backlog=65535
```





**高并发场景下优化**

**开启TCP keepalive**

**内核全局（对所有 TCP 连接生效）**

```nginx
# 开启 TCP keepalive
sysctl -w net.ipv4.tcp_keepalive_time=600      # 空闲 600 秒后发探测包
sysctl -w net.ipv4.tcp_keepalive_intvl=30      # 每隔 30 秒重试一次
sysctl -w net.ipv4.tcp_keepalive_probes=5      # 连续 5 次失败就判死
```

> 这意味着：一个连接空闲 10 分钟 → 开始发探测 → 2.5 分钟内没回应就关闭。

**Nginx 后端长连接（`keepalive` 指令）**

在 upstream 配置中打开后端 TCP keepalive：

```nginx
upstream backend {
    server 10.0.0.1:8080;
    keepalive 1000;
    keepalive_timeout 60s;
    keepalive_requests 1000;
}
```

配合：

```nginx
proxy_socket_keepalive on;   # 启用内核 TCP keepalive 探测
```





**对齐超时**

- Nginx `proxy_read_timeout` 和 `keepalive_timeout` ≤ 上游空闲超时
- 常见做法是统一 60s，或者按业务压到 15~30s



**磁盘IO优化**

**直接IO绕过磁盘高速缓存**

![image-20250815003417107](D:\git_repository\cyber_security_learning\markdown_img\image-20250815003417107.png)





正常默认情况下，进程在用户空间调用read()方法，会将磁盘中的数据先读到内核缓冲区中，然后再讲内核缓冲区的数据拷贝到用户缓冲区。此时做了两次数据拷贝。

而写的时候调用write()方法，此时会将用户缓冲区的数据拷贝到内核缓冲区，这里会有一些参数对这个内核缓冲区做一个策略，比如，多少数据，多少时间统一落盘。也就是脏页写回。写的过程，也发生了两次拷贝。



而在直接IO的过程中，省略了拷贝到缓冲区的过程，而是用户缓存区直接和磁盘进行数据拷贝。



在正产模式下，将数据写入缓冲区是由好处的，也为如果下次访问磁盘命中缓存，则可以直接使用缓冲区的数据，而不用访问磁盘。



**适用于大文件：直接IO**

当磁盘上的文件大小超过SIZE后，启用directIO功能，避免BufferedIO模式下，磁盘页缓存中的拷贝消耗。

```bash
Syntax:    directio size | off
Default:   directio off;
Context:   http;server;location
```

```bash
# 直接IO的数据对齐方式，通常不用改
Syntax:    directio_alignment size;
Default:   directio_alignment 512;
Context:   http;server;location
```



**异步IO**

![image-20250815004938055](D:\git_repository\cyber_security_learning\markdown_img\image-20250815004938055.png)



在传统方式中，进程在用户空间调用read()方法，read()在读数据的时候，会阻塞，等到数据加载到用户缓冲区后，在唤醒进程，继续执行。

而在异步IO中，用户进程在读取数据，阻塞的过程中，用户程序可以处理其他任务。



```nginx
Syntax: aio on | off | threads[=pool]
Default: aio off;
Context: http,server,location
```



```nginx
# 通常情况下，aio_write是没有必要打开的，因为写入磁盘高速缓存很快
# 所以通常，仅在一种情况下，会使用aio_write，我们接受上游服务发来的响应，而且我们把上游服务的响应打开了proxy_buffer,把它记录到临时文件中，只有在这种场景下，我们才使用aio_write
Syntax: aio_write on|off
Default: aio_write off;
Context: http,server,location
```





**异步读IO线程池**

![image-20250815005850812](D:\git_repository\cyber_security_learning\markdown_img\image-20250815005850812.png)





要启用线程池，必须编译的时候加`--with-threads`



正常情况下，任务会在事件驱动框架下执行，但是对于一些有的任务的执行，我们认为它可能会阻塞住，既然它可能阻塞，就要避免让它阻塞到进程里。所以派生出一堆线程。

派生出一堆线程后，比如如图派生出三个线程。派生出三个线程后，使用一个生产者消费者模型，生产者是就是worker processes，生产一个NEW TASK，并将该TASK放到一个TASK QUEUE，这样一个先入先出的模型。线程会从队列中抢出一个任务去执行。执行过程中可能会阻塞住线程，执行完后就会继续往下走，最后扔到worker Processes进行非阻塞操作。



首先为什么会存在Tread Pool场景？

我们知道nginx中，官方的模块都是非阻塞的，为什么会出现阻塞呢？

这是因为当我们的nginx做静态资源的时候，如果处理了太多的文件，这些文件由于特别多，导致缓存inode失效（因为内存不够大），所以，大部分操作在正常情况下，会命中缓存。不会阻塞，但在这种场景下，会可能出现阻塞。

在nginx的一篇很有名的博客中，介绍了这里会有一个9倍的性能提升。



**定义线程池**

```bash
# 仅用于在处理静态资源的时候，读取文件
Syntax:    tread_pool name threads=number [max_queue=number];
Default:   tread_pool default_threads=32 max_queue=65536;
Context    main
```



**异步IO中的缓存**

将磁盘文件读入缓存中等待处理，例如gzip模块会使用。

```bash
Syntax: output_buffers number size;
Default: output_buffers 2 32k;
Context: http;server;location
```



**推荐落地配置**

```nginx
# 全局：线程池
thread_pool default threads=32 max_queue=65536;

http {
    # 小/热文件路径（默认）：页缓存 + sendfile
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    aio threads;                 # 命中 miss 或需文件读时用线程池做，避免卡 worker

    # 静态小文件优化（元信息）
    open_file_cache           max=200000 inactive=60s;
    open_file_cache_valid     120s;
    open_file_cache_min_uses  2;

    # 大/冷文件（下载、视频等）位置
    server {
        listen 80 reuseport;

        location /download/ {
            root /data/www;
            # 1) 大文件才直读，避免污染页缓存
            directio 4m;                     # 超过 4MB 启用 O_DIRECT
            directio_alignment 4096;         # 4K 对齐更通用
            # 2) 必配：线程池异步化
            aio threads=default;
            # 3) 输出缓冲（对齐且足够大）
            output_buffers 2 1m;
            # 4) 顺序预读 & 分块发送（更平滑）
            read_ahead 1m;
            sendfile_max_chunk 4m;           # 仅影响非 directio 的场景；留着也无害
            # 注：本 location 内，sendfile 自动失效（因为 directio 生效）
        }
    }
}
```

小热文件走 sendfile+页缓存；大/冷文件用 directio；所有可能阻塞的文件 I/O 交给 `aio threads` 的线程池处理





s



## 叙述Https的工作过程

```bat
1. 第一步：客户端发起请求
2. 第二步：服务端将证书传给客户端
3. 第三步：客户端使用CA公钥解析验证证书,没问题的话生成一个随机值,使用服务器公钥加密随机值发给服务端
4. 第四步：服务端收到公钥加密的随机值,使用自己的私钥解密,后续客户端和服务端使用这个随机值进行对称加密，机密传输。
```







## 写出Web浏览器发起HTTP请求访问网站的过程

```bat
- 首先服务器监听打开了443或者80端口
- 浏览器从url中解析出域名
- 根据域名查询DNS，获取域名对应得IP地址
- 浏览器根据ip地址，和服务器三次握手建立TCP链接，https会额外完成TLS/SSL的握手
- 构造HTTP请求，在构造请求的过程中，填充相应的HTTP头部，包括上下文所需要的信息，至头部中
- 通过链接发起HTTP请求
- 服务器接收到HTTP请求后，完成资源的表述，把客户端请求的文件如html页面作为包体返回给浏览器
- 浏览器在渲染引擎中解析响应，根据这个响应中一些其他的超链接资源去构造其他HTTP请求
```





## 什么是linu系统负载load average

```bat
[root@ubuntu2204 ~]#man uptime
System  load  averages  is the average number of processes that are either in a runnable or uninterruptable state.  A process in a runnable state is either using the CPU or waiting to use the CPU.  A process in uninterruptable state is waiting for some I/O
access, eg waiting for disk.  The averages are taken over the three time intervals.  Load averages are not normalized for the number of CPUs in a system, so a load average of 1 means a single CPU system is loaded all the time while on a  4  CPU  system it means it was idle 75% of the time.

系统平均负载是指处于可运行或不可中断状态的进程的平均数量。处于可运行状态的进程要么正在使用 CPU，要么正在等待使用 CPU。处于不可中断状态的进程正在等待某些 I/O 访问，例如等待磁盘访问。系统平均负载是在三个时间间隔内计算得出的。平均负载并未根据系统中的 CPU 数量进行标准化，因此，对于单 CPU 系统，平均负载为 1 表示始终处于负载状态；而对于四 CPU 系统，平均负载为 1 表示 75% 的时间处于空闲状态。
```





## 简述DNS进行域名解析的过程？

```bat
用户要访问http://www.baidu.com，会先找本机的host文件，再找本地设置的DNS服务器，如果也没有
的话，就去网络中找根服务器，根服务器反馈结果，说只能提供一级域名服务器.cn，就去找一级域名服
务器，一级域名服务器说只能提供二级域名服务器.com.cn,就去找二级域名服务器，二级域服务器只能
提供三级域名服务器.http://baidu.com.cn，就去找三级域名服务器，三级域名服务器正好有这个网站
http://www.baidu.com，然后发给请求的服务器，保存一份之后，再发给客户端
```





## TCP三次握手过程

```bat
三次握手：

初始阶段，客户端是CLOSE状态，服务端需要监听服务所在端口，因此处于LISTEN状态

客户端发来SYN分组，到达了服务器，此时客户端会从CLOSE状态立即变为SYN-SENT状态

SYN到达服务端，在服务器内核中，会将"根据SYN分组内容创建的内核数据结构实例"放入SYN队列中，同时会发送一个SYN+ACK数据分组给客户端，此时服务端从LISTEN状态变为SYN-RECEIVED状态

客户端收到SYN+ACK分组，会给服务端发送ACK分组，并从SYN-SENT状态变为ESTABLISTED状态

服务端收到ACK分组后会从SYN-RECEIVED状态转换为ESTABLISHED状态，但实际在内核中，会把之前放入SYN队列中的数据结构实例移出，放入ACCEPT队列中，然后由应用程序调用accept方法从ACCEPT队列中将该数据结构实例取出，并返回一个新的文件描述符，供应用程序进行后续数据处理和通信。该文件描述符对应的是一个新的 socket，通过它，应用程序可以继续与客户端进行数据收发。
```





10. free -m ，解释 total , used , free , buff/cache , available 之间的关系

```bat
buff/cache = 页缓存 + 脏页缓存 + slab
total = used + free + shared + buff/cache
available = buff/cache(部分可回收的) + free
```

