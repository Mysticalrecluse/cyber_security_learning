# 第二次月考



**姓名：**



## 什么是OOM，Java程序如何解决OOM问题？



**OOM**：在 **Java 程序中**，OOM 通常指 **JVM 在堆或非堆内存中无法再分配对象空间**，导致运行时崩溃。通常系统级OOM发生的情况很少。



**生产环境中的 Java OOM 故障定位流程**

**业务告警触发（通常从前端开始）**

- 大量 500 错误、接口响应超时、服务不可用

- 日志/监控平台告警、Prometheus 报警、APM 探针（如 Skywalking）预警



 **临时恢复业务**

- 重启服务！

- 临时提升 `-Xmx` 或元空间大小
- 如在容器中，临时调高 `memory limit`



**运维介入排查协助开发分析堆快照并推进根因修复**

**在上游服务中发现 Java 报错，根据报错内容判断地址的OOM原因**

- **JVM OOM**
- **OS OOM**（通常很少）



**JVM OOM 最常见的三种类型**



**一、堆内存 OOM（Heap Space OOM）**

```ABAP
# 错误信息
java.lang.OutOfMemoryError: Java heap space
```

**典型成因：**

- 对象创建过多或生命周期过长（如：大缓存、无限集合）
- 内存泄漏（对象不再使用但仍被引用）
- GC 策略不当，无法及时回收无用对象
- 堆配置太小（如默认 `-Xmx256m`）

**解决思路：**

- 增加堆大小：`-Xmx1g -Xms1g`
- 配置合适的 GC 策略（如 CMS/G1）

> **快速止损：重启服务 + 增加堆内存（临时)**
>
> **收集现场信息（关键）**
>
> `-XX:+PrintGCDetails -Xloggc:/tmp/gc.log`
>
> 这是 **开启 GC 日志记录并输出到指定文件** 的方式，用于分析 JVM 内部垃圾回收行为
>
> ```bash
> java -XX:+PrintGCDetails -Xloggc:/tmp/gc.log ...
> ```
>
> - `-XX:+PrintGCDetails`：输出详细的 GC 日志，包括 Minor GC、Full GC 次数、时间、回收大小等
> - `-Xloggc=/tmp/gc.log`：把这些信息写入日志文件中（不会输出到 stdout）
>
> **用于分析是否存在：**
>
> - Full GC 频繁
> - 每次 GC 回收内存太少
> - GC 时间是否过长
> - 是否因为 GC 耗时导致 OOM
>
> `-XX:+HeapDumpOnOutOfMemoryError`
>
> 这是 **发生 OOM 时自动生成堆内存快照（heap dump）** 的方式：
>
> ```bash
> java -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/heap.hprof ...
> ```
>
> - `.hprof` 是 JVM 堆转储文件（包含内存中所有对象、引用、类等信息）
> - 文件很大，注意磁盘空间（通常几十 MB ~ 几百 MB）
>
> 使用 VisualVM 分析 `.hprof`



**二、元空间 OOM（Metaspace OOM）**

```ABAP
java.lang.OutOfMemoryError: Metaspace
```

**典型成因：**

- 大量类加载（如反射、动态代理、热部署）
- 第三方类加载器未释放（如 Tomcat 中部署多 war 包）
- 开发代码逻辑问题：重复加载类、频繁动态生成类

**解决思路：**

- 增加元空间大小：`-XX:MaxMetaspaceSize=256m`
- 优化代码结构，避免频繁加载类
- 热部署使用专业框架（Spring Boot Devtools、JRebel）

> 临时解决：**扩大 Metaspace 限制**
>
> 通过增加启动参数：
>
> ```bash
> -XX:MaxMetaspaceSize=512m
> ```
>
> （默认是无限制，直到系统内存用完）注意：这个方法只是延迟 OOM 的发生，**不是根本解决方案**。
>
> ![image-20250808185357670](D:\git_repository\cyber_security_learning\markdown_img\image-20250808185357670.png)
>
> Metaspace OOM 的本质是：**类加载太多且无法卸载**，具体诱因通常有：
>
> | 诱因                                | 你可以通过 Probe 观察到的现象                       |
> | ----------------------------------- | --------------------------------------------------- |
> | 动态类大量生成（如 CGLIB 动态代理） | 某些 Servlet 或 Filter 的类数异常高，请求数也异常高 |
> | 类加载器泄漏（如热部署失败）        | 应用重启后类仍旧残留（可结合 JMX 查看类加载器）     |
> | 某些业务请求路径导致类爆炸增长      | 某些 Servlet 请求数暴增，但代码无明显业务量支持     |
>
> ```bat
> 注意：不要仅凭请求数就“断言”代码有问题,你发给开发的应该是“观察+建议”，而不是“定性指责”。
> ```





**三、本地线程 OOM（Native Thread OOM）**

```ABAP
java.lang.OutOfMemoryError: unable to create new native thread
```

**典型成因：**

- 创建线程数太多，超过了系统线程限制（ulimit -u）

- 每个线程栈大小太大：默认 `-Xss1m`，成千上万线程就会耗尽内存

- 使用了线程池但没设置上限（如无限队列、无限提交）



**解决思路：**

- 限制线程数：使用线程池 + 合理的最大线程配置
- 减小线程栈大小：`-Xss256k`

>  JVM 线程栈是在 **本地内存（native memory）** 中分配的，**不是在 Java 堆中！**
>
> 通常线程数越多，推荐内存越小

- 查看系统线程上限：`ulimit -u`，或 `/proc/sys/kernel/threads-max`

```bash
# 修改Tomcat 的配置：maxThreads
<Connector
    ...
    maxThreads="300"
    minSpareThreads="20"
/>

# 注意：maxThreads 一定要小于 ulimit -u
# 建议示例
ulimit -u = 4096
JVM 和系统基础线程消耗约 200～500
那么你 Tomcat 的 maxThreads 应该设在 2000～3000 左右是比较合理的

# 注意：根据业务的实际情况对 maxThreads 和 ulimit -u，通常CPU核数决定了maxThreads的值，否则maxThreads过多，会导致CPU抢占，从而导致响应变慢
# 建议：maxThreads 控制在 2～8 × CPU核数范围内，ulimit -u 设置为高于 maxThreads + 500，但不可无限大
```



 **总结：发给开发的信息建议包括**

```bat
OOM时间点及频次  
当时服务是否出现请求堆积或响应慢（结合 Probe 线程或请求数）  
导出的 heap dump（如 hprof）  
`jmap -histo:live` 中前 20 占用内存最多的类  
是否有 FullGC 频繁、GC回收率低
```













## 你在工作中监控过Java程序的哪些指标



**probe重点关注数据总结**

- **请求数**，分析是按个站点的请求内容比较多，如果出问题的话是哪个类引起的，这些都可以从下图看出，虽然运维看的不是特别明确但是开发是看的懂的，哪个类在消耗，开发自身应该很清楚
- ![alt text](D:\git_repository\cyber_security_learning\markdown_img\image98.png)
- 通过线程池数据分析线程数，看线程够不够用，如果线程用满了就即使去增大，防止出现排队，出现排队超时
- ![alt text](D:\git_repository\cyber_security_learning\markdown_img\image94.png)
- 观察内存中内存分布的情况（青年态，老年态）
  - （可以在catalina.sh的文件中指定内存的大小），后期根据实际情况去调整内存的分布,防止内存溢出和产生频发回收机制（比如年轻态过小，就会频繁出发回收，而频繁的GC会消耗系统资源）
    ![alt text](D:\git_repository\cyber_security_learning\markdown_img\image99.png)
  - 通过看系统资源的情况分析，比如由于内存过小，导致频繁出发GC，导致CPU使用率增高
  - JMP CPU UTILIZATION过高，就会导致CPU负载变高
  - FILE DESCRIPTIONS变高，又或者发生频繁的读取文件，频繁导致磁盘IO变高
    ![alt text](D:\git_repository\cyber_security_learning\markdown_img\image100.png)
- 观察（http）连接器里的点击率和吞吐量
  ![alt text](D:\git_repository\cyber_security_learning\markdown_img\image112.png)









## Nginx负载均衡的算法怎么实现的?策略有哪些?

**轮询与加权轮询**

```bash
#round-robin
upstream rrups {
    server 127.0.0.1:8011;
    server 127.0.0.1:8012;
    keepalive 32;
}

#加权round-robin
upstream rrups {
    server 127.0.0.1:8011 weight=2 max_conns=2 max_fails=2 fail_timeout=5;
    server 127.0.0.1:8012;
    keepalive 32;
}
```



**源地址hash**

```bash
#源ip地址hash
#ip_hash算法只使用ipv4的前24位做hash运算，如果客户端前24位一致，则会被调度 到同一台后端服务器
# Proxy_Server 配置
upstream group1 {
    ip_hash;         # 只要加上ip_hash即可
    server 10.0.0.210;
    server 10.0.0.159;
}
```



**使用自行指定的key做hash调度**

```bash
#使用自行指定的key做hash调度
# 三台server权重一样，调度算法hash($remote_addr)%3,值为0,1,2根据不同的值调度到不同server
upstream group1 {
    hash $remote_addr;
    server 10.0.0.210;
    server 10.0.0.159;
    server 10.0.0.213;
}

# 三台server权重不一样，调度算法hash($remote_addr)%(1+2+3),值为0,1,2,3,4,5
# 0 调度到210
# 1，2调度到159
# 3,4,5调度到213
upstream group1 {
    hash $remote_addr;
    server 10.0.0.210 weight=1;
    server 10.0.0.159 weight=2;
    server 10.0.0.213 weight=3;
}

# 也可以根据request_uri进行调度，不同客户端访问同一个资源会被调度到同一个后端服务器上
upstream group1 {
    hash $request_uri
    server 10.0.0.210;
    server 10.0.0.159;
    server 10.0.0.213;
}
```



**最少连接调度算法**

```bash
# 根据哪台服务器上的连接数少，就调度到哪里，如果多台服务器连接数一致，则这几台服务器按照轮询的方式进行调度
upstream group1 {
    least_conn;
    server 10.0.0.210;
    server 10.0.0.159;
}
```



**一致性hash**

```bash
upstream group1 {
    hash $remote_addr consistent;
    server 10.0.0.210;
    server 10.0.0.159;
    server 10.0.0.213;
}
```





## 你对nginx做过哪些优化

**根据CPU核数使用进程数**

```bash
worker_processes auto
```



**在主跑nginx的服务器上将worker进程优先级调高**

```bash
# -10起步，如果基本只跑nginx，可以调为-5
worker_priority -10
```



**调整 worker_rlimit_nofile** 

```bash
#nginx启动后，每个worker能够使用的文件描述符上限
#但因为文件描述符（FD）不仅用于连接，在 Nginx worker 进程里，FD 消耗来源主要有：
#- 网络连接（socket），每个客户端连接占用 1 个 FD（有时长连接、反向代理还会占更多 FD）。
#- 日志文件（access.log、error.log），每个打开的日志文件占用 1 个 FD。
#- 静态文件（HTML、图片等），传输过程中会打开文件 FD。
#- 反向代理/上游连接，如果启用了 upstream，一个请求可能占 2 个 FD（客户端 + upstream 服务器）。
#- 临时文件（大文件缓存、proxy_temp 等），占用临时文件 FD。

# 因此 worker_rlimit_nofile 必须大于 worker_connection

# 该数据受内核参数`ulimit -n`数量限制，来需要该外面的其他设置
# 如果是nginx直接启动，更改/etc/security/limits.conf 
# 更改limits.conf
# 添加 * hard nofile 10000
# * soft nofile 10000
# root hard nofile 10000
# root soft nofile 10000

# 如果是systemctl启动，则更改service文件中的参数
# LimitNOFILE=100000

# 百万并发建议
worker_rlimit_nofile 3200000

```



**通过亲源机制，把CPU和进程进行绑定**

```bash
# worker_cpu_affinity 不支持 auto，需要手动配置绑定掩码 
worker_cpu_affinity 00000001 00000010 00000100 00001000

# 通过脚本生成 worker_cpu_affinity 配置，实现细粒度的 CPU 绑定
CORES=$(nproc) # CPU 核数

for ((i=0; i<$CORES; i++)); do
    # 生成二进制掩码（1 左移 i 位）
    MASK=$(printf "%0${CORES}d" "$(echo "obase=2; $((1 << i))" | bc)")
    echo -n "$MASK "
done
```



5. 叙述Https的工作过程





6. 写出Web浏览器发起HTTP请求访问网站的过程





7.  什么是linu系统负载load average





8. 简述DNS进行域名解析的过程？





9. TCP三次握手过程





10. free -m ，解释 total , used , free , buff/cache , available 之间的关系