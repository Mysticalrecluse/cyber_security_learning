# 关于本文档

文档名称：《NVIDIA GPU 概论》 

使用协议：《知识共享公共许可协议(CCPL)》



## 贡献者

| 贡献者名称 | 贡献度 | 文档变更记录 | 个人主页                           |
| ---------- | ------ | ------------ | ---------------------------------- |
| 张艺峰     | 作者   |              | https://github.com/Mysticalrecluse |



## 文档协议

**署名要求**：使用本系列文档，您必须保留本页中的文档来源信息，具体请参考《知识共享 (Creative  Commons) 署名4.0公共许可协议国际版》。

**非商业化使用**：遵循《知识共享公共许可协议(CCPL)》，并且您不能将本文档用于马哥教育相关业务之 外的其他任何商业用途

**您的权利**：遵循本协议后，在马哥教育相关业务之外的领域，您将有以下使用权限

共享 — 允许以非商业性质复制本作品。

改编 — 在原基础上修改、转换或以本作品为基础进行重新编辑并用于个人非商业使用。



## 致谢

本文档中，部分素材参考了相关项目的文档，以及通过搜索引擎获得的内容，这里先一并向相关的贡献者表示感谢









# GPU 硬件架构基础

```ABAP
在运行拥有极致逼真画面的游戏时，显卡每秒需要执行多少次计算
```

**每秒一亿次运算？**

每秒一亿次运算仅仅是**1996年的游戏《超级马里奥64》**所需要的运算量

![image-20250219170541793](images\image-20250219170541793.png)

**每秒一千亿次运算？**你拥有的电脑可以在**2011年运行《我的世界》**

![image-20250219170923894](images\image-20250219170923894.png)

如果你想运行画质逼真的**《赛博朋克2077》**，你需要一块显卡，它每秒能够运行大约**36万亿次运算**

![image-20250219171131464](images\image-20250219171131464.png)



36万亿次计算，这个难以想象的庞大数字，需要我们花点时间去理解它

想象一下，你每秒做一道长乘法题。（例如：4689732 * 2764569），现在假设地球上的每个人都需要做类似的计算，但用的数字各不相同，为了达到这块显卡**每秒36万亿次运算**的计算能力，我们**需要大约4400个住满了地球的人，所有人一起工作，每人每秒完成一次计算**

![image-20250219171631681](images\image-20250219171631681.png)

很难想象一个设备能完成所有这些计算，下面我们将了解，为何显卡能做到如此快速的计算，以及显卡是如何工作的？



## GPU 应用场景



-  **运行游戏画面**
- **比特币挖矿**
- **神经网络**
- **人工智能**







## GPU 物理设计和架构



### GPU 和 CPU 的区别

![image-20250219172437807](images\image-20250219172437807.png)

**GPU，拥有超过1万个核心（10496 Cores），而CPU仅有24 Cores**

![image-20250219172540250](images\image-20250219172540250.png)

GPU核心数远超CPU，看似GPU理所当然的比CPU强大，但是实际情况要比这复杂的多

如果把GPU想象成一艘巨型游轮，而CPU则是一架大型喷气式飞机，货轮的载货量相当于可以处理的计算量和数据量，而船或飞机的速度则代表这些计算和数据被处理的速率。

本质上，这是一个权衡：大量计算以较慢的速度执行，相较于少量的计算，以极快的速度执行

另一个关键区别是飞机要灵活的多，因为它可以运载乘客，包裹，或者集装箱，而且可以在数以万计的任何一个机场起降。同样的**CPU也很灵活**，它们可以运行各种各样的程序和指令

![image-20250219173216267](images\image-20250219173216267.png)

然后，巨型货轮只能运载货物的集装箱，而且只能在港口之间航行，类似地，GPU比CPU的灵活性要差得多，它们只能运行简单的指令，比如基本的算术运算。

![image-20250219173420850](images\image-20250219173420850.png)

此外GPU不能运行操作系统，也不能与输入设备或网络交互



#### 那么GPU和CPU哪一个更快

这个问题，本质上，如果你想对海量数据运行一系列计算，那么GPU完成任务的速度会更快

然而，如果你需要处理的数据少得多，而且需要快速得到结果，那么CPU会更快。

此外，如果你需要运行操作系统，或支持网络连接，以及各种不同的应用或硬件，那么你需要的会是CPU





### GPU 的 物理结构

#### 印刷电路板（PCB）

各种各样的组件都安装在上面

![image-20250219173932078](images\image-20250219173932078.png)



#### 图形化处理区（GPU）

![image-20250219174035577](images\image-20250219174035577.png)



打开它时，会看到一块名**为GA102的大型芯片或晶粒，它由283亿个晶体管构成**，![image-20250219174230758](images\image-20250219174230758.png)

芯片的大部分区域都被处理核心占据，这些核心具有**分层的组织结构**，具体来说，这块芯片，被**分为7个图形处理集群，简称GPC**

![image-20250219174659069](images\image-20250219174659069.png)

**每个处理集群内有12个流式多处理器，简称SM**

![image-20250219174759796](images\image-20250219174759796.png)

**每个流式处理器内部有4个Warp和1个光线追踪核心**

![image-20250219174913196](images\image-20250219174913196.png)

**每个Warp内有32个CUDA核心，也叫着色核心，和1个张量核心**

![image-20250219174959379](images\image-20250219174959379.png)**整个GPU共有10752个CUDA核心，336个量核心，以及84个光线追踪核心。这三个核心执行GPU的所有**

![image-20250219175222656](images\image-20250219175222656.png)

这三种核心执行GPU的所有计算任务，而且每种核心都有不同的功能。



##### CUDA核心 

CUDA核心可以被看做是简单的计算任务，有加法按钮，乘法按钮等等，通常在运行游戏时使用的最多

![image-20250219175609616](images\image-20250219175609616.png)



##### 张量核心 TENSOR

张量核心是矩阵乘法和加法计算器，用于几何变换，以及处理神经网络和人工智能。

![image-20250219175752926](images\image-20250219175752926.png)



##### 光线追踪器 Ray Tracing Cores

光线追踪核心是体积最大的，但数量最少，它们用于执行光线追踪算法

![image-20250219175909274](images\image-20250219175909274.png)



##### 相同芯片的4款GPU为什么架构款式都不同

事实是，3080,3090,3080ti和3090ti显卡都使用相同的GA102芯片设计作为它们的GPU

![image-20250219180027518](images\image-20250219180027518.png)

这4款GPU的价格不同，而且发布的年份也不同，但是却使用相同的GA102芯片，为什么呢？

因为，在制造过程中，有时会出现图案错误，灰尘颗粒，或其他制造问题导致电路的某些区域损坏并产生缺陷。与其因为一个小缺陷就把整块芯片丢弃，工程师们会找到缺陷区域，并将其永久隔离和禁用附近的电路。由于GPU采用高度重复的设计，一个核心中出现的小缺陷只会损坏特定的流式多处理器电路，而不会影响芯片的其他区域。因此这些电路会经过测试和分类，或根据缺陷的数量进行分级。3090ti显卡拥有完美无瑕的GA102芯片，所有10752个

![image-20250219220647217](images\image-20250219220647217.png)

**3090有10496个核心可以工作**

![image-20250219220729052](images\image-20250219220729052.png)

**3080Ti有10240个核心可以工作**

![image-20250219220850861](images\image-20250219220850861.png)

**3080有8704个核心可以工作**，这相当于有16个损坏且被禁用的流式多处理器。

![image-20250219220936119](images\image-20250219220936119.png)

不同的显卡在最高频率和支持GPU的显存容量和代数也有所不同。

![image-20250219221158924](images\image-20250219221158924.png)



##### CUDA核心详解

其内部大约有41万个晶体管，其中5万个晶体管负责执行`A乘以B加C`的运算，这被称为融合乘加运算，简称FMA，是显卡执行的最常见操作，一半的CUDA核心使用32位浮点数，执行FMA运算，另一半核心则使用32位整数或32位浮点数。

核心的其他部分负责处理负数，并执行简单的功能，比如位移和位掩码，以及收集和排列传入的指令和操作数，然后累加并输出结果

因此，这单个核心只是简单的计算器，功能有限。这个计算器在每个时钟周期内完成一次乘法和一次加法运算。

因此这块3090显卡拥有10496个核心和1.7GHz的频率，我们每秒可以得到35.6万亿次计算

![image-20250219222827202](images\image-20250219222827202.png)



如果要处理更复杂的运算，比如除法，平方根，三角函数等，则需要**特殊功能单元（SFU）执行**，它们的数量很少，每个SM只有4个



#### GA102的其他部分

在边缘有**12个显存控制器，NVLink控制器和PCIe接口**，底部是一个6MB的2级SRAM缓存，缓存之间这就是**Gigathread引擎，**它管理着内部所有的图形处理集群和流式多处理器

![image-20250219224524786](images\image-20250219224524786.png)



#### 显卡内部的其他部分

![image-20250219224738984](images\image-20250219224738984.png)

左侧是用于连接显示器的各种接口

![image-20250219224919218](images\image-20250219224919218.png)

右上角是12V输入电源接口，

![image-20250219225019094](images\image-20250219225019094.png)

下方是插入主板的PCIe引脚

![image-20250219225127640](images\image-20250219225127640.png)

在PCB上，大部分较小的组件，即下图粉色部分，构成了电压调节模块，它**将输入的12V电压转换为1.1V**

![image-20250219225216253](images\image-20250219225216253.png)

![image-20250219225338444](images\image-20250219225338444.png)

这些电力会使GPU发热，因此显卡的大部分重量都来自于带有4根热管的散热器

![image-20250219230316233](images\image-20250219230316233.png)

这些热管将热量从GPU和显存芯片传递到散热鳍片，在那里风扇会帮忙散热

![image-20250219230500836](images\image-20250219230500836.png)

![image-20250219230601215](images\image-20250219230601215.png)



#### 显存芯片

除GPU外，一些重要的组件，或许就是24G的显存芯片，技术上称为GDDR6X SDRAM

![image-20250219230806224](images\image-20250219230806224.png)

每当你启动一个游戏或等待加载画面时，加载所需的时间主要花在将所有3D模型从固态硬盘移动到这些显存芯片中。



如前所述，GPU在其6MB的共享二级缓存中有少量的数据存储空间，可以容纳大约这么多的游戏环境数据。因此，为了渲染游戏，不同的场景块会不断地在显存和GPU之间传输。由于核心会不断地执行每秒数万亿次的计算，GPU是数据饥渴的机器，需要不断地输入TB级的数据，因此这些显存芯片的设计，就有点像多台起重机同时为一艘货轮装载货物。具体来说，这**24块芯片一次传输384位数据**，这被称为总线宽度，而可以传输的总数据量，也就是**带宽，大约是每秒1.15TB**。

![image-20250219232349935](images\image-20250219232349935.png)

相比之下，支持CPU的DRAM条只有64位总线宽度，最大带宽接近每秒64GB

![image-20250219232537298](images\image-20250219232537298.png)

一件有趣的事是，你可能认为计算机只能使用二进制的1和0来工作，然后为了提高数据传输速率，GDDR6X和最新的显存GDDR7使用多个电压等级，而不仅仅使用0和1通过总线线路发送和接收数据。例如，GDDR7使用3种不同的编码方案，将二进制位组合三进制数字，或RAM-3符号，电压分别是0,1和-1。以下是编码方案，说明了3个二进制位是如何编码成2个三进制数字的，这个方案与11位到7个三进制数字的编码方案相结合，最终可以使用只有176个三进制数字发送276个二进制位。

![image-20250219233138850](images\image-20250219233138850.png)

上一代GDDR6X，也就是3090显卡中的显存，使用了一种不同的编码方案，称为PAM-4，它使用4种不同的电压等级发送2位数据，

![image-20250219233457136](images\image-20250219233457136.png)

然后工程师和显存行业同意在未来的显卡中改用PAM-3，以降低编码器的复杂度，提高信噪比，以提高电源效率，





## GPU 计算架构

了解像游戏画面和比特币挖矿这样的应用是如何运行所谓的**“尴尬Embarrassingly”**并行操作的，虽然这个名字可能有点傻，但**“尴尬并行”**（Embarrassingly Parallel）实际上是对计算机问题的一种技术分类，指的是几乎不需要或完全不需要任何工作就能将问题分解成并行任务，而游戏渲染和比特币挖矿很容易归入此类。

本质上，GPU使用一种叫做SIMD的原理来解决尴尬并行问题。SIMD（Single Instruction Multiple Data）代表单指令多数据，即相同的指令或步骤会在成千上万甚至数百万不同的数字上重复运行。



### SIMD详解

#### 图形渲染示例

例如：了解SIMD，也就是单指令多数据是如何用来创建下图中的3D游戏环境的

![image-20250219235354385](images\image-20250219235354385.png)

例如：桌子上的牛仔帽大约由2.8万个三角形组成，这些三角形由大约1.4万个顶点连接而成，每个顶点有X,Y和Z坐标，

![image-20250219235443145](images\image-20250219235443145.png)

![image-20250219235620970](images\image-20250219235620970.png)

![image-20250219235655045](images\image-20250219235655045.png)

这些顶点坐标是使用一个名为模型空间的坐标系构建的

![image-20250219235742595](images\image-20250219235742595.png)

原点(0,0,0)位于帽子的中心，为了构建一个3D世界，我们会放置数百个物体，

![image-20250219235856980](images\image-20250219235856980.png)

每个物体都有自己的模型空间，并放入世界环境中

![image-20250219235948648](images\image-20250219235948648.png)

为了让摄像机能够判断每个物体相对于其他物体的位置，我们必须转换或变化每个独立模型空间中的所有顶点到共享的世界坐标系或世界空间中，

![image-20250220000139771](images\image-20250220000139771.png)

举个例子，我们如何将牛仔帽的1.4万个顶点从模型空间转换到世界空间？

我们使用一个单一指令，将帽子原点在世界空间中的位置，添加到模型空间中单个顶点对应的X,Y,Z坐标上。

![image-20250220000453240](images\image-20250220000453240.png)

接下来，我们将这个指令复制到多个数据上，也就是剩余的X，Y，Z坐标，这些坐标用于构建帽子的其他数千个顶点。

![image-20250220000425065](images\image-20250220000425065.png)

然后，我们对桌子和场景中的其他数百个物体进行相同的操作，每次都使用相同的指令，但使用不同物体在世界空间中的坐标，以及每个物体在模型空间中的数千个顶点。结果，所有物体的所有顶点和三角形都被转换到一个共同的世界空间坐标中，摄像机现在可以确定哪些物体在前面，哪些物体在后面了。

这个例子体现了SIMD，也就是单指令多数据的强大功能，以及如何将一个单一指令应用于场景中5629个不同的物体，总共830万个顶点，最终执行了2500万次加法计算。

SIMD和尴尬并行程序的关键在于这数百万次计算中的每一次都不依赖于任何其他的计算，因此这些计算都可以分配给GPU的数千个核心，彼此并行完成。

需要注意的是，从模型空间到世界空间的顶点变换只是相当复杂的游戏图形渲染管线的第一步，此外我们也跳过了对每个物体进行旋转和缩放的变换，但考虑到这些值是一个类似的过程，需要额外的SIMD计算。



#### 比特币示例

为什么GPU最初被用于比特币挖矿？

 本质上是为了在区块链上创建一个区块，SHA-256哈希算法会在一组包含交易，时间戳，附加数据和一个名为nonce的随机数的数据上运行。

![image-20250220152808493](images\image-20250220152808493.png)

将这些值输入SHA-256哈希算法后，会输出一个随机的256位值。

![image-20250220153048569](images\image-20250220153048569.png)

你可以把这个算法想象成一个彩票生成器，你无法选择彩票号码，但基于输入数据，SHA-256算法会生成一个随机的彩票号码。

![image-20250220153312514](images\image-20250220153312514.png)

因此，如果你改变nonce值，而保持其他交易数据不变，你就会生成一个新的随机彩票号码。这场比特币挖矿彩票的赢家是第一个随机生成的，前80位全为零的彩票号码，而其余的176位则无关紧要

![image-20250220153519738](images\image-20250220153519738.png)

一旦找到中奖的比特币彩票，奖励就是3个比特币，然后彩票会重置，并使用一组新的交易和输入值。

那么，为什么GPU会被用于挖矿呢？因为GPU可以运行数千次SHA-256算法迭代，使用相同的交易，时间戳和其他数据，但使用不同的nonce值。因此，像这样的显卡每秒可以生成大约9500万个SHA-256哈希值，或者说9500万张随机号码彩票

![image-20250220153851305](images\image-20250220153851305.png)

然后，希望能找到前80位全为零的号码。然后，如今装满ASIC，也就是专用集成电路的计算机每秒可以执行250万亿次哈希运算，相当于2600块显卡的性能，因此在比特币挖矿领域，显卡就像挖掘机旁边的小勺子，而挖掘机就是ASIC矿机。







### 计算架构如何与物理架构匹配

本质上，一个指令由一个线程完成

![image-20250220002110344](images\image-20250220002110344.png)

而这个线程会匹配到一个CUDA核心。

![image-20250220002135050](images\image-20250220002135050.png)

线程会被捆绑成32个一组，称为warp。

![image-20250220002209477](images\image-20250220002209477.png)

同一系列指令会发送给一个Wrap中的所有线程。

![image-20250220002227808](images\image-20250220002227808.png)

接下来，warp会被分组成线程块，由流式多处理器处理。

![image-20250220002258950](images\image-20250220002258950.png)

最后，线程块会被分组称作网络，在整个GPU上继续计算。

![image-20250220002336438](images\image-20250220002336438.png)

所有这些计算都由Gigathread引擎管理和调度，它可以高效地将线程块映射到可用的流式处理器上

![image-20250220002601721](images\image-20250220002601721.png)

一个重要的区别是，在SIMD架构中，一个WARP中的所有32个线程都遵循相同的指令，并彼此步调一致。这种步调一致的执行方式适合于GPU，直到2016年左右

然后，较新的GPU遵循SIMT架构，也就是单指令多线程（Single Instraction Multiple Threads）SIMD和SIMT的区别在于，虽然两者都会向每个线程发送相同的指令，但是在SIMT中，各线程不需要彼此步调一致，可以以不同的速率运行。用技术术语来说，就是每个线程都有自己的程序计数器。

此外，在SIMT中，流式处理器中的所有线程都使用共享的128KB一级缓存，因此一个线程输出的数据可以随时另一个线程使用。

从SIMD到SIMT的改进使得在遇到warp分歧时（通过依赖于数据的分支条件）可以有更大的灵活性，并且更容易让线程重新汇聚，以达到屏障同步。

![image-20250220003550711](images\image-20250220003550711.png)

本质上，较新的GPU架构更加灵活高效，尤其是在遇到代码分支时。还有一点需要注意，虽然你可能认为warp这个词来自于曲速引擎，但是实际上它来自于纺织







# GPU 进阶



## GPU 的层次化计算结构

在 **NVIDIA GPU 架构** 中，**GPC（Graphics Processing Cluster）**、**TPC（Texture Processing Cluster）** 和 **SM（Streaming Multiprocessor）** 之间的关系体现了 **GPU 的层次化计算结构**，用于组织 CUDA 核心、Tensor 核心、纹理处理单元等计算资源。下面我们详细解析它们之间的关系



### **关系概述**

#### **GPC（Graphics Processing Cluster，图形处理集群）**

- **GPC 是 GPU 最高一级的计算单元，每个 GPC 包含多个 TPC**。
- 每个 GPC 主要由：
  - **多个 TPC（Texture Processing Cluster）**
  - **Raster Engine（光栅化引擎，主要用于图形渲染）**
  - **L2 Cache 连接通道**
- 在数据流上，GPC 处理的任务来自全局指令流，通过 **Warp Scheduler（线程调度器）** 分配任务到 **SM**。



#### **TPC（Texture Processing Cluster，纹理处理集群）**

- **TPC 是 GPC 内的一个子单元，每个 TPC 包含 1~2 个 SM**。
- 主要组件：
  - **SM（Streaming Multiprocessor，流式多处理器）**
  - **Texture Unit（纹理单元，用于图形渲染）**
- **TPC 的主要作用是负责计算和纹理处理**，其中 SM 进行核心的 CUDA 计算。



#### **SM（Streaming Multiprocessor，流式多处理器）**

- SM 是执行 CUDA 核心计算的基本单位

  ，包含：

  - **CUDA Cores**（负责通用计算）
  - **Tensor Cores**（负责深度学习计算）
  - **SFU（Special Function Unit，处理特殊数学函数，如 `sin, cos, exp, sqrt`）**
  - **LD/ST 单元（负责内存加载/存储）**
  - **Warp Scheduler（负责任务调度）**

- **每个 SM 负责执行多个 Warp（32 线程）并行计算任务**



### **具体架构示例（NVIDIA A100 vs RTX 4090）**

| **架构**            | **GPC 数量** | **TPC 数量** | **SM 数量** | **CUDA 核心** | **Tensor 核心** |
| ------------------- | ------------ | ------------ | ----------- | ------------- | --------------- |
| **NVIDIA A100**     | 8 GPC        | 48 TPC       | 108 SM      | 6912          | 432             |
| **NVIDIA RTX 4090** | 11 GPC       | 64 TPC       | 128 SM      | 16384         | 512             |



## **各种计算任务的分工**

| **运算类型**                                    | **负责的硬件**              | **说明**                                       |
| ----------------------------------------------- | --------------------------- | ---------------------------------------------- |
| **加法、乘法（FMA, Fused Multiply-Add）**       | **CUDA 核心**               | 最常见的运算，适用于通用计算                   |
| **矩阵乘法（MMA, Matrix Multiply-Accumulate）** | **Tensor 核心**             | 主要用于深度学习计算                           |
| **整数计算（INT8, INT32, INT64）**              | **CUDA 核心**               | 适用于整数运算，如索引计算                     |
| **除法（DIV）、开方（SQRT）**                   | **特殊功能单元（SFU）**     | 比 FMA 运算慢，需要额外硬件支持                |
| **三角函数（sin, cos, tan）**                   | **SFU**                     | 由于计算复杂性，GPU 采用 LUT（查表）或逼近算法 |
| **指数、对数（exp, log）**                      | **SFU**                     | 也是通过查表或迭代逼近实现                     |
| **光线追踪（Ray Tracing）**                     | **RT 核心（RTX GPU 专用）** | 专门用于加速光线追踪计算                       |
| **AI 推理（低精度计算）**                       | **Tensor 核心**             | 处理 FP16、BF16、INT8 计算                     |
| **纹理采样、插值**                              | **纹理单元（TMU）**         | 主要用于图形渲染                               |

虽然CUDA Core可以用于AI深度学习，但其设计目的主要是用于图形渲染，物理模拟，科学计算等**并行处理任务**，其针对矩阵运算的性能提升有限。

而矩阵运算是深度学习，所使用的神经网络的进行训练和推理的核心运算。

为此2017年5月，NVDIA公司发布 **Tensor Core**，专门用于执行神经网络训练和推理中的“**矩阵乘法累加**”操作，即MMA（Martrix Multiply-Accumulate） 



###  **CUDA Core vs. Tensor Core 的核心区别**

| **计算单元**    | **主要任务**                        | **运算模式**                                      |
| --------------- | ----------------------------------- | ------------------------------------------------- |
| **CUDA Core**   | **标量 FMA**（单个元素运算）        | `C[i] = A[i] * B[i] + C[i]`                       |
| **Tensor Core** | **矩阵 FMA（MMA）**（块状矩阵运算） | `C = A × B + C`（一次性处理 4x4、8x8 或更大矩阵） |

**关键区别：**

- **CUDA Core 逐个处理标量 FMA 运算**（适用于一般计算）。
- **Tensor Core 以矩阵块（Warp 级并行）执行 MMA（矩阵乘加）运算**，但本质上它仍然是 FMA 运算的一种。



#### **Tensor Core 计算过程（MMA 本质上包含 FMA）**

在 NVIDIA Tensor Core 计算时，它执行的矩阵乘加（MMA）操作：

C=A×B+CC = A \times B + CC=A×B+C

其实本质上是多个 **FMA（乘加）运算的并行执行**。

以 **NVIDIA Ampere 架构** 为例：

- Tensor Core 可以 **并行计算 8x8 或 16x16 大小的矩阵**。
- 在计算 `C = A × B + C` 时，它内部会拆分为 **多个 FMA 运算** 并行执行。

**示例（4x4 矩阵计算时，每个元素计算方式）：**

![image-20250220143614508](images\image-20250220143614508.png)

这本质上仍然是多个 FMA 运算的并行执行。



#### **什么时候 Tensor Core 不会被调用？**

尽管 Tensor Core 进行的 MMA 计算是基于 FMA 运算的，但 **只有满足特定数据类型和矩阵大小** 时，Tensor Core 才会被调用：

1. **数据类型要求**：
   - 仅支持 **FP16, BF16, INT8, FP8, TF32**（不同架构支持不同数据类型）。
   - 对于 **FP32 及 FP64 计算，默认使用 CUDA Core，而不是 Tensor Core**。
2. **计算方式要求**：
   - Tensor Core **仅用于矩阵计算（MMA）**，**不会用于标量 FMA 计算**。
   - 如果你的计算只是 `A[i] * B[i] + C[i]`，即逐个元素计算，它仍然会使用 **CUDA Core** 而不是 Tensor Core。



与 CUDA Core 相比，Tensor Core的核心特别大，它可以把整个矩阵都载入寄存器中批量运算，用于深度学习神经网络加速时，可实现十几倍的效率提升





#### 以NVIDIA GH100 GPU为例分析

NVIDIA GH100 GPU 包含**8个 GPC** (GPU Processing Cluster) 、**72个TCP**（Texture Processing Cluster，9TPCs/GPC）、**144个流式多处理器**（SM，Streaming Multiprocessor，2SMs/TPC）、

每个SM包含**16 * 4=64个INT32**的CUDA Core、包含**32*4=128个FP32**的CUDA Core、包含**16 * 4=64个FP64**的CUDA Core、包含**1*4=4个Tensor Core**（第四代）

![image-20250220160056777](images\image-20250220160056777.png)





#### 从 FP32 到 TF32

##### FP32详解

FP32表示精度为32的单精度浮点数，

![image-20250220160424430](images\image-20250220160424430.png)

在 **IEEE 754 单精度浮点数（32-bit）** 中，数据是通过以下三个部分表示的：

| **符号位 (1 bit)** | **指数位 (8 bits)** | **尾数（小数部分）(23 bits)** |
| ------------------ | ------------------- | ----------------------------- |
| 符号 `S`           | 指数 `E`（带偏移）  | 尾数 `M`（隐藏的 1）          |

**浮点数的计算公式：**

![image-20250220161126166](images\image-20250220161126166.png)

其中：

- **`S`（符号位）**：0 表示正数，1 表示负数。
- **`E`（指数位）**：表示 2 的幂次，采用 **偏移量（Bias）127** 方式编码，即： 实际指数=E−127\text{实际指数} = E - 127实际指数=E−127
- **`M`（尾数）**：`1.xxxx` 形式的二进制数（即隐式的 1 加上 23-bit 尾数）。



##### **FP32的数据范围总结**

| **数值类型**       | **指数 (`E`)**   | **实际指数 (`E - 127`)** | **数据范围**        |
| ------------------ | ---------------- | ------------------------ | ------------------- |
| **最大规格化数**   | `E = 254`        | `127`                    | `≈ 3.4 × 10^{38}`   |
| **最小规格化数**   | `E = 1`          | `-126`                   | `≈ 1.18 × 10^{-38}` |
| **最小非规格化数** | `E = 0`          | `-126`（特殊）           | `≈ 1.4 × 10^{-45}`  |
| **最大非规格化数** | `E = 0`          | `-126`                   | `≈ 1.18 × 10^{-38}` |
| **正负无穷大**     | `E = 255, M = 0` | 无效                     | `±∞`                |
| **NaN（非数）**    | `E = 255, M ≠ 0` | 无效                     | `NaN`               |

```ABAP
总结：
指数位越多，尾数位则越少，其表示的范围越大，但精度就会降低
指数位越少，尾数位则越多，其表示的范围越小，但精度就会越高
```

单精度FP32浮点数，能平衡数值范围和精度，非常适合做科学计算。

然而，对于AI深度学习来说，特别是在神经网络的“初期训练”阶段，与FP32相比，FP16提供的精度已经足够，且FP16的运算速度更快。同时，利用Tensor Core，可以进行混合精度FMA浮点运算（即，乘法时使用FP16加快计算进度，累加时，使用FP32保证结果的精度）基于Tensor Core的混合精度计算，让相同资源的硬件能够训练更大的神经网络模型，或更快的处理大数据集，这对于拥有大规模数据并需要大量训练时间的AI应用，如图像识别，自然语言处理等意义重大。

![image-20250220162142591](images\image-20250220162142591.png)

虽然FP16的运算速度快，但是他的数据范围有限，但是对于AI深度学习而言，数值范围非常重要。如何能兼顾FP32的数值范围，同时拥有FP16的运算速度呢？？



##### BF16详解

2017年，Google的AI研究小组Google Brain，在开发其深度学习框架TensorFlow的过程中发明了一种新型浮点数值格式。称为BF16/BFloat16 (Brain Floating Point Format 16) 

![image-20250220162618979](images\image-20250220162618979.png)

谷歌大脑团队看到FP16格式数值范围过小的限制，通过调整FP16的指数位（从5位增加到8位），同时减小尾数位（从10位减小到7位），在依然使用十六位二进制的前提下，将FP16调整为BF16,使得BF16所表示的动态数值范围与FP32一致，精度变为3为有效十进制数字

BF16扩大FP16的数值范围，但是损失了FP16的精度

![image-20250220165035258](images\image-20250220165035258.png)

##### TF32详解

2020年5月，NVIDIA在发布Ampere架构（Tensor Core 3.0）的时候，创新的发布了一种新的浮点类型，称为TF32（TensorFloat-32）

即用19位(bit)二进制表示，包含一位符号，8位指数位（与FP32相同）和10位尾数位（与FP16相同）

![image-20250220165519828](images\image-20250220165519828.png)

可见，TF32的设计在BF16的基础上增加了尾数位，让TF32拥有FP32的数值范围的同时，在计算效率上接近于FP16，在实际运行中，只需把FP32浮点数的尾数截断，就变成了TF32，而TF32在计算后可补尾数直接转成FP32

这让很多AI深度学习的程序代码零更改即可获得巨大的性能提升



在TF32出现后，PyTorch在1.7-1.11几个版本使用了TF32作为默认的浮点类型，但是从**1.12版本**开始，**PyTorch**重新开始使用FP32作为默认浮点数类型，**如果要使用TF32浮点数需要手动设置开启**

潜在原因是：PyTorch除了被用在深度学习领域，在科学计算领域的应用更加广泛，TF32作为默认值，会导致科学计算应用的精度下降，进而出现科学计算应用的结果偏差



## 认识 NVLink

### 从 PCI-e 到 NVLink

#### 认识PCI-e

PCI-e是英文"Peripheral Component Interconnect Express"的缩写，中文直译“外部设备互联快速总线”，是一种高速串行计算机扩展总线标准

简单说，就是PCI-e是一种通用的主板接口，可用来将常见的GPU，声卡，网卡，固态硬盘，WIFI模块等硬件设备连接到主板上

![image-20250220170918027](images\image-20250220170918027.png)

随着AI深度学习的发展，对GPU的算力需求不断提升，单机单卡算力已无法满足应用需求，单机2卡，单机4卡，单机8卡的GPU算力服务器应用而生，此时一个新的问题出现了：

如何让一台服务器中的多个GPU之间有效并快速地数据传输呢？

![image-20250220172131474](images\image-20250220172131474.png)



#### 传统服务器：2卡GPU数据传输模式

![image-20250220172317979](images\image-20250220172317979.png)

示例是拥有两个GPU的传统服务器，每个GPU都安装在PCIe插槽中，GPU0将数据传输到GPU1的具体过程如下：

- CPU从GPU0的显存读取数据，通过PCIe总线将数据复制到主机内存中
- CPU再通过PCIe总线将刚刚复制进主机内存中的数据再复制到GPU1的显存中



如上，通过CPU的中转，GPU0完成数据到GPU1的一次传输，在这种数据传输模式下，GPU之间的数据的每次传输都必须经过CPU中转，读取1次，复制两次，且传输过程均通过PCIe总线完成

上述经过CPU中转的传输方式，效率低下，一个显而易见的优化思路是想办法让GPU0和GPU1之间直接通信，进行数据传输

2011年，NVIDIA公司推出**GPUDirect P2P**（Peer-to-Peer）技术，即“GPU点对点直连”技术，允许同一台服务器内的多个GPU通过PCI-e总线点对点的直接连接通信，数据直接从一个GPU的显存，复制到另一个GPU的显存，而不需要CPU利用主机内存，并使用PCI-e总线中转，数据只需要被读取1次并复制1次，GPUDirect P2P技术，直接减少了数据复制次数，从而降低了数据传输延迟，提升了PCIe总线的带宽利用率，适用于需要频繁进行GPU间数据交换的**AI深度学习任务**，虽然GPUDirect P2P 技术解决了GPU间的直接通讯问题，但随着AI深度学习任务规模的不断增加，GPU间的数据交换量也大幅增长，PCIe总线带宽逐渐成为数据交换的性能瓶颈

2014年NVIDIA公司发布GPU间数据传输通信的全新总线架构，成为**NVLink**。GPU间不再依赖PCIe总线进行数据传输，彼时PCIe总线发展到3.0版本，双向带宽不足32GB/s，而NVIDIA新发布的**NVLink1.0的双向带宽高达160GB/s**，比PCI-e3.0快5倍左右，NVLink的提出解决了PCIe带宽不足的问题，但其本质上是GPUDirect P2P技术的延伸。

随着GPU卡数的增加，P2P技术的固有问题逐渐显现，要想让单台服务器上的全部GPU之间进行高速通信，就需要建立GPU间的全互联(Full-Mesh)NVlink架构

举个例子：8卡GPU之间全互联，每个GPU需要7个NVLink通道，总共需要8*7/2=28个NVLink连接，16卡GPU之间全互联，每个GPU需要15个NVLink通道，总共需要16 * 15/2=120个NVLink连接

![image-20250220222601089](images\image-20250220222601089.png)

显然，这种全互联NVLink架构的横向扩展能力是有问题的

- 单个GPU的NVLink通道有限，不可能随着GPU数量的增长而不断增长
- 每对GPU之间只有1条数据通路，存在单点故障隐患

![image-20250220222803405](images\image-20250220222803405.png)



### 从 NVLink 到 NVSwitch

事实上，在传统的计算机网络通信中，基于多层交换机实现多节点互联已经是很成熟的方案了

![image-20250220222942229](images\image-20250220222942229.png)

于是2018年，NVIDIA公司发布“NVSwitch”技术架构，它是一种基于NVLink技术组网的交换芯片/设备，将GPU间的高速通讯利用NVSwitch，组成一个基于NVLink的互联拓扑结构

![image-20250220223211561](images\image-20250220223211561.png)

如下图所示：一个8卡H100服务器内部基于NVLink和NVSwitch的互联通信拓扑结构，从图中可见每个GPU通过4个NVLink通道和4个NVSwitch相连，每个NVSwitch与8个GPU通过NVLink相连 ，总共需要4 * 8 = 32 个NVLink连接

![image-20250220223301261](images\image-20250220223301261.png)

![image-20250220223714490](images\image-20250220223714490.png)

与8卡NVLink全互联架构相比,这个基于NVSwitch的网状互联拓扑看起来多了4个NVlLink连接，但是带来如下好处：

1. 在“基于NVSwitch”的网状互联拓扑下，图中每个GPU只需要1条NVLink通道，通过NVSwitchd的中转，即实现与其他全GPU的全联通

![image-20250220224032352](images\image-20250220224032352.png)

而在点对点的互联架构下，实现全联通，每个GPU需要7条NVLink通道

![image-20250220224139573](images\image-20250220224139573.png)

易知，随着GPU数目的增加，点对点架构实现全联通所需要的NVLink通道数量与GPU数量成正比，而基于NVSwitch的网状互联架构实现全联通，只与NVSwitch所能承载的通道数，以及其与GPU的连接拓扑相关，



2. 在基于NVSwitch的网状互联结构下，图中任意GPU之间可通过4条NVLink通道

![image-20250220224444201](images\image-20250220224444201.png)

如果GPU的某条NVLink通道中断或者堵塞，它还可以通过其他3条NVLink通道进行通信，而在点对点互联架构下，任意GPU只有1条NVLink通道，如果某个NVLink通道中断或者堵塞，即会引起NVLink连接的GPU之间的通信故障

![image-20250220224737746](images\image-20250220224737746.png)

可见，与点对点互联架构相比，基于NVSwitch的网状互联架构不仅提高通信带宽（4倍），还避免了单点故障

3. 在基于NVSwitch的网状互联结构下，假设NVSwitch所能承载的通道数不变，那么只需要增加单个GPU所支持的NVLink通道数，即可以增加单GPU的整体通信带宽

![image-20250220225024716](images\image-20250220225024716.png)

事实上，单个GPU的通道数，从4条（NVLink1.0）已经发展到18条（NVLink5.0），通信带宽从160GBps发展到1.8TBps

4. 在基于NVSwitch的网状互联架构下，假设NVSwitch数量不变，且单个GPU所支持的通道数不变，那么只需要增加NVSwitch所支持的通道数，就能够轻松增加系统所支持的整体GPU数量，比如：图中单个NVSwitch可连接8个GPU，如果其所支持的NVLink通道扩展为16，那么很容易将系统中的GPU数量扩展为16个，

![image-20250220225458953](images\image-20250220225458953.png)

综上所述，NVIDIA公司通过NVLink解决了GPU通信效率和带宽问题，而通过NVSwitch解决了GPU节点数量问题，



## 认识 RDMA

### 从TCP/IP 到 RDMA

随着AI深度学习的发展，对GPU的算力需求不断提升，AI基础设施从使用单服务器1卡，发展到使用单服务器8卡，进而发展到使用多服务器8卡的分布式计算环境，在早期的分布式计算中，各节点Node（即服务器）之间的通信是基于Ethernet（以太网）的TCP/IP技术栈实现的

 ![image-20250220230207614](images\image-20250220230207614.png)

下图展示了基于传统TCP/IP技术栈，节点Node1的应用程序，向节点Node2的应用程序传输数据的过程，具体如下

![image-20250220230354112](images\image-20250220230354112.png)

- 节点Node1的应用程序将要发送的数据放入其本地应用程序的用户缓冲区（User Buffer）中
- 节点Node1的内核（Kernel）进程定期检查本地应用程序的用户缓冲区，当检测到数据存在时，将数据从用户缓冲区复制到内核的网络套接字缓冲区（Socket Buffer）,**这是第一次复制**
- 节点Node1的应用程序数据被复制到内核套接字缓冲区后，TCP/IP协议栈开始处理数据，数据会在TCP/IP协议栈不同层级间，进行封装和传递
- 节点Node1的TCP/IP协议栈处理完数据后，数据从TCP/IP协议栈驱动缓冲区（Driver Buffer）被复制到网络适配器（Adapter）的发送缓冲区（Sending Buffer）这是**第二次复制**
- 节点Node1的网络适配器将数据包发送到以太网络中，节点Node1至此完成“数据发送“流程，数据现在进入节点Node2的”数据接收“流程
- 节点Node2在以太网络上发现来自节点Node1的数据，进入到与”数据发送“流程向逆的”数据接收“流程
- 节点Node2将以太网络中的数据接收到网络适配器的接收缓冲区（Receiving Buffer）然后复制到节点Node2的内核TCP/IP协议栈的驱动缓冲区（Driver Buffer）这是**第三次复制**
- 节点Node2将接受到驱动缓冲区的数据在TCP/IP协议栈的不同层级间进行解封装和传递，发送到套接字缓冲区
- 节点Node2将套接字缓冲区的数据复制到应用程序的用户缓冲区（User Buffer）供节点Node2d的应用程序使用，这是**第四次复制**

从上述数据传输过程可见，节点Node1到节点Node2的一次数据传输，至少经过4次数据复制和2次用户态/内核态的上下文切换，这不仅大量消耗系统资源，还严重限制网络通信带宽，并增加数据延迟，上述问题随着AI深度学习的发展，其弊端日益凸显，因为各节点Node的GPU之间需要交换大量的数据，于是，如何提高多节点Node之间网络数据的传输性能，以满足AI深度学习不断增长的性能需求成为亟待解决的新问题。

为了解决上述问题，2003年，国际商用机器，IBM公司和惠普HP公司提出来**远程直接内存访问（RDMA,Remote Direct Memory Access）**，RDMA技术解决问题的思路是绕过CPU，去除用户态/内核态上下文切换和数据复制，直接在不同节点Node的应用程序的内存间实现数据传输，即允许本地节点的应用程序直接访问远程节点应用程序的内存的内容

![image-20250220232642824](images\image-20250220232642824.png)

与传递的TCP/IP传输协议相比，RDMA有如下好处

- **零复制 Zero Copy** ：即避免用户态和内核态之间的数据复制，减少内存消耗
- **内核旁路（Kernel Bypass）**：即避免用户态和内核态之间的上下文切换降低数据传输延迟
- **CPU减负（CPU Offload）**：即数据传输过程中不需要CPU参与，降低CPU资源的消耗



### RDMA技术通过InfiniBand和RoCE落地

![image-20250220233112998](images\image-20250220233112998.png)

为了实现RBMA技术的落地，需要在软件和硬件上进行升级改造，具体来说，在软件上，需要定义新的技术规范，以取代传统的TCP/IP网络协议规范，这就是**RDMA Verbs**

它是由“IB贸易协会（IBTA, InfiniBand Trade Association）”组织指定的一组规范，用于定义RDMA传输过程中应支持的特性和行为

在此基础上，开放结构联盟组织（OFA, OpenFabrics Alliance）基于Verbs的定义推动实现了RDMA的应用编程接口，即**Verbs API**，允许应用程序可以无视底层硬件和协会的差异，用于替代传统TCP/IP网络协议的套接字API

在硬件上，服务器需要使用支持RDMA的网络适配器   （RDMA Network Adapter）以取代传统的网络适配器

![image-20250220233800649](images\image-20250220233800649.png)

RDMA网络适配器是RDMA技术的主要承载者，有多种不同的通信协议，典型如InfiniBand，RoCE（RDMA over Converged Ethernet）



### iWARP

事实上，RDMA网络适配器所支持的协议不同，其专用名称也不同，比如

- 在InfiniBand和RoCE协议中，RDMA网络适配器可被称为“**主机通道适配器**（HCA，Host Channel Adapter）”
- 仅支持InfiniBand协议的RDMA网络适配器，又被成为**IB适配器**（InfiniBand Adapter）,IB网络或IB卡
- 在iWARP协议中，RDMA网络适配器被称为“**可感知RDMA的网络接口控制器**”（RNIC, RDMA-aware Network Interface Controller）

如下图所示，在RDMA通信中，数据包的解析，封装与解封装等工作都在RDMA网卡上完成，完全旁路操作系统内核，极大简化数据处理流程，显著降低数据传输延迟

![image-20250220234434835](images\image-20250220234434835.png)

事实上，最早参与制定并发布InfiniBand协议标准的公司Mellanax，在2019年3月11日被NVIDIA收购，在分布式计算环境中，基于IB网卡组建高速互联网络实现多服务器多网卡间的数据通信，已经成为AI深度学习神经网络“训练”环境的主流方案，当然RoCE方案也逐渐成熟，被越来越多的公司采用，基于RDMA的高速互联网络环境的AI应用越来越广泛