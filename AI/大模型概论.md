# 大模型



## 大模型概论

### 大模型分类

- 面向通用领域：ChatGPT，llama，Deepseek
- 细分领域：（企业中完成业务需求）一般用 **6-8B** 的模型进行训练





## RAG

问题：将Deepseek下载到本地，能满足我们企业的特定任务需求吗？

**肯定不能**

![image-20250304101917773](D:\git_repository\cyber_security_learning\markdown_img\image-20250304101917773.png)



类似上述模型，通常称为**预训练模型**，无法使用，通常需要设计一套解决方案，来实现需求

例如下图：针对情感分析的一套解决方案（文本分类的模型需求与设计）

![image-20250304102329201](D:\git_repository\cyber_security_learning\markdown_img\image-20250304102329201.png)

- 首先输入一段文本（比如：这件商品非常不错）
- 在上游任务准备一个基座模型，这里选择了一个BERT模型（这里需要学习：根据需求能够选择对应的模型）
  - BERT模型（上游任务或主干网络），它做两件事情
    - 将输入的文本的**特征提取**出来
      - 输出得分
      - 输出Label（0或1,0表示负向评价，1表示正向评价）
- 设计一个下游任务的模型，根据上游模型提取出来的特征，来做统计，通过给下游任务的模型喂投数据集，来训练它，让它学习相关语义，使其理解提取出的标签，来理解它的意图，从而特出输入文本时正向评价还是负向评价





### AI应用解决方案

基于大模型，了解大模型的训练原理（重要）

- 模型设计（算法），通过一些渠道拿到训练的数据（公开的，没有标注的数据），对模型进行预训练，得到一个预训练模型
- SFT：指令微调（有监督的微调，即拿着标注数据进行训练）

**标注数据**，如下图

![image-20250304103904056](D:\git_repository\cyber_security_learning\markdown_img\image-20250304103904056.png)

上述数据，即为有标注的数据，通过人工标注，将文本与0,1即正向文本和负向文本，做标注，后续让模型进行学习

大模型微调的核心：就是高质量的数据集

- RLHF：人类反馈强化学习（即对模型的输出有控制）



通用领域的预训练模型，使用在细分领域会出现局限性

其一，最严重的是会出现**幻觉**（一本正经的胡说）

当前解决方案有两种：

- **RAG**（性价比非常高的解决方案）：不需要更改基座模型，只需要生成结果去给他答案，即开卷考试，从被给予的结果集中，选出答案
- **fine tuning 微调**：需要高质量数据集，成本很高，
- **多卡的联合部署**
- **推理的加速优化**



**Agent**是 AI 应用的最合适的载体，类似于前后端分离中的前端，背后依然需要依赖于LLM大模型的能力

- Agent可以通过感知环境，自主的调用模型进行推理，自主的调用工具集完成一些任务
- 可以把 Agent 看作 AI 项目中的前端的载体



大模型的重点当前在于 RAG 和 Fine Tuning ，类似于后端





## 术语解析：

### 蒸馏技术

在**AI 领域**，**蒸馏技术（Knowledge Distillation, KD）\**是一种用于\**模型压缩**的方法，其核心思想是让一个**小模型（学生模型, Student Model）**从一个**大模型（教师模型, Teacher Model）**中学习知识，从而在保持高精度的同时降低计算开销和参数量



#### 为什么需要知识蒸馏

度学习模型通常非常庞大（如 GPT-4、BERT-Large、ViT-Huge），这些模型在训练和推理时**计算开销巨大**，对**存储、带宽和推理速度**都有很高的要求。为了让 AI 模型适用于**移动端、边缘设备、实时应用**等低资源环境，我们需要**压缩模型**，但同时要尽可能保持精度。

**知识蒸馏（KD）** 就是一种能有效压缩模型，同时保留高性能的技术。



#### 知识蒸馏的核心思想

在传统的监督学习中，我们用**标注数据**训练模型，让其输出正确的类别标签。但是，**大模型（教师模型）不仅仅提供最终的预测类别，它还包含丰富的知识，比如各类别的概率分布、特征表示等**。蒸馏技术的目标就是把这些知识提取出来，教给小模型。

知识蒸馏的关键思想：

- 训练一个 **较小的学生模型**（通常参数量更少、计算量更低）。
- 用 **大模型（教师模型）** 作为监督，帮助小模型更好地学习数据的结构和分类特征，而不是单纯依赖真实标签。
- 让小模型学习**教师模型的软目标（Soft Targets）**，即概率分布，而不是硬标签（Hard Labels）。



#### 知识蒸馏的应用

**模型压缩**：

- 将 BERT-Large 蒸馏为 DistilBERT（参数减少 40%，速度提升 60%）。
- 用 ResNet-50 训练 MobileNet，提升小模型性能。

**加速推理**：

- 服务器端：减少 GPU 计算量，加速大规模推理。
- 端侧 AI：用于智能手机、IoT 设备，提高实时性。

**迁移学习与强化学习**：

- 让小模型学习大模型在不同任务上的泛化能力。



#### **知识蒸馏 vs 其他模型压缩方法**

| **方法**                             | **原理**                                             | **优点**           | **缺点**               |
| ------------------------------------ | ---------------------------------------------------- | ------------------ | ---------------------- |
| **知识蒸馏（KD）**                   | 让小模型学习大模型的概率分布或特征                   | 能保持高精度       | 训练时间较长           |
| **剪枝（Pruning）**                  | 移除不重要的神经元或权重                             | 计算加速明显       | 可能损失部分精度       |
| **量化（Quantization）**             | 低精度计算（如 FP16、INT8）代替高精度计算（如 FP32） | 速度快，适用于硬件 | 量化后可能降低模型性能 |
| **低秩分解（Tensor Decomposition）** | 用矩阵分解方法减少计算                               | 适用于 CNN         | 计算代价高             |



### 模型训练 — 涌现

在**AI 领域**，**涌现（Emergence）** 指的是 **当模型规模、数据量或训练时间增加到一定程度后，模型会表现出一些之前无法预测的新能力或行为**，这些能力并非直接由训练目标显式设计，而是**自发涌现**的。



#### **为什么会出现涌现现象？**

深度学习模型（尤其是大规模 Transformer 模型，如 GPT、BERT、T5）在训练过程中，随着参数规模、数据量和计算资源的增长，可能会出现非线性的能力提升。例如：

- 小模型无法完成的任务，大模型可以做到。
- 任务的难度增加时，大模型的表现提升幅度远超预期。

这些现象说明了 **模型能力并不是随着规模线性增长，而是存在某些“阈值”**，当模型超越这些阈值时，会突然展现出新的能力。



#### **涌现的典型表现**

##### **(1) 语言理解能力的涌现**

在小规模模型中，可能只能处理简单的句法结构，但随着模型规模扩大，开始展现更复杂的推理能力。例如：

- 100M 参数的模型：只能执行简单的文本填空。
- 1B 参数的模型：可以理解上下文并回答基本问题。
- 10B+ 参数的模型：可以进行复杂的多步推理，甚至推断隐含信息。

##### **(2) 任务泛化能力**

模型在训练时并没有显式学习某个任务，但大模型可以在没有明确监督的情况下，**学会泛化并执行新任务**。比如：

- GPT-3 之前的模型**只能完成预定义的 NLP 任务**，需要微调（Fine-tuning）。
- GPT-3 之后的模型可以**直接通过少样本学习（Few-shot Learning）或零样本学习（Zero-shot Learning）**完成新任务，而无需额外训练。

##### **(3) 代码生成能力**

- 早期的 NLP 模型几乎无法理解和生成代码。
- 当模型规模扩展到百亿级（如 Codex, GPT-4），**开始能够自动完成代码编写、调试甚至优化**。

##### **(4) 逻辑推理能力**

- 小模型只能基于训练数据做**简单的模式匹配**。
- 大模型（如 GPT-4）已经能够**执行多步逻辑推理**，甚至解决数学题或进行规划。

##### **(5) 多模态能力**

- 传统 NLP 模型只能处理文本。
- GPT-4 这样的**大模型可以同时理解文本、图像，甚至未来可以处理音频、视频等**。



#### **涌现的关键因素**

涌现通常发生在**模型规模、数据量和训练时间**达到一定阈值后。影响涌现的主要因素包括：

1. **模型参数规模**
   - 研究表明，参数数量达到 10 亿级（Billion Scale）以上，才开始出现明显的涌现现象。
   - 例如，GPT-3（175B 参数）远远超越 GPT-2（1.5B 参数）在多任务泛化上的能力。
2. **训练数据规模**
   - **多样性** 和 **质量** 影响模型的能力。更大的语料库（如 Common Crawl, Wikipedia, GitHub 代码等）提供了更多泛化能力。
3. **计算资源**
   - 训练过程中的计算资源限制（如 TPU/GPU 计算能力）会影响模型的学习能力。
   - 例如，Google 的 PaLM 模型（540B 参数）在使用大量计算资源后，展现出了超越 GPT-3 的能力。



#### **涌现的挑战**

虽然涌现带来了强大的 AI 能力，但它也带来了一些挑战：

1. **难以解释**

   - 目前无法精确预测**哪些能力会涌现**，也不清楚其**具体机制**。
   - 为什么 GPT-4 能解决复杂数学问题，而 GPT-3 不能？目前还没有理论解释。

2. **数据和算力的成本高昂**

   - 大规模训练（如 GPT-4 训练成本可能数千万美元）使得涌现现象只在少数顶级 AI 机构（OpenAI、Google DeepMind、Anthropic）能够研究。

3. **不可控性**

   - 由于涌现现象难以预测，可能会导致

     模型产生意外行为

     ，例如：

     - 生成**虚假信息**（Hallucination）。
     - 过度**拟合训练数据**，导致安全风险。



#### **未来展望**

随着 AI 领域的发展，涌现现象可能会进一步推动：

1. **更高效的 AI 训练**
   - 研究如何在小模型中复现大模型的涌现能力，如使用**知识蒸馏**等技术。
2. **更好的理论理解**
   - 研究人员正在尝试构建数学理论，解释涌现的机制，例如 **Scaling Laws** 和 **相变（Phase Transition）** 现象。
3. **通用人工智能（AGI）**
   - 许多研究者认为，大规模模型的涌现能力可能是迈向 AGI（通用人工智能）的关键一步。







### 模型训练 — 泛化

在**模型训练**中，**泛化（Generalization）\**指的是\**模型在未见过的新数据上的表现能力**。换句话说，**一个模型的泛化能力越强，它就越能在测试集或真实世界的数据上保持良好的预测性能，而不仅仅是在训练集上表现良好。**



#### **为什么泛化很重要？**

机器学习的最终目标是**学习数据的潜在模式**，并在**新数据**上做出准确的预测。如果一个模型只能在训练数据上表现很好，而在新数据上效果很差，那说明它**过拟合（Overfitting）**了，没有学到真正的规律。

理想情况下，模型应该：

- 在训练数据上表现良好（低训练误差）。
- 在测试数据上表现良好（低测试误差）。
- 不会记住训练集的具体样本，而是学到**数据的底层模式**。

**泛化能力是衡量机器学习模型是否真正学会了数据的内在结构，而不仅仅是“记住”数据。**



#### **泛化的衡量指标**

##### **(1) 训练误差 vs. 测试误差**

泛化能力通常通过**训练误差（Training Error）**和**测试误差（Test Error）**的对比来衡量：

- **训练误差**：模型在训练数据上的误差。
- **测试误差**：模型在未见过的数据（测试集）上的误差。

如果训练误差很低，但测试误差很高，说明模型在训练集上过拟合了，而泛化能力较差。

##### **(2) 交叉验证（Cross Validation）**

- 通过**K 折交叉验证（K-Fold Cross Validation）**，可以在不同的数据划分上测试模型，检查其泛化能力是否稳定。

##### **(3) 偏差-方差权衡（Bias-Variance Tradeoff）**

泛化能力与**偏差（Bias）和方差（Variance）**密切相关：

- **高偏差（Bias，高训练误差）**：模型过于简单，无法捕捉数据的复杂模式，称为**欠拟合（Underfitting）**。
- **高方差（Variance，高测试误差）**：模型过于复杂，过度拟合训练数据，导致在新数据上表现不佳，称为**过拟合（Overfitting）**。

良好的泛化能力意味着**适当的偏差-方差平衡**，即：



#### **泛化的影响因素**

模型的泛化能力受到多个因素影响：

##### **(1) 数据质量**

- **训练数据的多样性**：如果训练数据只涵盖一小部分情况，模型可能无法泛化到真实世界的多种情况。
- **数据清洗**：错误或噪声数据会影响泛化能力。

##### **(2) 模型复杂度**

- **模型太简单**（如线性回归）可能会欠拟合，导致泛化能力不足。
- **模型太复杂**（如深度神经网络）可能会过拟合，需要正则化。

#### **(3) 训练数据量**

- **更多的数据**通常有助于提升泛化能力，但前提是数据质量要高。

##### **(4) 训练方式**

- **数据增强（Data Augmentation）**：通过增加数据的多样性提高泛化能力（如图像翻转、旋转、颜色变换）。
- 正则化（Regularization）：
  - **L1/L2 正则化**（如 Lasso, Ridge）防止权重过大，减少过拟合。
  - **Dropout**（神经网络中的随机失活）防止网络过度依赖特定神经元。
  - **Batch Normalization** 让训练更加稳定，提高泛化能力。

##### **(5) 训练时间**

- 训练时间过长可能会导致过拟合（memorization of training data）。
- **Early Stopping（提前停止）**可以在验证集误差开始增加时停止训练，以防止过拟合。



#### **泛化的典型问题**

##### **(1) 过拟合（Overfitting）**

- **表现**：训练误差低，测试误差高。
- 原因：
  - 模型太复杂
  - 训练数据太少
  - 训练时间过长
- 解决方案：
  - 增加训练数据
  - 使用正则化（L1/L2, Dropout）
  - 早停（Early Stopping）

##### **(2) 欠拟合（Underfitting）**

- **表现**：训练误差高，测试误差也高。
- 原因：
  - 模型太简单
  - 训练数据不足
- 解决方案：
  - 增加模型复杂度（如使用更深的神经网络）
  - 训练更长时间



#### **泛化在深度学习中的应用**

在**深度学习**中，提升泛化能力的技巧包括：

- **使用更大规模的数据集**（如 ImageNet）
- **迁移学习（Transfer Learning）**：利用在大数据上训练的预训练模型（如 BERT、GPT）
- **数据增强（Data Augmentation）**：对输入数据进行变换，如图像翻转、旋转等
- **正则化技术**（如 Dropout, Batch Normalization）
- **减少神经网络的参数量**（如模型蒸馏 Knowledge Distillation）





## 知识补充



### 实际生产环境下，中文语境模型建议

在当前的实际生产环境下，建议使用 **Qwen2.5 7B** 或 **InternLM**



