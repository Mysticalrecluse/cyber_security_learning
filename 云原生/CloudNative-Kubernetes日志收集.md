# Kubernetesæ—¥å¿—æ”¶é›†



## æ—¥å¿—æ”¶é›†ç®€ä»‹



**æ—¥å¿—æ”¶é›†çš„ç›®çš„**

- åˆ†å¸ƒå¼æ—¥å¿—æ•°æ®ç»Ÿä¸€æ”¶é›†ï¼Œå®ç°é›†ä¸­å¼æŸ¥è¯¢å’Œç®¡ç†
- æ•…éšœæ’æŸ¥
- å®‰å…¨ä¿¡æ¯å’Œäº‹ä»¶ç®¡ç†
- æŠ¥è¡¨ç»Ÿè®¡åŠå±•ç¤ºåŠŸèƒ½



**æ—¥å¿—æ”¶é›†çš„ä»·å€¼**

- æ—¥å¿—æŸ¥è¯¢ï¼Œé—®é¢˜æ’æŸ¥ï¼Œæ•…éšœæ¢å¤ï¼Œæ•…éšœè‡ªæ„ˆ
- åº”ç”¨æ—¥å¿—åˆ†æï¼Œé”™è¯¯æŠ¥è­¦
- æ€§èƒ½åˆ†æï¼Œç”¨æˆ·è¡Œä¸ºåˆ†æ



### æ—¥å¿—æ”¶é›†æµç¨‹

![image-20250504142022011](../markdown_img/image-20250504142022011.png)

### æ—¥å¿—æ”¶é›†æ–¹å¼ç®€ä»‹

```http
https://kubernetes.io/zh/docs/concepts/cluster-administration/logging/
```



1. node èŠ‚ç‚¹æ”¶é›†ï¼ŒåŸºäºdaemonsetéƒ¨ç½²æ—¥å¿—æ”¶é›†è¿›ç¨‹ï¼Œå®ç°json-fileç±»å‹ï¼ˆæ ‡å‡†è¾“å‡º/dev/stdoutï¼Œé”™è¯¯è¾“å‡º/dev/stderrï¼‰æ—¥å¿—æ”¶é›†
   - å…¬æœ‰äº‘é€šå¸¸è¿™ç§æ–¹å¼ç”¨çš„æ¯”è¾ƒå¤šï¼Œç®€åŒ–ç”¨æˆ·çš„ä½¿ç”¨æˆæœ¬

2. ä½¿ç”¨sidecarå®¹å™¨ï¼ˆä¸€ä¸ªPodå¤šå®¹å™¨ï¼‰æ”¶é›†å½“å‰Podå†…ä¸€ä¸ªæˆ–å¤šä¸ªä¸šåŠ¡å®¹å™¨çš„æ—¥å¿—ï¼ˆé€šå¸¸åŸºäºemptyDirå®ç°ä¸šåŠ¡å®¹å™¨ä¸sidecarä¹‹é—´çš„æ—¥å¿—å…±äº«ï¼‰
3. åœ¨å®¹å™¨å†…ç½®æ—¥å¿—æ”¶é›†æœåŠ¡è¿›ç¨‹



## æ—¥å¿—æ”¶é›†ç¤ºä¾‹

### æ—¥å¿—ç¤ºä¾‹â€”daemonsetæ”¶é›†æ—¥å¿—

åŸºäºdaemonsetè¿è¡Œæ—¥å¿—æ”¶é›†æœåŠ¡ï¼Œä¸»è¦æ”¶é›†ä¸€ä¸‹ç±»å‹æ—¥å¿—ï¼š

- nodeèŠ‚ç‚¹æ”¶é›†ï¼ŒåŸºäºdaemonsetéƒ¨ç½²æ—¥å¿—æ”¶é›†è¿›ç¨‹ï¼Œå®ç°json-fileç±»å‹ï¼ˆæ ‡å‡†è¾“å‡º/dev/stdoutï¼Œé”™è¯¯è¾“å‡º/dev/stderrï¼‰æ—¥å¿—æ”¶é›†ï¼Œå³åº”ç”¨ç¨‹åºäº§ç”Ÿçš„æ ‡å‡†è¾“å‡ºå’Œé”™è¯¯è¾“å‡ºçš„æ—¥å¿—
- å®¿ä¸»æœºç³»ç»Ÿæ—¥å¿—ç­‰ä»¥æ—¥å¿—æ–‡ä»¶å½¢å¼ä¿å­˜çš„æ—¥å¿—

| å¯¹æ¯”ç±»å‹     | containerd                                                   | docker                                                       |
| ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| æ—¥å¿—å­˜å‚¨è·¯å¾„ | çœŸå®è·¯å¾„ï¼š/var/log/pods/$container_name  # çœŸå®è·¯å¾„<br />è½¯é“¾æ¥ï¼šåŒæ—¶kubeletä¹Ÿä¼šåœ¨/var/log/contaiersç›®å½•ä¸‹åˆ›å»ºè½¯è¿æ¥æŒ‡å‘/var/log/pods/$contaienr_name | çœŸå®è·¯å¾„ï¼š/var/lib/contaienrs/$containerd<br />è½¯é“¾æ¥ï¼škubeletä¼šåœ¨/var/log/poså’Œ/var/log/contgainersåˆ›å»ºè½¯è¿æ¥æŒ‡å‘/var/lib/docker/containers/$CONTAINERID |
| æ—¥å¿—é…ç½®å‚æ•° | é…ç½®æ–‡ä»¶ï¼š/etc/systemd/system/kubelet.service<br />é…ç½®å‚æ•°ï¼š<br />- --container-log-max-files=5 \ <br />   --container-log-max-size="100Mi" \ <br />   --logging-forma="json" | é…ç½®æ–‡ä»¶ï¼š/etc/docker/daemon.json<br />å‚æ•°ï¼š"log-driver": "json-file"<br />"log-opts": {<br />    "max-file": "5",<br />    "max-size": "100m"<br />} |



#### æ—¥å¿—ç¤ºä¾‹â€”daemonsetæ”¶é›†æ—¥å¿—æ¶æ„

![image-20250504164732851](../markdown_img/image-20250504164732851.png)

#### æ—¥å¿—ç¤ºä¾‹â€”daemonsetæ”¶é›†jsonfileæ—¥å¿—-éƒ¨ç½²webæœåŠ¡



**åŸºç¡€ç¯å¢ƒ**

- **zookeeper && kafka**
- **elasticsearch cluster**
- **logstash**
- **kibana**



**å‰æå‡†å¤‡**

```bash
# éƒ¨ç½²å¥½ELasticsearché›†ç¾¤å’Œkafkaé›†ç¾¤
```



**æ„å»ºlogstashé•œåƒ**

```bash
# æŸ¥çœ‹å‡†å¤‡æ–‡ä»¶
[root@master1 ~/ELK-case/daemonset-logstash]# pwd
/root/ELK-case/daemonset-logstash

# æŸ¥çœ‹ç›®å½•ä¸‹æ–‡ä»¶
[root@master1 ~/ELK-case/daemonset-logstash]# ls
1.logstash-image-Dockerfile  2.DaemonSet-logstash.yaml  3.logstash-daemonset-jsonfile-kafka-to-es.conf

# è¿›å…¥é•œåƒæ„å»ºç›®å½•
[root@master1 ~/ELK-case/daemonset-logstash]# cd 1.logstash-image-Dockerfile/
[root@master1 ~/ELK-case/daemonset-logstash/1.logstash-image-Dockerfile]# ls
Dockerfile  command.sh  logstash.conf  logstash.yml

# æŸ¥çœ‹é•œåƒæ„å»ºæ–‡ä»¶
[root@master1 ~/ELK-case/daemonset-logstash/1.logstash-image-Dockerfile]# cat Dockerfile 
FROM harbor.magedu.mysticalrecluse.com/k8simage/logstash:7.12.1

USER root
WORKDIR /usr/share/logstash
ADD logstash.yml /usr/share/logstash/config/logstash.yml
ADD logstash.conf /usr/share/logstash/pipeline/logstash.conf

# æŸ¥çœ‹ç›®å½•ä¸‹å…¶ä»–é…ç½®æ–‡ä»¶
[root@master1 ~/ELK-case/daemonset-logstash/1.logstash-image-Dockerfile]# cat logstash.conf 
input {
  file {
    #path => "/var/lib/docker/containers/*/*-json.log" #docker
    path => "/var/log/pods/*/*/*.log"
    start_position => "beginning"
    type => "jsonfile-daemonset-applog"
  }
  file {
    path => "/var/log/*.log"
    start_position => "beginning"
    type => "jsonfile-daemonset-syslog"
  }
}

output {
  if [type] == "jsonfile-daemonset-applog" {
    kafka {
      bootstrap_servers => "${KAFKA_SERVER}"
      topic_id => "${TOPIC_ID}"
      batch_size => 16384
      codec => "${CODEC}"
    }
  }

  if [type] == "jsonfile-daemonset-syslog" {
    kafka {
      bootstrap_servers => "${KAFKA_SERVER}"
      topic_id => "${TOPIC_ID}"
      batch_size => 16384
      codec => "${CODEC}" # ç³»ç»Ÿæ—¥å¿—ä¸æ˜¯jsonæ ¼å¼
    }
  }
}

[root@master1 ~/ELK-case/daemonset-logstash/1.logstash-image-Dockerfile]# cat logstash.yml 
http.host: "0.0.0.0"
#xpack.monitoring.elasticsearch.hosts: ["http://elasticsearch:9200"]

[root@master1 ~/ELK-case/daemonset-logstash/1.logstash-image-Dockerfile]# cat command.sh 
#!/bin/bash

nerdctl build -t harbor.magedu.mysticalrecluse.com/k8simage/logstash:v7.12.1-json-file-log-v1 .
nerdctl push harbor.magedu.mysticalrecluse.com/k8simage/logstash:v7.12.1-json-file-log-v1

# æ„å»ºé•œåƒ
# æ„å»ºçš„æ—¶å€™ï¼Œå¦‚æœæŠ¥é”™ï¼Œè¯´æ²¡æ‰¾åˆ°buildkitå¯ä»¥æ‰§è¡Œ buildkitd &ï¼Œåå°è¿è¡Œbuildkit
[root@master1 ~/ELK-case/daemonset-logstash/1.logstash-image-Dockerfile]# bash command.sh
```



**å¯åŠ¨logstashçš„pod**

```bash
# æŸ¥çœ‹æ¸…å•æ–‡ä»¶
[root@master1 ~/ELK-case/daemonset-logstash]$ cat 2.DaemonSet-logstash.yaml 
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: logstash-elasticsearch
  namespace: kube-logging
  labels:
    k8s-app: logstash-logging
spec:
  selector:
    matchLabels:
      name: logstash-elasticsearch
  template:
    metadata:
      labels:
        name: logstash-elasticsearch
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      hostAliases:
      - ip: "10.2.1.139"
        hostnames:
        - "harbor"
      containers:
      - name: logstash-elasticsearch
        image: harbor.magedu.mysticalrecluse.com/k8simage/logstash:v7.12.1-json-file-log-v1
        env:
        - name: "KAFKA_SERVER"
          value: "10.2.1.139:9092"
        - name: "TOPIC_ID"
          value: "jsonfile-log-topic"
        - name: "CODEC"
          value: "json"
        volumeMounts:
        - name: varlog  # å®šä¹‰å®¿ä¸»æœºç³»ç»Ÿæ—¥å¿—æŒ‚è½½è·¯å¾„
          mountPath: /var/log # å®¿ä¸»æœºç³»ç»Ÿæ—¥å¿—æŒ‚è½½ç‚¹
        - name: varlibdockercontainers # å®šä¹‰å®¹å™¨æ—¥å¿—æŒ‚è½½è·¯å¾„ï¼Œå’ŒLogstashé…ç½®æ–‡ä»¶ä¸­çš„æ”¶é›†è·¯å¾„ä¿æŒä¸€è‡´
          mountPath: /var/log/pods  # containerdæŒ‚è½½è·¯å¾„ï¼Œï¼Œæ­¤è·¯å¾„ä¸logstashçš„æ—¥å¿—æ”¶é›†è·¯å¾„å¿…é¡»ä¸€è‡´
          readOnly: false
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/log/pods
          
#å¯ç”¨æ¸…å•æ–‡ä»¶
[root@master1 ~/ELK-case/daemonset-logstash]# kubectl apply -f 2.DaemonSet-logstash.yaml
daemonset.apps/logstash-elasticsearch created

# ä½¿ç”¨offset ExploreræŸ¥çœ‹ï¼Œæ—¥å¿—æ”¶é›†æˆåŠŸ
```

![image-20250505000259162](../markdown_img/image-20250505000259162.png)

å°†å±æ€§è°ƒä¸ºstring,æ›´æ–°åæŸ¥çœ‹å…·ä½“æ—¥å¿—ä¿¡æ¯

![image-20250505000742091](../markdown_img/image-20250505000742091.png)

![image-20250505000636456](../markdown_img/image-20250505000636456.png)



éƒ¨ç½²Logstashï¼Œä»Kafkaæ¶ˆè´¹æ—¥å¿—ï¼Œå¹¶ä¼ é€’ç»™Elasticsearchå¤„ç†

```bash
[root@haproxy-dns-etc]# apt update && apt install -y openjdk-11-openjdk
[root@haproxy-dns-etc]# wget https://mirrors.aliyun.com/elasticstack/8.x/apt/pool/main/l/logstash/logstash-8.6.1-amd64.deb
[root@haproxy-dns-etc]# dpkg -i logstash-8.6.1-amd64.deb
[root@haproxy-dns-etc]# systemctl enable --now logstash.service

# ç¼–å†™é…ç½®æ–‡ä»¶
[root@haproxy-dns-etc]$ cat /etc/logstash/conf.d/logstash-daemonset-jsonfile-kafka-to-es.conf
input {
  kafka {
    bootstrap_servers => "10.2.1.139:9092"
    topics => ["jsonfile-log-topic"]
    codec => "json"
  }
}

output {
  if [type] == "jsonfile-daemonset-applog" {
    elasticsearch {
      hosts => ["10.2.1.139:9200"]
      index => "jsonfile-daemonset-applog-%{+YYYY.MM.dd}"
    }
  }
  if [type] == "jsonfile-daemonset-syslog" {
    elasticsearch {
      hosts => ["10.2.1.139:9200"]
      index => "jsonfile-daemonset-syslog-%{+YYYY.MM.dd}"
    }
  }
}

# é‡å¯
[root@haproxy-dns-etc]# systemctl restart logstash.service

# æŸ¥çœ‹ELasticsearchä¸Šçš„æ•°æ®
# æ³¨æ„ï¼šåœ¨å•æœºElasticsearchä¸­ï¼Œä¼šå‡ºç°ä¸€ä¸ªç°è±¡
```

![image-20250505003515759](../markdown_img/image-20250505003515759.png)

**é»„ç‰Œï¼ˆYellowï¼‰æ„å‘³ç€å‰¯æœ¬åˆ†ç‰‡æœªåˆ†é…**

- ä½ æœ‰ **1 ä¸ªä¸»åˆ†ç‰‡ï¼ˆprimary shardï¼‰å·²ç»æˆåŠŸåˆ†é…å¹¶è¿è¡Œ**ï¼Œæ‰€ä»¥èƒ½å†™å…¥æ•°æ®ã€‚
- ä½†å¯¹åº”çš„ **å‰¯æœ¬åˆ†ç‰‡ï¼ˆreplica shardï¼‰æœªèƒ½åˆ†é…åˆ°å…¶ä»–èŠ‚ç‚¹**ï¼Œå› æ­¤é›†ç¾¤å¥åº·çŠ¶æ€ä¸º `yellow`ã€‚



**ä¸ºä»€ä¹ˆæ˜¯ Yellowï¼Ÿ**

å› ä¸ºä½ è¿è¡Œçš„æ˜¯**å•èŠ‚ç‚¹**ï¼ˆå¦‚å›¾ä¸­åªæœ‰ `harbor-minio-etc` ä¸€å°æœºå™¨ï¼‰ï¼Œè€Œé»˜è®¤æƒ…å†µä¸‹æ¯ä¸ªç´¢å¼•éƒ½æœ‰å‰¯æœ¬ï¼ˆ`number_of_replicas: 1`ï¼‰ï¼Œ**è€Œå‰¯æœ¬ä¸èƒ½ä¸ä¸»åˆ†ç‰‡éƒ¨ç½²åœ¨åŒä¸€ä¸ªèŠ‚ç‚¹ä¸Š**ï¼Œæ‰€ä»¥å‰¯æœ¬å°±ä¼šå¤„äº `Unassigned` çŠ¶æ€ã€‚



**å¦‚ä½•è®©å®ƒå˜ä¸º Greenï¼ˆå…¨ç»¿ï¼‰**

æ–¹æ³•ä¸€ï¼šé™ä½å‰¯æœ¬æ•°ä¸º 0ï¼ˆé€‚åˆå•èŠ‚ç‚¹éƒ¨ç½²ï¼‰

```bash
[root@harbor-minio-etc]# curl -X PUT "http://10.2.1.139:9200/jsonfile-daemonset-applog-2025.05.04/_settings" -H 'Content-Type: application/json' -d '{
  "index": {
    "number_of_replicas": 0
  }
}'
{"acknowledged":true}
```

ç„¶ååˆ·æ–°é¡µé¢ï¼Œ**é›†ç¾¤çŠ¶æ€å°±ä¼šå˜æˆ Greenï¼ˆç»¿è‰²ï¼‰**

![image-20250505003729873](../markdown_img/image-20250505003729873.png)

æµ‹è¯•åªæ”¶é›†åˆ°äº†`jsonfile-daemonset-applog-2025.05.04`çš„æ—¥å¿—ï¼Œè¿˜æ²¡æœ‰syslog

```bash
# åœ¨ä»»æ„workerèŠ‚ç‚¹æ‰§è¡Œ
[root@work1]# logger "hello"

# åˆ·æ–°ELasticsearch Headï¼ŒæŸ¥çœ‹
```

![image-20250505004255007](../markdown_img/image-20250505004255007.png)

æŸ¥è¯¢å‘é€çš„helloæ—¥å¿—æ•°æ®

![image-20250505004645464](../markdown_img/image-20250505004645464.png)

é™ä½å‰¯æœ¬æ•°ä¸º 0ï¼Œå°†é›†ç¾¤å˜ä¸ºç»¿è‰²

```bash
[root@harbor-minio-etc]# curl -X PUT "http://10.2.1.139:9200/jsonfile-daemonset-syslog-2025.05.04/_settings" -H 'Content-Type: application/json' -d '{
  "index": {
    "number_of_replicas": 0
  }
}'
{"acknowledged":true}

# æŸ¥çœ‹ç»“æœ
```

![image-20250505004829102](../markdown_img/image-20250505004829102.png)



**éƒ¨ç½²Kibana**

```bash
[root@harbor-minio-etc]# wget https://mirrors.aliyun.com/elasticstack/8.x/apt/pool/main/k/kibana/kibana-8.15.0-amd64.deb

[root@harbor-minio-etc]# dpkg -i kibana-8.15.0-amd64.deb
#é»˜è®¤æ²¡æœ‰å¼€æœºè‡ªåŠ¨å¯åŠ¨ï¼Œéœ€è¦è‡ªè¡Œè®¾ç½®
[root@harbor-minio-etc]# dpkg -i kibana-8.15.0-amd64.deb
(Reading database ... 110454 files and directories currently installed.)
Preparing to unpack kibana-8.15.0-amd64.deb ...
Unpacking kibana (8.15.0) over (8.15.0) ...
Setting up kibana (8.15.0) ...
Creating kibana group... OK
Creating kibana user... OK
Kibana is currently running with legacy OpenSSL providers enabled! For details and instructions on how to disable see h
ttps://www.elastic.co/guide/en/kibana/8.15/production.html#openssl-legacy-provider
Created Kibana keystore in /etc/kibana/kibana.keystore

# ä¿®æ”¹é…ç½®æ–‡ä»¶
[root@es-node1 ~]#vim /etc/kibana/kibana.yml 
[root@es-node1 ~]#grep "^[a-Z]" /etc/kibana/kibana.yml 

server.port: 5601  #ç›‘å¬ç«¯å£,æ­¤ä¸ºé»˜è®¤å€¼
server.host: "0.0.0.0" #ä¿®æ”¹æ­¤è¡Œçš„ç›‘å¬åœ°å€,é»˜è®¤ä¸ºlocalhostï¼Œå³ï¼š127.0.0.1:5601

#ä¿®æ”¹æ­¤è¡Œ,æŒ‡å‘ESä»»æ„æœåŠ¡å™¨åœ°å€æˆ–å¤šä¸ªèŠ‚ç‚¹åœ°å€å®ç°å®¹é”™,é»˜è®¤ä¸ºlocalhost
elasticsearch.hosts: 
["http://10.0.0.101:9200","http://10.0.0.102:9200","http://10.0.0.103:9200"] 

i18n.locale: "zh-CN"   #ä¿®æ”¹æ­¤è¡Œ,ä½¿ç”¨"zh-CN"æ˜¾ç¤ºä¸­æ–‡ç•Œé¢,é»˜è®¤è‹±æ–‡

#8.Xç‰ˆæœ¬æ–°æ·»åŠ é…ç½®,é»˜è®¤è¢«æ³¨é‡Š,ä¼šæ˜¾ç¤ºä¸‹é¢æç¤º
server.publicBaseUrl: "http://kibana.mystical.org"

# æµè§ˆå™¨è®¿é—®ï¼šhttp://kibana.mystical.org:5601
```

![image-20250505011210542](../markdown_img/image-20250505011210542.png)

**é…ç½®ç´¢å¼•**

![image-20250505011730515](../markdown_img/image-20250505011730515.png)

**åˆ›å»ºæ•°æ®è§†å›¾**

![image-20250505011948507](../markdown_img/image-20250505011948507.png)

![image-20250505012059817](../markdown_img/image-20250505012059817.png)

![image-20250505012147566](../markdown_img/image-20250505012147566.png)

åŸºäºåˆšæ‰åˆ›å»ºçš„æ•°æ®è§†å›¾ï¼ŒæŸ¥çœ‹æ•°æ®

![image-20250505012241181](../markdown_img/image-20250505012241181.png)

![image-20250505012333620](../markdown_img/image-20250505012333620.png)



### æ—¥å¿—ç¤ºä¾‹â€”sidecaræ¨¡å¼æ¶æ„

![image-20250505171941048](../markdown_img/image-20250505171941048.png)









### æ—¥å¿—ç¤ºä¾‹â€”å®Œå…¨åŸºäºK8Séƒ¨ç½²çš„æ—¥å¿—é‡‡é›†ç³»ç»Ÿ

#### åŸºç¡€ç¯å¢ƒå‡†å¤‡

åœ¨å®‰è£…ELasticsearché›†ç¾¤ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆåˆ›å»ºä¸€ä¸ªåç§°ç©ºé—´ï¼Œæˆ‘ä»¬ä¹‹åéƒ¨ç½²çš„ç»„ä»¶å°†åœ¨è¿™ä¸ªå‘½åç©ºé—´ä¸‹

**åˆ›å»ºèµ„æºå­˜æ”¾ä½ç½®**

```bash
[root@master1 ~]# mkdir logging/namespace -p
[root@master1 ~]# cd logging/namespace/
```



**åˆ›å»ºloggingå‘½åç©ºé—´**

```bash
# åˆ›å»ºèµ„æºæ¸…å•æ–‡ä»¶
[root@master1 ~/logging/namespace]# vim logging-namespace.yaml

# æ›´æ–°èµ„æºæ¸…å•æ–‡ä»¶
[root@master1 ~/logging/namespace]# kubectl apply -f logging-namespace.yaml 
namespace/logging created

# æŸ¥çœ‹åˆ›å»ºçš„loggingåç§°ç©ºé—´
[root@master1 ~/logging/namespace]$ kubectl get ns logging 
NAME      STATUS   AGE
logging   Active   100s
```



**é…ç½®é»˜è®¤å­˜å‚¨**

æˆ‘ä»¬åé¢éƒ¨ç½²çš„åº”ç”¨å¯èƒ½éœ€è¦å°†æ•°æ®ç›®å½•æŒä¹…åŒ–å‡ºå»ï¼Œå¦‚æœä¸åšæŒä¹…åŒ–ï¼Œå®¹å™¨å‘ç”Ÿé‡å¯ï¼Œæ•°æ®å°±ä¼šä¸¢å¤±

**å®‰è£…NFS Serverå¹¶åˆ›å»ºæ•°æ®å­˜æ”¾ç›®å½•**

```bash
[root@haproxy-dns-etc]# apt update && apt -y install nfs-server

root@haproxy-dns-etc]# systemctl status nfs-server
â— nfs-server.service - NFS server and services
     Loaded: loaded (/lib/systemd/system/nfs-server.service; enabled; vendor preset: enabled)
     Active: active (exited) since Mon 2025-05-05 10:42:23 UTC; 26s ago
   Main PID: 78911 (code=exited, status=0/SUCCESS)
        CPU: 16ms

May 05 10:42:21 haproxy-dns-etc systemd[1]: Starting NFS server and services...
May 05 10:42:21 haproxy-dns-etc exportfs[78910]: exportfs: can't open /etc/exports for reading
May 05 10:42:23 haproxy-dns-etc systemd[1]: Finished NFS server and services.

[root@master1 ~]# mkdir -pv /data/sc-nfs 
[root@master1 ~]# chown 777 /data/sc-nfs
[root@master1 ~]# vim /etc/exports
#æˆæƒworkerèŠ‚ç‚¹çš„ç½‘æ®µå¯ä»¥æŒ‚è½½
#/data/sc-nfs *(rw,no_root_squash,all_squash,anonuid=0,anongid=0) 
/data/sc-nfs *(rw,no_root_squash) 

[root@master1 ~]# exportfs -r
[root@master1 ~]# exportfs -v
/data/sc-nfs <world>
(sync,wdelay,hide,no_subtree_check,anonuid=0,anongid=0,sec=sys,rw,secure,no_root_squash,all_squash)

#å¹¶åœ¨æ‰€æœ‰workerèŠ‚ç‚¹å®‰è£…NFSå®¢æˆ·ç«¯ 
[root@nodeX ~]# apt update && apt -y install nfs-common æˆ–è€… nfs-client
```



**åˆ›å»ºServiceAccountå¹¶æˆæƒ**

```yaml
[root@master1 yaml] # cat rbac.yaml 
# åˆ›å»ºç‹¬ç«‹çš„åç§°ç©ºé—´
apiVersion: v1
kind: Namespace
metadata:
  name: nfs-provisioner-demo
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-client-provisioner
  # replace with namespace where provisioner is deployed æ ¹æ®ä¸šåŠ¡éœ€è¦ä¿®æ”¹æ­¤å¤„åç§°ç©ºé—´
  namespace: nfs-provisioner-demo
  
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: nfs-client-provisioner-runner
rules:
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "update", "patch"]
  - apiGroups: [""]
    resources: ["services", "endpoints"]
    verbs: ["get", "list", "watch", "create", "update", "delete"]
    
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: run-nfs-client-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: nfs-provisioner-demo
roleRef:
  kind: ClusterRole
  name: nfs-client-provisioner-runner
  apiGroup: rbac.authorization.k8s.io
  
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: leader-locking-nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: nfs-provisioner-demo
rules:
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]
    
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: nfs-provisioner-demo
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: nfs-provisioner-demo
roleRef:
  kind: Role
  name: leader-locking-nfs-client-provisioner
  apiGroup: rbac.authorization.k8s.io


# åº”ç”¨
[root@master1 yaml] # kubectl apply -f rbac.yaml
serviceaccount/nfs-client-provisioner created
clusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner created
clusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created
role.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created
rolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created

# æŸ¥çœ‹ç³»ç»Ÿç”¨æˆ·
[root@master1 yaml]#kubectl get sa
NAME                     SECRETS   AGE
default                  0         34d
nfs-client-provisioner   0         9s
```



**éƒ¨ç½² NFS-Subdir-External-Provisioner å¯¹åº”çš„ Deployment**

```yaml
[root@master1 nsf-provisioner] #vim nfs-client-provisioner.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-client-provisioner
  labels:
    app: nfs-client-provisioner
  namespace: nfs-provisioner-demo
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: nfs-client-provisioner
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
      - name: nfs-client-provisioner     
        image: k8s.gcr.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2 #æ­¤é•œåƒå›½å†…å¯èƒ½æ— æ³•è®¿é—®
        imagePullPolicy: IfNotPresent
        volumeMounts:
        - name: nfs-client-root
          mountPath: /persistentvolumes
        env:
        - name: PROVISIONER_NAME
          value: k8s-sigs.io/nfs-subdir-external-provisioner # åç§°ç¡®ä¿ä¸nfs-StorageClass.yamlæ–‡ä»¶ä¸­çš„provisioneråç§°ä¿æŒä¸€è‡´
        - name: NFS_SERVER
          value: nfs.mystical.org
        - name: NFS_PATH
          value: /nfs-data/sc-nfs
      volumes:
      - name: nfs-client-root
        nfs:
          server: nfs.mystical.org
          path: /nfs-data/sc-nfs
          
# åº”ç”¨
[root@master1 nsf-provisioner]# kubectl apply -f nfs-client-provisioner.yaml 
deployment.apps/nfs-client-provisioner created

# æŸ¥çœ‹
[root@master1 nsf-provisioner]#kubectl get pod -n nfs-provisioner-demo 
NAME                                      READY   STATUS    RESTARTS   AGE
nfs-client-provisioner-74d7c6bf46-kkpmd   1/1     Running   0          4m9s
```



**åˆ›å»ºNFSèµ„æºçš„storageClass**

```bash
[root@master1 nsf-provisioner] # vim nfs-storageClass.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: sc-nfs
  annotations:
    storageclass.kubernetes.io/is-default-class: "false" # æ˜¯å¦è®¾ç½®ä¸ºé»˜è®¤çš„storageClass
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner # or choose another name, must match deployment's env PROVISIONER_NAME
parameters:
  archiveOnDelete: "true" # è®¾ç½®ä¸ºfalseæ—¶åˆ é™¤PVCä¸ä¼šä¿ç•™æ•°æ®ï¼Œ"true"åˆ™ä¿ç•™æ•°æ®ï¼ŒåŸºäºå®‰å…¨åŸå› å»ºè®®è®¾ä¸º"true"


# åº”ç”¨
[root@master1 nsf-provisioner] # kubectl apply -f nfs-storageClass.yaml 
storageclass.storage.k8s.io/sc-nfs created

# æŸ¥çœ‹
[root@master1 nsf-provisioner]#kubectl get sc -n nfs-provisioner-demo 
NAME     PROVISIONER                                   RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
sc-nfs   k8s-sigs.io/nfs-subdir-external-provisioner   Delete          Immediate           false                  15s
```



#### éƒ¨ç½²ELasticsearch

ä½¿ç”¨StatefulSetéƒ¨ç½²Elasticsearché›†ç¾¤å¯ä»¥æä¾›èŠ‚ç‚¹ä¹‹é—´çš„ç¨³å®šç½‘ç»œæ ‡è¯†ï¼Œæœ‰åºçš„éƒ¨ç½²å’Œæ‰©å±•ã€æŒä¹…åŒ–å­˜å‚¨å’ŒçŠ¶æ€ç®¡ç†åŠŸèƒ½ã€‚è¿™äº›åŠŸèƒ½ä½¿å¾—Elasticsearchåœ¨Kubernetesä¸Šæ›´åŠ å¯é ï¼Œæ˜“äºç®¡ç†ï¼Œå¹¶ä¿è¯æ•°æ®çš„å¯é æ€§å’Œå¯ç”¨æ€§

**åˆ›å»ºHeadless Service**

```bash
[root@master1 ~]# mkdir /root/logging/elasticsearch
[root@master1 ~]# cd /root/logging/elasticsearch/
[root@master1 ~/logging/elasticsearch]# vim elasticsearch-svc.yaml
kind: Service
apiVersion: v1
metadata:
  name: elasticsearch
  namespace: logging
  labels:
    app: elasticsearch
spec:
  selector:
    app: elasticsearch
  clusterIP: None
  ports:
    - name: tcp-9200
      port: 9200
    - name: tcp-9300
      port: 9300
      
# æ›´æ–°èµ„æºæ¸…å•æ–‡ä»¶
[root@master1 ~/logging/elasticsearch]# kubectl apply -f elasticsearch-svc.yaml 
service/elasticsearch created

# æŸ¥çœ‹
[root@master1 ~/logging/elasticsearch]# kubectl get svc -n logging
NAME            TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)             AGE
elasticsearch   ClusterIP   None         <none>        9200/TCP,9300/TCP   3m9s
```

å¯¹ä¸Šè¿°æ— å¤´æœåŠ¡çš„yamlæ–‡ä»¶è¯´æ˜

```bash
    è¿™æ˜¯ä¸€ä¸ªéƒ¨ç½²Elasticsearché›†ç¾¤æ—¶ä½¿ç”¨çš„Serviceé…ç½®ç¤ºä¾‹ã€‚è¯¥Serviceä½¿ç”¨äº†clusterIPï¼šNoneï¼Œæ„å‘³ç€è¯¥Serviceä¸ä¼šåˆ†é…ClusterIPï¼Œå®ƒåªä¼šä¸ºé›†ç¾¤ä¸­çš„æ¯ä¸ªELasticsearch Pod åˆ†é…ä¸€ä¸ªç¨³å®šçš„DNSåç§°
    
    è¿™ä¸ªServiceçš„ä¸»è¦ä½œç”¨æ˜¯ä¸ºå…¶ä»–åº”ç”¨ç¨‹åºæˆ–æœåŠ¡æä¾›ä¸ElasticsearchèŠ‚ç‚¹çš„é€šä¿¡ã€‚åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼ŒServiceåç§°ä¸ºâ€œelasticsearchâ€ï¼Œåç§°ç©ºé—´ä¸ºâ€œloggingâ€ã€‚å®ƒä½¿ç”¨äº†selectorå­—æ®µæ¥åŒ¹é…å…·æœ‰appï¼šelasticsearchæ ‡ç­¾çš„Podï¼Œå¹¶å°†æµé‡å¯¼å‘è¿™äº›Pod
    
    è¯¥Serviceåœ¨ä¸¤ä¸ªç«¯å£ä¸Šå®šä¹‰äº†ç›‘å¬ï¼š
    â€œport: 9200â€ï¼š ç”¨äºElasticsearch HTTP APIçš„é€šä¿¡
    â€œport: 9300â€ï¼š ç”¨äºElasticsearché›†ç¾¤å†…èŠ‚ç‚¹ä¹‹é—´çš„é€šä¿¡
    
    è¿™ä¸ªé…ç½®ç¤ºä¾‹å¯ä»¥åœ¨Kubernetesé›†ç¾¤ä¸­éƒ¨ç½²ï¼Œä»¥ä¾¿åœ¨å…¶ä»–åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨ç›¸åº”çš„DNSåç§°å’Œç«¯å£æ¥è®¿é—®Elasticsearché›†ç¾¤
```



#### åŸºäºStatefulSetèµ„æºéƒ¨ç½²Elasticsearché›†ç¾¤

åˆ›å»ºèµ„æºæ¸…å•æ–‡ä»¶

```bash
[root@master1 ~/logging/elasticsearch]# cat elasticsearch-statefulset.yaml 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: es-cluster
  namespace: logging
spec:
  serviceName: elasticsearch
  replicas: 3
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      initContainers:
      - name: fix-permissions
        image: busybox  # å¯ä»¥æ”¹ä¸ºç§æœ‰ä»“é•œåƒåœ°å€
        imagePullPolicy: IfNotPresent
        command: ["sh", "-c", "chown -R 1000:1000 /usr/share/elasticsearch/data"] 
        securityContext:
          privileged: true
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
      - name: increase-vm-max-map
        image: busybox
        imagePullPolicy: IfNotPresent
        command: ["sysctl", "-w", "vm.max_map_count=262144"]
        securityContext:
          privileged: true
      - name: increase-fd-ulimit
        image: busybox
        imagePullPolicy: IfNotPresent
        command: ["sh", "-c", "ulimit -n 65536"]
        securityContext:
          privileged: true
      containers:
      - name: elasticsearch
        image: elasticsearch:7.17.8     # å¯ä»¥æ”¹ä¸ºç§æœ‰ä»“é•œåƒåœ°å€
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            cpu: 1000m
          requests:
            cpu: 100m
        ports:
        - containerPort: 9200
          name: tcp-9200
          protocol: TCP
        - containerPort: 9300
          name: tcp-9300
          protocol: TCP
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
        env:
          - name: cluster.name
            value: k8s-logs
          - name: node.name
            valueFrom:
              fieldRef:  # Kubernetesä¸­å®¹å™¨ç¯å¢ƒå˜é‡çš„å®šä¹‰æ–¹å¼
                fieldPath: metadata.name # å°†å½“å‰Podçš„åç§°ï¼ˆmetadata.nameï¼‰èµ‹å€¼ç»™å®¹å™¨é‡Œçš„ç¯å¢ƒå˜é‡node.name
          - name: discovery.seed_hosts
            value: "es-cluster-0.elasticsearch.logging.svc.cluster.local,es-cluster-1.elasticsearch.logging.svc.cluster.local,es-cluster-2.elasticsearch.logging.svc.cluster.local" # å¦‚æœåæœŸè¦æ–°å¢èŠ‚ç‚¹æ•°ï¼Œè¿™é‡Œå°†æ·»åŠ æ–°å¢çš„åç§°
          - name: cluster.initial_master_nodes
            value: "es-cluster-0,es-cluster-1,es-cluster-2"
          - name: ES_JAVA_OPTS
            value: "-Xms512m -Xmx512m" # ç”Ÿäº§ç¯å¢ƒçš„è¯ï¼Œå»ºè®®æœºå™¨å†…å­˜çš„ä¸€åŠï¼Œå®˜æ–¹å»ºè®®æœ€å¤š32G
  volumeClaimTemplates:
  - metadata:
      name: data
      labels:
        app: elasticsearch
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: sc-nfs
      resources:
        requests:
          storage: 5Gi
          
# å¯ç”¨æ¸…å•æ–‡ä»¶
[root@master1 ~/logging/elasticsearch]# kubectl apply -f elasticsearch-statefulset.yaml

# æŸ¥çœ‹
[root@master1 ~/logging/elasticsearch]# kubectl get pod -n logging
NAME           READY   STATUS    RESTARTS   AGE
es-cluster-0   1/1     Running   0          2m2s
es-cluster-1   1/1     Running   0          90s
es-cluster-2   1/1     Running   0          48s

# podéƒ¨ç½²å®Œæˆä¹‹åï¼Œå¯ä»¥é€šè¿‡REST APIæ£€æµ‹Elasticsearché›†ç¾¤æ˜¯å¦éƒ¨ç½²æˆåŠŸï¼Œä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤å°†æœ¬åœ°ç«¯å£9200è½¬å‘åˆ°ElasticsearchèŠ‚ç‚¹ï¼ˆå¦‚es-cluster-0ï¼‰å¯¹åº”çš„ç«¯å£
[root@master1 ~/logging/elasticsearch]# kubectl port-forward es-cluster-0 9200:9200 -n logging

# åœ¨å¦å¤–çš„ç»ˆç«¯çª—å£ä¸­ï¼Œæ‰§è¡Œå¦‚ä¸‹è¯·æ±‚ï¼Œæ–°å¼€ä¸€ä¸ªk8s-master01ç»ˆç«¯
[root@master1 ~]# curl http://localhost:9200/_cat/nodes
10.200.200.6 46 51 5 0.64 0.39 0.25 cdfhilmrstw - es-cluster-0
10.200.32.6  64 53 5 0.05 0.06 0.10 cdfhilmrstw * es-cluster-1
10.200.236.7 40 54 5 0.56 0.30 0.24 cdfhilmrstw - es-cluster-2
```

çœ‹åˆ°ä¸Šé¢çš„ä¿¡æ¯å°±è¡¨æ˜æˆ‘ä»¬çš„Elasticsearché›†ç¾¤æˆåŠŸåˆ›å»ºäº†3ä¸ªèŠ‚ç‚¹ï¼š`es-cluster-0`ï¼Œ`es-cluster-1`å’Œ`es-cluster-2`ï¼Œå½“å‰ä¸»èŠ‚ç‚¹æ˜¯`es-cluster-0`



#### cerebroå¯è§†åŒ–æŸ¥çœ‹ESé›†ç¾¤

Cerebroæ˜¯ä¸€ä¸ªå¼€æºçš„å¯è§†åŒ–å·¥å…·ï¼Œç”¨äºç®¡ç†å’Œç›‘æ§Elasticsearché›†ç¾¤ã€‚ä»–æä¾›äº†ä¸€ä¸ªç›´è§‚çš„ç•Œé¢ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿè½»æ¾åœ°æŸ¥çœ‹å’Œé…ç½®ç´¢å¼•ï¼ŒèŠ‚ç‚¹ï¼Œåˆ†ç‰‡ç­‰ä¿¡æ¯

Cerebroæä¾›äº†ä¸€äº›æœ‰ç”¨çš„åŠŸèƒ½ï¼Œä¾‹å¦‚æ‰§è¡Œç´¢å¼•æ“ä½œï¼Œæœç´¢ï¼Œåˆ›å»ºå’Œåˆ é™¤ç´¢å¼•ï¼Œè¿è¡ŒæŸ¥è¯¢å’Œèšåˆç­‰ã€‚å®ƒè¿˜å…è®¸ç”¨æˆ·æŸ¥çœ‹å’Œç›‘æ§èŠ‚ç‚¹çš„çŠ¶æ€ï¼Œè´Ÿè½½ï¼Œæ€§èƒ½æŒ‡æ ‡ç­‰ã€‚

Cerebroæ˜¯ä¸€ä¸ªç‹¬ç«‹çš„Javaåº”ç”¨ç¨‹åºï¼Œå¯ä»¥åœ¨æœ¬åœ°éƒ¨ç½²ï¼Œä¹Ÿå¯ä»¥ä½œä¸ºä¸€ä¸ªDockerå®¹å™¨è¿è¡Œã€‚å®ƒå¯ä»¥ä¸è¿œç¨‹çš„Elasticsearché›†ç¾¤è¿æ¥ï¼Œå¹¶æä¾›å¼ºå¤§çš„å¯è§†åŒ–å·¥å…·æ¥ç®¡ç†å’Œæ“ä½œé›†ç¾¤



**åˆ›å»ºcerebro svc**

```bash
# åˆ›å»º Cerebro svc èµ„æºæ¸…å•æ–‡ä»¶
[root@master1 ~/logging/elasticsearch]# mkdir /root/logging/cerebro
[root@master1 ~/logging/elasticsearch]# cd /root/logging/cerebro/
[root@master1 ~/logging/cerebro]# vim cerebro-svc.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: cerebro
  name: cerebro
  namespace: logging
spec:
  ports:
  - port: 9000
    protocol: TCP
    targetPort: 9000
  selector:
    app: cerebro
  type: NodePort
    
# å¯ç”¨èµ„æºæ¸…å•
[root@master1 ~/logging/cerebro]# kubectl apply -f cerebro-svc.yaml 
service/cerebro created

# æŸ¥çœ‹
[root@master1 ~/logging/cerebro]$ kubectl get svc -n logging cerebro 
NAME      TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
cerebro   NodePort   10.100.98.81   <none>        9000:32105/TCP   36s
```



**åˆ›å»ºCerebro deployment**

```bash
# åˆ›å»ºcerebro deploymentèµ„æºæ¸…å•æ–‡ä»¶
[root@master1 ~/logging/cerebro]# vim cerebro-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: cerebro
  name: cerebro
  namespace: logging
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cerebro
  template:
    metadata:
      labels:
        app: cerebro
      name: cerebro
    spec:
      containers:
      - image: lmenezes/cerebro:0.8.5   # è¿™é‡Œå¯ä»¥æ›¿æ¢æˆç§æœ‰ä»“çš„é•œåƒ
        imagePullPolicy: IfNotPresent
        name: cerebro
        resources:
          limits:
            cpu: 1
            memory: 1Gi
          requests:
            cpu: 100m
            memory: 250Mi
            
# æ›´æ–°èµ„æºæ¸…å•
[root@master1 ~/logging/cerebro]# kubectl apply -f cerebro-deployment.yaml 
deployment.apps/cerebro created

# æŸ¥çœ‹
[root@master1 ~/logging/cerebro]# kubectl get pod -n logging 
NAME                       READY   STATUS    RESTARTS   AGE
cerebro-67d4d8bc85-ktwrr   1/1     Running   0          85s
es-cluster-0               1/1     Running   0          158m
es-cluster-1               1/1     Running   0          157m
es-cluster-2               1/1     Running   0          156m

# è®¿é—®æµè§ˆå™¨ï¼šhttp://worker1IP:32105
```

![image-20250506020052406](../markdown_img/image-20250506020052406.png)

ä½¿ç”¨`http://elasticsearch.logging.svc.cluster.local:9200`è¿æ¥Elasticsearché›†ç¾¤

![image-20250506021129196](../markdown_img/image-20250506021129196.png)

![image-20250506021217941](../markdown_img/image-20250506021217941.png)



#### éƒ¨ç½²Kibanaå¯è§†åŒ–UIç•Œé¢

Kibanaæ˜¯Elastic Stackä¸­çš„ä¸€ä¸ªæ•°æ®å¯è§†åŒ–å·¥å…·ï¼Œç”¨äºå±•ç¤ºåˆ†æElasticsearchä¸­çš„æ•°æ®ã€‚é€šè¿‡Kibanaï¼Œæ‚¨å¯ä»¥åˆ›å»ºä»ªè¡¨ç›˜ï¼Œå›¾è¡¨ï¼Œåœ°å›¾ç­‰å¤šç§å¯è§†åŒ–æ–¹å¼ï¼Œä»¥ä¾¿æ›´å¥½çš„ç†è§£å’Œå‘ˆç°æ‚¨çš„æ•°æ®

**åˆ›å»ºKibana svc**

```bash
# åˆ›å»ºKibana svc èµ„æºæ¸…å•
[root@master1 ~]# mkdir /root/logging/kibana
[root@master1 ~]# cd /root/logging/kibana/
[root@master1 ~/logging/kibana]# vim kibana-svc.yaml
apiVersion: v1
kind: Service
metadata: 
  name: kibana
  namespace: logging
  labels:
    app: kibana
spec:
  type: NodePort
  ports:
  - port: 5601
  selector:
    app: kibana
   
# æ›´æ–°èµ„æºæ¸…å•
[root@master1 ~/logging/kibana]# kubectl apply -f kibana-svc.yaml 
service/kibana created

# æŸ¥çœ‹
[root@master1 ~/logging/kibana]# kubectl get svc -n logging
NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE
cerebro         NodePort    10.100.98.81    <none>        9000:32105/TCP      7h32m
elasticsearch   ClusterIP   None            <none>        9200/TCP,9300/TCP   11h
kibana          NodePort    10.100.35.184   <none>        5601:31802/TCP      101s
```



**åˆ›å»ºKibana Deployment**

```bash
[root@master1 ~/logging/kibana]# vim kibana-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kibana
  namespace: logging
  labels:
    app: kibana
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kibana
  template:
    metadata:
      labels:
        app: kibana
    spec:
      containers:
      - name: kibana
        image: kibana:7.17.8   # å¯ä»¥ä½¿ç”¨ç§ç”¨ä»“é•œåƒ
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            cpu: 1000m
          requests:
            cpu: 100m
        env:
          - name: ELASTICSEARCH_URL
            value: http://elasticsearch.logging.svc.cluster.local:9200
        ports:
        - containerPort: 5601

# æ›´æ–°èµ„æºæ¸…å•
[root@master1 ~/logging/kibana]# kubectl apply -f kibana-deploy.yaml 
deployment.apps/kibana created

# æŸ¥çœ‹
[root@master1 ~/logging/kibana]# kubectl get pod -n logging 
NAME                       READY   STATUS    RESTARTS   AGE
cerebro-67d4d8bc85-ktwrr   1/1     Running   0          7h20m
es-cluster-0               1/1     Running   0          9h
es-cluster-1               1/1     Running   0          9h
es-cluster-2               1/1     Running   0          9h
kibana-7b5ff7fb95-8mrgj    1/1     Running   0          33s

# æŸ¥çœ‹svc
[root@master1 ~/logging/kibana]# kubectl get svc -n logging
NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE
cerebro         NodePort    10.100.98.81    <none>        9000:32105/TCP      7h32m
elasticsearch   ClusterIP   None            <none>        9200/TCP,9300/TCP   11h
kibana          NodePort    10.100.35.184   <none>        5601:31802/TCP      101s

# æµè§ˆå™¨è®¿é—®
http://workerIP:31802
```

![image-20250506092038608](../markdown_img/image-20250506092038608.png)



#### Filebeatæ—¥å¿—é‡‡é›†å®æˆ˜

filebeaté‡‡é›†å™¨é…ç½®æ–‡ä»¶ï¼Œå‚è€ƒåœ°å€

```http
https://github.com/elastic/beats/blob/7.17/deploy/kubernetes/filebeat-kubernetes.yaml
```

![image-20250506180741406](../markdown_img/image-20250506180741406.png)

è¯´æ˜ï¼š

è¿™é‡Œéƒ¨ç½²FilebeatæŒ‰ç…§åç§°ç©ºé—´è¿›è¡Œåˆ†ç±»é‡‡é›†ã€‚ä¸€ä¸ªåç§°ç©ºé—´å¯¹åº”ä¸€ä¸ªç´¢å¼•ã€‚é‡‡é›†loggingå’Œkube-systemåç§°ç©ºé—´

##### **åˆ›å»ºsaã€roleã€cluster roleã€rolebinding**

```bash
[root@master1 ~/logging]# mkdir /root/logging/filebeat
[root@master1 ~/logging]# cd /root/logging/filebeat/

# åˆ›å»ºsaèµ„æºæ¸…å•æ–‡ä»¶
[root@master1 ~/logging/filebeat]# vim filebeat-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: filebeat
subjects:
- kind: ServiceAccount
  name: filebeat
  namespace: logging
roleRef:
  kind: ClusterRole
  name: filebeat
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: filebeat
  namespace: logging
subjects:
  - kind: ServiceAccount
    name: filebeat
    namespace: logging
roleRef:
  kind: Role
  name: filebeat
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: filebeat-kubeadm-config
  namespace: logging
subjects:
  - kind: ServiceAccount
    name: filebeat
    namespace: logging
roleRef:
  kind: Role
  name: filebeat-kubeadm-config
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: filebeat
  labels:
    k8s-app: filebeat
rules:
- apiGroups: [""]
  resources:
  - namespaces
  - pods
  - nodes
  verbs:
  - get
  - watch
  - list
- apiGroups: ["apps"]
  resources:
  - replicasets
  verbs: ["get","list","watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: filebeat
  namespace: logging
  labels:
    k8s-app: filebeat
rules:
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs: ["get","create","update"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: filebeat-kubeadm-config
  namespace: logging
  labels:
    k8s-app: filebeat
rules:
  - apiGroups: [""]
    resources:
      - configmaps
    resourceNames:
      - kubeadm-config
    verbs: ["get"]
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: filebeat
  namespace: logging
  labels:
    k8s-app: filebeat
    
# æ›´æ–°èµ„æºæ¸…å•
[root@master1 ~/logging/filebeat]# kubectl apply -f filebeat-role.yaml
clusterrolebinding.rbac.authorization.k8s.io/filebeat created
rolebinding.rbac.authorization.k8s.io/filebeat unchanged
rolebinding.rbac.authorization.k8s.io/filebeat-kubeadm-config unchanged
clusterrole.rbac.authorization.k8s.io/filebeat unchanged
role.rbac.authorization.k8s.io/filebeat unchanged
role.rbac.authorization.k8s.io/filebeat-kubeadm-config unchanged
serviceaccount/filebeat unchanged
```

##### åˆ›å»ºfilebeat configmapé…ç½®æ–‡ä»¶

```bash
[root@master1 ~/logging/filebeat]# cat filebeat-configmap.yaml 
apiVersion: v1
kind: ConfigMap
metadata:
  name: filebeat-config
  namespace: logging
  labels:
    k8s-app: filebeat
data:
  filebeat.yaml: |-
    filebeat.inputs:
    - type: container
      paths:
        - /var/log/containers/*logging*.log         # é‡‡é›†æºè·¯å¾„
      fields:
        index: logging
      processors:                                   # å¯¹é‡‡é›†çš„æ—¥å¿—è¿›è¡Œå¤„ç†çš„é…ç½®
        - add_kubernetes_metadata:                  # æ·»åŠ Kubernetesç›¸å…³çš„å…ƒæ•°æ®åˆ°é‡‡é›†çš„æ—¥å¿—
            default_indexers.enabled: true          # å¯ç”¨é»˜è®¤çš„ç´¢å¼•å™¨ï¼Œï¼Œç”¨åœ¨æ—¥å¿—ä¸­æ·»åŠ ç´¢å¼•ä¿¡æ¯
            default_matchers.enabled: true          # å¯ç”¨é»˜è®¤çš„åŒ¹é…å™¨ï¼Œï¼Œç”¨äºåŒ¹é…ç›¸å…³æ—¥å¿—
            host: ${NODE_NAME}                      # configmapé‡Œçš„å†…å®¹åªæ˜¯é™æ€æ¨¡ç‰ˆï¼Œéœ€è¦åç»­æŸç§æ–¹å¼åšæ¸²æŸ“
            matchers:                               # æŒ‡å®šåŒ¹é…è§„åˆ™
            - logs_path:
                logs_path: "/var/log/containers/"

    - type: container
      paths:
        - /var/log/containers/*kube-system*.log
      fields:
        index: kube-system
      processors:
        - add_kubernetes_metadata:
            default_indexers.enabled: true
            default_matchers.enabled: true
            hosts: ${NODE_NAME}
            matchers:
            - logs_path:
                logs_path: "/var/log/containers/"

    output.elasticsearch:
      hosts: ['elasticsearch.logging.svc.cluster.local:9200']
      indices:
        - index: "logging-ns-%{+yyyy.MM.dd}"
          when.equals:
            fields:
              index: "logging"
        - index: "kube-system-ns-%{+yyyy.MM.dd}"
          when.equals:
            fields:
              index: "kube-system"

# æ›´æ–°èµ„æºæ¸…å•
[root@master1 ~/logging/filebeat]# kubectl apply -f filebeat-configmap.yaml 
configmap/filebeat-config created
```

**æ ¸å¿ƒé…ç½®å­—æ®µè¯¦è§£**

```bash
      processors:
        - add_kubernetes_metadata:
            default_indexers.enabled: true
            default_matchers.enabled: true
            hosts: ${NODE_NAME}
            matchers:
            - logs_path:
                logs_path: "/var/log/containers/"
```

ä¸Šè¿°é…ç½®æ˜¯**Filebeat ä¸ Kubernetes é›†æˆä¸­æœ€æ ¸å¿ƒçš„é…ç½®ä¹‹ä¸€**ï¼Œä¸»è¦åŠŸèƒ½æ˜¯è‡ªåŠ¨ä¸ºæ—¥å¿—æ‰“ä¸Š Kubernetes çš„å…ƒæ•°æ®æ ‡ç­¾ï¼Œæ¯”å¦‚ pod åã€namespaceã€container åç­‰ï¼Œæ–¹ä¾¿åç»­æ—¥å¿—åˆ†æå’Œç­›é€‰ã€‚æˆ‘ä»¬é€è¡Œæ¥è¯¦ç»†è§£é‡Šæ¯ä¸€é¡¹å†…å®¹ï¼š



###### **âœ…`processors` å­—æ®µ**

åœ¨ Filebeat çš„é…ç½®ä¸­ï¼Œ`processors` å­—æ®µè¡¨ç¤ºä½¿ç”¨å†…ç½®æˆ–è‡ªå®šä¹‰çš„æ’ä»¶ï¼Œå¯¹æ—¥å¿—æ•°æ®è¿›è¡Œâ€œå¤„ç†å¢å¼ºã€è¿‡æ»¤ã€æ·»åŠ å­—æ®µâ€ç­‰æ“ä½œã€‚å®ƒæ˜¯ **Filebeat å®˜æ–¹æ”¯æŒçš„æœºåˆ¶**ã€‚

```yaml
processors:
  - add_kubernetes_metadata:
```

è¡¨ç¤ºé…ç½® **Filebeat æ—¥å¿—å¤„ç†æ’ä»¶ï¼ˆå¤„ç†å™¨ï¼‰**ã€‚
 å…¶ä¸­ `add_kubernetes_metadata` æ˜¯ä¸€ä¸ª **å®˜æ–¹å†…ç½®æ’ä»¶**ï¼Œç”¨äºè‡ªåŠ¨ä¸ºæ—¥å¿—æ·»åŠ  Kubernetes ä¸Šä¸‹æ–‡ï¼ˆmetadataï¼‰ã€‚

âœ…**Filebeat ä¸­å¸¸è§çš„ `processors` æ’ä»¶ä¸¾ä¾‹**

| æ’ä»¶å                    | ä½œç”¨æè¿°                                   |
| ------------------------- | ------------------------------------------ |
| `add_kubernetes_metadata` | æ·»åŠ  Kubernetes ä¸Šä¸‹æ–‡                     |
| `drop_event`              | ä¸¢å¼ƒä¸éœ€è¦çš„æ—¥å¿—                           |
| `drop_fields`             | ç§»é™¤å¤šä½™å­—æ®µ                               |
| `add_fields`              | ç»™æ—¥å¿—æ·»åŠ é™æ€å­—æ®µ                         |
| `decode_json_fields`      | å°† JSON å­—ç¬¦ä¸²å­—æ®µè§£ææˆç»“æ„åŒ–å­—æ®µ         |
| `rename`                  | é‡å‘½åå­—æ®µ                                 |
| `copy_fields`             | æ‹·è´å­—æ®µå†…å®¹åˆ°å…¶ä»–å­—æ®µå                   |
| `script`                  | ä½¿ç”¨ JS è„šæœ¬å¯¹äº‹ä»¶åšè‡ªå®šä¹‰å¤„ç†ï¼ˆé«˜çº§ç”¨æ³•ï¼‰ |

**ğŸ” ç¤ºä¾‹ï¼šä½¿ç”¨å¤šä¸ª processors æ’ä»¶**

```yaml
filebeat.inputs:
- type: container
  paths:
    - /var/log/containers/*.log
  processors:
    - add_kubernetes_metadata:
        default_indexers.enabled: true
        default_matchers.enabled: true
    - decode_json_fields:
        fields: ["message"]
        target: "json"
        overwrite_keys: true
    - drop_event:
        when:
          equals:
            json.level: "debug"
    - add_fields:
        target: ""
        fields:
          env: "prod"
```



###### **âœ…add_kubernetes_metadataè¯¦è§£**

**è¯¥æ’ä»¶çš„ç›®çš„æ˜¯ï¼š**â€œä¸ºæ¯ä¸€æ¡æ—¥å¿—è‡ªåŠ¨æ‰“ä¸Š Kubernetes ä¸Šä¸‹æ–‡çš„å…ƒä¿¡æ¯ï¼ˆmetadataï¼‰ã€‚â€

**åŸå§‹æ—¥å¿—å†…å®¹ï¼ˆæœªåŠ å…ƒæ•°æ®ï¼‰**

å‡è®¾æŸä¸ª Nginx å®¹å™¨ä¸­è¾“å‡ºäº†å¦‚ä¸‹æ—¥å¿—ï¼š

```log
127.0.0.1 - - [05/May/2025:09:00:00 +0000] "GET /index.html HTTP/1.1" 200 612
```

è¿™æ¡æ—¥å¿—åªåŒ…å« HTTP è¯·æ±‚ä¿¡æ¯ï¼Œ**æˆ‘ä»¬ä¸çŸ¥é“å®ƒæ˜¯å“ªä¸ªå®¹å™¨ã€å“ªä¸ª Podã€å“ªä¸ª Namespace æ‰“å°çš„**ã€‚

**åŠ ä¸Š Kubernetes å…ƒæ•°æ®åçš„æ•ˆæœ**

é€šè¿‡ Filebeat çš„ `add_kubernetes_metadata` æ’ä»¶å¤„ç†ä¹‹åï¼Œå®ƒå˜æˆç»“æ„åŒ–çš„ JSONï¼Œå¦‚ä¸‹ï¼š

```json
{
  "@timestamp": "2025-05-05T09:00:00Z",
  "message": "127.0.0.1 - - [05/May/2025:09:00:00 +0000] \"GET /index.html HTTP/1.1\" 200 612",
  "kubernetes": {
    "pod": {
      "name": "nginx-deployment-7db8b57b95-jkzpt"
    },
    "namespace": "production",
    "node": {
      "name": "worker-node-1"
    },
    "container": {
      "name": "nginx"
    },
    "labels": {
      "app": "nginx",
      "env": "prod"
    }
  },
  "log": {
    "file": {
      "path": "/var/log/containers/nginx-deployment-7db8b57b95-jkzpt_default_nginx-xxxx.log"
    }
  }
}
```

**ğŸ¯æ ¸å¿ƒå·¥ä½œæœºåˆ¶**

åœ¨ Filebeat ä½¿ç”¨ `add_kubernetes_metadata` æ’ä»¶æ—¶ï¼Œå®ƒçš„æ ¸å¿ƒæµç¨‹å°±æ˜¯ï¼š**é€šè¿‡ Indexer å®šä½å‡ºå®¹å™¨ ID**ï¼Œå†**é€šè¿‡ Matcher å…³è”è¯¥å®¹å™¨çš„ Kubernetes å…ƒæ•°æ®ï¼ˆPodã€Nodeã€Namespace ç­‰ï¼‰**ã€‚

ğŸ”**`default_indexers.enabled: true`è¯¦è§£**

**ä½œç”¨ï¼š**é»˜è®¤å¼€å¯æ—¶ï¼ŒFilebeat ä¼šå°è¯•ä»æ—¥å¿—è·¯å¾„ä¸­**æå–å®¹å™¨ ID**ï¼Œå¹¶å°†å…¶ä½œä¸ºç´¢å¼• key å»æŸ¥è¯¢è¯¥å®¹å™¨å¯¹åº”çš„ Kubernetes å…ƒæ•°æ®ã€‚

**âœ… é»˜è®¤çš„ Indexer ç±»å‹ï¼ˆå®¹å™¨ IDï¼‰ï¼š**

ä¸¾ä¾‹ï¼šæ—¥å¿—è·¯å¾„ä¸ºï¼š

```bash
# å®¹å™¨çš„æ—¥å¿—åè§„åˆ™ï¼š<podåç§°>_<åç§°ç©ºé—´>_<containeråç§°>-<å®¹å™¨ID>.log
/var/log/containers/nginx-pod_default_nginx-12345abcde.log
```

Filebeat ä¼šè§£æå‡º container ID ä¸º `12345abcde`ï¼Œç„¶åè°ƒç”¨ K8s API æŸ¥è¯¢è¯¥ ID çš„å®¹å™¨æ‰€å± pod ç­‰ä¿¡æ¯ï¼Œå¹¶åŠ åˆ°æ—¥å¿—ä¸­ã€‚

**ğŸ”`default_matchers.enabled: true`**

 **ä½œç”¨ï¼š**é»˜è®¤å¼€å¯æ—¶ï¼ŒFilebeat ä¼šä½¿ç”¨è‡ªå¸¦çš„ **è·¯å¾„åŒ¹é…é€»è¾‘** å»â€œçŒœæµ‹â€æ—¥å¿—å¯¹åº”çš„å®¹å™¨ã€‚

**å®ƒå¦‚ä½•åŒ¹é…ï¼š**

Matcher ä¼šæ‰«æé…ç½®ä¸­æŒ‡å®šçš„æ—¥å¿—è·¯å¾„ï¼ˆå¦‚ `/var/log/containers/`ï¼‰ï¼Œå¹¶å°†æ—¥å¿—æ–‡ä»¶åä¸­çš„ä¿¡æ¯æ‹†è§£ä¸ºï¼š

- pod åç§°
- namespace åç§°
- container åç§°

å†ä¸å½“å‰ä¸»æœºä¸Š kubelet æä¾›çš„å®¹å™¨çŠ¶æ€ä¿¡æ¯å¯¹æ¯”ï¼Œç¡®å®šå®¹å™¨æ¥æºã€‚

**ç¤ºä¾‹ï¼š**

æ—¥å¿—è·¯å¾„ï¼š

```lua
/var/log/containers/nginx-6799fc88d8-kpx8z_default_nginx-abcdef123456.log
```

filebeat ä¼šä»ä¸­æå–ï¼š

| å­—æ®µ         | å€¼                     |
| ------------ | ---------------------- |
| Pod å       | nginx-6799fc88d8-kpx8z |
| Namespace    | default                |
| Container å | nginx                  |
| å®¹å™¨ ID      | abcdef123456           |

ç„¶åé€šè¿‡ Kubernetes API Server æŸ¥è¯¢ `nginx-6799fc88d8-kpx8z` è¿™ä¸ª Pod çš„å…ƒæ•°æ®ï¼Œå¹¶è‡ªåŠ¨æ·»åŠ åˆ°æ—¥å¿—ä¸­ã€‚

**ğŸ”`matchers` å­—æ®µï¼ˆè‡ªå®šä¹‰åŒ¹é…è§„åˆ™ï¼‰**

å¦‚æœä½ æƒ³æ›´ç²¾ç»†åœ°æ§åˆ¶åŒ¹é…è¡Œä¸ºï¼Œæ¯”å¦‚åœ¨è‡ªå®šä¹‰è·¯å¾„ä¸‹æ”¶é›†æ—¥å¿—æ—¶æ— æ³•è§¦å‘é»˜è®¤è§„åˆ™ï¼Œå°±å¯ä»¥ä½¿ç”¨ `matchers` è‡ªå®šä¹‰åŒ¹é…è§„åˆ™

```yaml
matchers:
  - logs_path:
      logs_path: "/var/log/containers/"
```

è¿™ä¸ªè§„åˆ™ä¼šæ˜ç¡®å‘Šè¯‰æ’ä»¶ï¼šâ€œä½ ä»è¿™ä¸ªè·¯å¾„ä¸‹çš„æ–‡ä»¶è¯»å–æ—¥å¿—æ—¶ï¼Œè¯·ç”¨æ–‡ä»¶åæ¥åŒ¹é… Kubernetes å®¹å™¨çš„å…ƒæ•°æ®â€ã€‚

**âš™ï¸ å·¥ä½œæµç¨‹å›¾ç¤º**

```lua
        +---------------------------+
        |   Filebeat è¯»å–æ—¥å¿—æ–‡ä»¶   |
        +------------+--------------+
                     |
                     v
     +------------------------------------+
     | default_indexers: æå–å®¹å™¨ ID       |
     +------------------------------------+
                     |
                     v
     +------------------------------------+
     | default_matchers: åŒ¹é… Pod ä¿¡æ¯è·¯å¾„  |
     +------------------------------------+
                     |
                     v
        +---------------------------+
        |   è°ƒç”¨ K8s API æŸ¥è¯¢å…ƒæ•°æ®  |
        +---------------------------+
                     |
                     v
     +------------------------------------+
     | å°† Kubernetes metadata åŠ åˆ°æ—¥å¿—ä¸­  |
     +------------------------------------+
```

**ğŸ§ å®é™…æ„ä¹‰**

**æé«˜å¯è§‚å¯Ÿæ€§**ï¼šä½ å¯ä»¥åœ¨ Kibana ç­‰åœ°æ–¹æ ¹æ® namespaceã€podã€container æŸ¥è¯¢ï¼›

**æŒ‰è´£ä»»äºº/æœåŠ¡å½’æ¡£**ï¼šæ¯”å¦‚ä¸åŒå›¢é˜Ÿéƒ¨ç½²åœ¨ä¸åŒ namespaceï¼Œæ—¥å¿—è‡ªåŠ¨å¸¦ namespaceï¼Œä¾¿äºåŒºåˆ†ï¼›

**è‡ªåŠ¨åŒ–è·¯ç”±**ï¼šå¯æ ¹æ® metadata é…ç½® logstash æˆ– elasticsearch å°†ä¸åŒæœåŠ¡æ—¥å¿—å†™åˆ°ä¸åŒç´¢å¼•ï¼›

**å¢å¼ºå®¡è®¡èƒ½åŠ›**ï¼šæ—¥å¿—è®°å½•æ¸…æ¥šæ¥è‡ªå“ªä¸ª pod/nodeï¼Œä¾¿äºè¿½è¸ªé—®é¢˜ã€‚



```ABAP
é—®é¢˜ï¼šæ—¢ç„¶ default_indexers å’Œ default_matchers éƒ½æ˜¯ä¸ºäº†è·å– Kubernetes å…ƒæ•°æ®ï¼Œä¸ºä»€ä¹ˆè¦æœ‰ä¸¤ä¸ªæœºåˆ¶ï¼Ÿ
```

ä¸¤è€…**åŠŸèƒ½ç±»ä¼¼ï¼Œä½†æœºåˆ¶ä¸åŒã€äº’ä¸ºè¡¥å……**ï¼Œç”¨äºå¢å¼º **é²æ£’æ€§ï¼ˆå¥å£®æ€§ï¼‰ä¸å…¼å®¹æ€§**ã€‚

**åŒºåˆ«ä¸ä½¿ç”¨æ—¶æœºè¯¦è§£ï¼š**

| åŠŸèƒ½é¡¹     | `default_indexers.enabled`                           | `default_matchers.enabled`                        |
| ---------- | ---------------------------------------------------- | ------------------------------------------------- |
| ä½œç”¨æ–¹å¼   | **ä»æ—¥å¿—å†…å®¹æˆ–è·¯å¾„ä¸­æå–å®¹å™¨ ID**ï¼Œä½œä¸ºå…³é”®ç´¢å¼•      | **ä»æ—¥å¿—è·¯å¾„ä¸­æå– pod/container åç§°**ï¼Œç”¨äºåŒ¹é… |
| ç²¾ç¡®åº¦     | é«˜ï¼ˆå®¹å™¨ ID å”¯ä¸€ï¼‰                                   | ä¸­ï¼ˆå¯èƒ½å¤šä¸ª pod åç§°é‡å¤ï¼‰                       |
| å¯¹è·¯å¾„ä¾èµ– | ä½ï¼ˆå“ªæ€•ä½ æ—¥å¿—ä¸åœ¨æ ‡å‡†ç›®å½•ï¼Œåªè¦èƒ½æ‹¿åˆ°å®¹å™¨ ID å°±è¡Œï¼‰ | é«˜ï¼ˆè¦æ±‚æ—¥å¿—è·¯å¾„ç¬¦åˆæ ‡å‡† Kubernetes æ—¥å¿—ç»“æ„ï¼‰    |
| ä½¿ç”¨åœºæ™¯   | ä»»æ„æ—¥å¿—æ”¶é›†ä½ç½®ï¼›æ”¯æŒå®¹å™¨ runtime                   | æ—¥å¿—è·¯å¾„ç¬¦åˆ `/var/log/containers/xxx.log`        |
| å®ç°åŸç†   | é€šè¿‡ container runtime è·å– metadata                 | ç›´æ¥åŒ¹é…æ—¥å¿—è·¯å¾„è§„åˆ™ï¼Œå…³è” kubelet æä¾›çš„å…ƒæ•°æ®   |
| é€šå¸¸ä¼˜å…ˆçº§ | **å…ˆç”¨ indexerï¼Œfallback åˆ° matcher**                | è¢«åŠ¨åŒ¹é…ï¼Œè¾…åŠ©å…œåº•                                |

**ğŸ”§ ä¸ºä»€ä¹ˆè¦ä¸¤ä¸ªéƒ½å¼€ï¼Ÿ**

å› ä¸ºç”Ÿäº§ä¸­å¯èƒ½é‡åˆ°è¿™äº›æƒ…å†µï¼š

1. ğŸ” **å®¹å™¨è¿è¡Œæ—¶ä¿¡æ¯ä¸¢å¤±**
    æ¯”å¦‚å®¹å™¨åˆšç»“æŸæˆ– kubelet çŠ¶æ€æœªåŒæ­¥ï¼Œcontainer ID æŸ¥ä¸åˆ°æ—¶ï¼Œmatcher å¯ä»¥å…œåº•åŒ¹é…ï¼›
2. ğŸªµ **è‡ªå®šä¹‰æ—¥å¿—è·¯å¾„æˆ–éæ ‡å‡†å®¹å™¨è¿è¡Œæ—¶**
    æœ‰äº›å®šåˆ¶åŒ–ç³»ç»Ÿï¼Œæ— æ³•æå–å®¹å™¨ IDï¼Œmatcher å¯ä»¥ä»è·¯å¾„ä¸­â€œçŒœâ€å‡º pod åï¼›
3. ğŸ§© **å¢åŠ å…¼å®¹æ€§ä¸å¼¹æ€§**
    ä¸¤ç§æœºåˆ¶äº’ä¸º backupï¼Œæé«˜æ—¥å¿—å…ƒæ•°æ®é‡‡é›†çš„æˆåŠŸç‡ï¼Œä¸æ˜“ä¸¢æ•°æ®ã€‚

**ğŸ“Œ å®é™…ä½¿ç”¨å»ºè®®ï¼š**

```yaml
processors:
  - add_kubernetes_metadata:
      default_indexers.enabled: true
      default_matchers.enabled: true
```

- æ¨èä¸¤ä¸ªéƒ½å¼€ï¼›
- é»˜è®¤ Indexer æä¾›ç²¾ç¡®åŒ¹é…ï¼›
- Matcher æ˜¯å®¹é”™æœºåˆ¶ï¼Œå…œåº•è¡¥å…¨å…ƒæ•°æ®ã€‚



###### **âœ… `hosts: ${NODE_NAME}` çš„ä½œç”¨**

å®ƒç”¨äº**å‘Šè¯‰ Filebeat å½“å‰è¿è¡Œåœ¨å“ªä¸ª Node ä¸Š**ï¼Œè®©å®ƒçŸ¥é“åº”è¯¥ä»å“ªä¸ª kubelet ä¸Šå»æŸ¥è¯¢å®¹å™¨è¿è¡Œæƒ…å†µï¼Œä»¥ä¾¿è§£ææ—¥å¿—å’Œé™„åŠ  Kubernetes å…ƒæ•°æ®ã€‚

**ğŸ” èƒŒåæœºåˆ¶ï¼š**

1. **Filebeat ä¼šè¯»å–æ—¥å¿—æ–‡ä»¶ï¼ˆå¦‚ `/var/log/containers/\*.log`ï¼‰**ï¼›

2. ä¸ºäº†ç»™æ—¥å¿—æ·»åŠ æ­£ç¡®çš„ Kubernetes å…ƒæ•°æ®ï¼ˆPod åã€Namespaceã€å®¹å™¨åç­‰ï¼‰ï¼š

   - å®ƒä¼šæ ¹æ®å®¹å™¨ ID æŸ¥è¯¢ kubeletï¼›
   - kubelet è¿”å›è¿™ä¸ªå®¹å™¨å±äºå“ªä¸ª Podï¼›

3. **ä½† kubelet æ˜¯ Node æœ¬åœ°çš„ç»„ä»¶**ï¼Œæ‰€ä»¥ Filebeat å¿…é¡»çŸ¥é“å®ƒå½“å‰è¿è¡Œåœ¨å“ªå°ä¸»æœºï¼ˆNodeï¼‰ï¼›

4. `hosts: ${NODE_NAME}` å°±æ˜¯æ˜ç¡®æŒ‡å®š â€œè¿™æ˜¯å“ªå°ä¸»æœºâ€ï¼Œä»è€Œ Filebeat å¯ä»¥æ„é€ è¯·æ±‚ URLï¼Œå¦‚ï¼š

   ```http
   https://<NODE_NAME>:10250/pods
   ```

**ğŸ§© å˜é‡æ¥æº**

`${NODE_NAME}` æ˜¯æ¥è‡ª Pod çš„ç¯å¢ƒå˜é‡ï¼Œä¸€èˆ¬åœ¨ DaemonSet ä¸­é€šè¿‡å¦‚ä¸‹æ–¹å¼è®¾ç½®ï¼š

```yaml
env:
  - name: NODE_NAME
    valueFrom:
      fieldRef:
        fieldPath: spec.nodeName
```

**âœ… æ€»ç»“**

| é…ç½®é¡¹                | ä½œç”¨è¯´æ˜                                                     |
| --------------------- | ------------------------------------------------------------ |
| `hosts: ${NODE_NAME}` | æŒ‡å®š Filebeat å½“å‰è¿è¡Œçš„ Node åï¼Œç”¨äºè”ç³»æœ¬åœ° kubelet é‡‡é›† Pod å…ƒæ•°æ® |
| `${NODE_NAME}`        | æ¥è‡ª Pod çš„ `spec.nodeName`ï¼Œé€šè¿‡ `env` æ³¨å…¥å®¹å™¨ç¯å¢ƒå˜é‡     |



###### âœ… ä¸ºä»€ä¹ˆ **ä¸éœ€è¦ `envsubst`**æ¸²æŸ“ï¼Ÿ

å½“ä½ å°† `ConfigMap` æŒ‚è½½ä¸ºæ–‡ä»¶æ—¶ï¼ˆæ¯”å¦‚ `filebeat.yaml`ï¼‰ï¼Œ**Kubernetes ä¸ä¼šæ›¿æ¢æ–‡ä»¶ä¸­çš„ `${}` å ä½ç¬¦**ã€‚ä½†æ˜¯ï¼š

- å¦‚æœä½ çš„åº”ç”¨ï¼ˆå¦‚ Filebeatï¼‰åœ¨è¿è¡Œæ—¶**è‡ªå·±è§£æé…ç½®ä¸­çš„ç¯å¢ƒå˜é‡**ï¼Œé‚£ä¹ˆå°±ä¸éœ€è¦æå‰ `envsubst`ï¼›
- **Filebeat æ”¯æŒåœ¨å…¶é…ç½®æ–‡ä»¶ä¸­åŠ¨æ€è§£æ `${ENV_VAR}` æ ¼å¼çš„å˜é‡**ï¼Œä¼šåœ¨è¿è¡Œæ—¶è‡ªåŠ¨ç”¨å®¹å™¨çš„ç¯å¢ƒå˜é‡å€¼æ›¿æ¢ã€‚

**âœ… æ­£ç¡®çš„åšæ³•ï¼ˆä½ ç›®å‰çš„æ–¹å¼æ˜¯å¯¹çš„ï¼‰ï¼š**

1. `ConfigMap` ä¸­ä¿ç•™ `${NODE_NAME}`ï¼š

   ```
   yaml
   
   
   CopyEdit
   host: ${NODE_NAME}
   ```

2. åœ¨ DaemonSet çš„å®¹å™¨å®šä¹‰ä¸­é€šè¿‡ `env` è®¾ç½®å˜é‡ï¼š

   ```
   yamlCopyEditenv:
     - name: NODE_NAME
       valueFrom:
         fieldRef:
           fieldPath: spec.nodeName
   ```

3. Filebeat å¯åŠ¨æ—¶è¯»å–é…ç½®ï¼Œå‘ç°æœ‰ `${NODE_NAME}`ï¼Œè‡ªåŠ¨æ›¿æ¢æˆå®é™…å€¼ï¼ˆå¦‚ `work1.mystical.org`ï¼‰ã€‚

**ğŸ” Filebeat çš„èƒ½åŠ›ï¼š**

Filebeatã€Logstashã€Elasticsearch ç­‰ Elastic Stack ç»„ä»¶æœ¬èº«æ”¯æŒè§£æ `${ENV_VAR}` å½¢å¼å˜é‡ï¼ˆåŸºäº Go çš„ `os.Getenv`ï¼‰ï¼Œæ‰€ä»¥å¯ä»¥æ”¾å¿ƒåœ¨ ConfigMap ä¸­ç”¨å ä½ç¬¦ï¼Œä¸éœ€è¦é¢„å¤„ç†ã€‚



##### Filebeat é‡‡é›†å®¹å™¨æ—¥å¿—çš„å®Œæ•´æœºåˆ¶æµç¨‹

ä»¥ DaemonSet æ¨¡å¼ä¸ºä¾‹ï¼Œæ¯ä¸ª Node å¯åŠ¨ä¸€ä¸ª Filebeat å®ä¾‹ï¼Œæµç¨‹å¦‚ä¸‹ï¼š

###### ğŸ§­ æ­¥éª¤ 1ï¼šè¯»å–å®¹å™¨æ—¥å¿—æ–‡ä»¶

- Filebeat é€šè¿‡æŒ‚è½½ `/var/log/containers/*.log` è·¯å¾„è¯»å–æ—¥å¿—ï¼›
- è¿™äº›æ–‡ä»¶æ˜¯ Kubelet æŠŠå®¹å™¨ stdout/stderr é‡å®šå‘åˆ°æœ¬åœ°æ–‡ä»¶åçš„è·¯å¾„ã€‚

###### ğŸ§­ æ­¥éª¤ 2ï¼šä½¿ç”¨ `add_kubernetes_metadata` æ’ä»¶è§£ææ—¥å¿—æ¥æº

- æ’ä»¶ä¼šä»æ—¥å¿—è·¯å¾„ä¸­è§£æå‡º container IDï¼›
- ç„¶åå‘ K8s API æŸ¥è¯¢è¯¥ container ID å±äºå“ªä¸ª Podï¼›
- å¹¶è·å– Pod çš„ç›¸å…³ä¿¡æ¯ï¼ˆå¦‚ namespaceã€labelsã€annotationsã€node åï¼‰ï¼›

ğŸ‘‰ æ‰€ä»¥è¿™ä¸€æ­¥å¿…é¡»èƒ½è®¿é—® kube-apiserverã€‚

###### ğŸ§­ æ­¥éª¤ 3ï¼šå‘æ—¥å¿—ä¸­æ³¨å…¥å…ƒä¿¡æ¯

å¤„ç†åçš„æ—¥å¿—ä¼šæ·»åŠ å¦‚ä¸‹å­—æ®µï¼ˆä¸¾ä¾‹ï¼‰ï¼š

```json
{
  "log": "app started",
  "kubernetes": {
    "container": {
      "name": "nginx"
    },
    "pod": {
      "name": "nginx-7c97bd8fc9-4kwnr",
      "uid": "f32f42fa..."
    },
    "namespace": "default",
    "node": {
      "name": "node1"
    },
    "labels": {
      "app": "nginx"
    }
  }
}
```

###### ğŸ§­ æ­¥éª¤ 4ï¼šå‘é€æ—¥å¿—åˆ° Elasticsearch / Logstash / Kafka

- å¯ä»¥ç›´æ¥è¾“å‡ºåˆ° Elasticsearchï¼›
- æˆ–è€…ä¸­è½¬è‡³ Logstash å¤„ç†å†è¾“å‡ºï¼›
- ç´¢å¼•åã€å­—æ®µå¯ä»¥æ ¹æ® `kubernetes.namespace` ç­‰è¿›è¡Œåˆ†æµã€‚



**âœ… ç¤ºä¾‹ï¼šServiceAccount + ClusterRole çš„é…ç½®æƒé™**

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: filebeat
rules:
  - apiGroups: [""]
    resources: ["pods", "namespaces", "nodes"]
    verbs: ["get", "watch", "list"]

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: filebeat
  namespace: logging

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: filebeat
roleRef:
  kind: ClusterRole
  name: filebeat
  apiGroup: rbac.authorization.k8s.io
subjects:
  - kind: ServiceAccount
    name: filebeat
    namespace: logging
```



**âœ… æ€»ç»“**

| é—®é¢˜                        | è§£ç­”                                          |
| --------------------------- | --------------------------------------------- |
| ä¸ºä»€ä¹ˆè¦ ServiceAccountï¼Ÿ   | å› ä¸ºéœ€è¦é€šè¿‡ Kubernetes API è·å– Pod å…ƒä¿¡æ¯   |
| Filebeat éœ€è¦è®¿é—®å“ªäº›èµ„æºï¼Ÿ | Podã€Nodeã€Namespaceï¼Œéƒ¨åˆ†æƒ…å†µè¿˜åŒ…æ‹¬ Service  |
| Filebeat çš„æ ¸å¿ƒæµç¨‹æ˜¯ï¼Ÿ     | è¯»å–æ—¥å¿— â†’ æ·»åŠ  k8s å…ƒæ•°æ® â†’ è¾“å‡ºåˆ° ES ç­‰ç³»ç»Ÿ |



##### åˆ›å»ºfilebeat daemonseté…ç½®

```bash
[root@master1 ~/logging/filebeat]# cat filebeat-daemonset.yaml 
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: filebeat
  namespace: logging
  labels:
    k8s-app: filebeat
spec:
  selector:
    matchLabels:
      k8s-app: filebeat
  template:
    metadata:
      labels:
        k8s-app: filebeat
    spec:
      serviceAccountName: filebeat
      terminationGracePeriodSeconds: 30
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet
      tolerations:
        - key: "node-role.kubernetes.io/control-plane"
          operator: "Exists"
          effect: "NoSchedule"
      containers:
      - name: filebeat
        image: docker.elastic.co/beats/filebeat:7.17.12
        args: [
          "-c","/etc/filebeat.yaml",
          "-e",
        ]
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        securityContext:
          runAsUser: 0
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 100Mi
        volumeMounts:
        - name: config
          mountPath: /etc/filebeat.yaml
          readOnly: true
          subPath: filebeat.yaml
        - name: data
          mountPath: /usr/share/filebeat/data
            #        - name: varlibdockercontainers
            #          mountPath: /var/lib/docker/containers
            #          readOnly: true
        - name: varlog
          mountPath: /var/log
          readOnly: true
      volumes:
      - name: config
        configMap:
          defaultMode: 0640
          name: filebeat-config
            #      - name: varlibdockercontainers
            #        hostPath:
            #          path: /var/lib/docker/containers
      - name: varlog
        hostPath:
          path: /var/log
      - name: data
        hostPath:
          path: /var/lib/filebeat-data
          type: DirectoryOrCreate
          
# æ›´æ–°èµ„æºæ¸…å•

```



###### ä¸Šè¿°filebeat-daemonset.yamlè§£è¯»

**1. è¯¦ç»†è§£é‡Š Filebeat ä½¿ç”¨ `hostNetwork: true` çš„æ„ä¹‰**

**âœ…è·å–çœŸå®çš„ Node IP**

æŸäº›æ’ä»¶ï¼Œæ¯”å¦‚ `add_kubernetes_metadata` éœ€è¦ä½¿ç”¨ node IP å»åŒ¹é… kubelet ä¸Šçš„æ—¥å¿—æ¥æºï¼ˆå°¤å…¶æ˜¯å®¹å™¨è¿è¡Œæ—¶çš„ socket æ–‡ä»¶ï¼‰ï¼Œæ¯”å¦‚ï¼š

```yaml
processors:
  - add_kubernetes_metadata:
      host: ${NODE_NAME}
```

è¿™ä¸ª `host` å¯¹åº”çš„æ˜¯ node çš„åç§°ï¼Œ**æœ€ç»ˆä¼šé€šè¿‡ node åæŸ¥æ‰¾å…¶ IPï¼Œç„¶åè®¿é—®ç›¸å…³ APIï¼ˆå¦‚ kubelet 10250 ç«¯å£ï¼‰æ¥æå–å®¹å™¨çš„å…ƒä¿¡æ¯**ã€‚

ä½†å¦‚æœä½  **æ²¡æœ‰å¯ç”¨ `hostNetwork`**ï¼Œå®¹å™¨å†…éƒ¨å¯èƒ½æ‹¿åˆ°çš„æ˜¯è™šæ‹Ÿ IPï¼Œä¸æ˜¯çœŸå®çš„ node IPï¼Œå¯¼è‡´ï¼š

- è®¿é—® kubelet å¤±è´¥ï¼›
- è·å–å®¹å™¨ metadata å¤±è´¥ï¼›
- æœ€ç»ˆæ—¥å¿—ä¸­æ²¡æœ‰ pod.nameã€namespace ç­‰å­—æ®µã€‚

ğŸ’¡ å¯ç”¨ `hostNetwork: true` åï¼Œå®¹å™¨çš„ IP = Node çš„ IPï¼Œæ­¤æ—¶è®¿é—® kubelet æ›´å®¹æ˜“æˆåŠŸã€‚

**âœ… ä¸¾ä¾‹è¯´æ˜**

å‡è®¾ä½ æœ‰ä¸€ä¸ª Nodeï¼ŒIP æ˜¯ `10.2.1.100`ï¼Œæœ‰ä¸€ä¸ª pod åœ¨æ­¤è¿è¡Œï¼Œæ—¥å¿—æ–‡ä»¶è·¯å¾„æ˜¯ï¼š

```bash
/var/log/containers/nginx-pod_default_nginx-container-<container-id>.log
```

å¯ç”¨ `hostNetwork: true` çš„ Filebeat Pod ä¼šï¼š

- **ç›´æ¥è®¿é—®** `/var/log/containers/...`ï¼›
- ä½¿ç”¨è‡ªèº« IPï¼ˆå³ 10.2.1.100ï¼‰**ä¸ kubelet é€šä¿¡**ï¼›
- `add_kubernetes_metadata` æˆåŠŸä» kubelet ä¸­æ‹‰å–å…ƒä¿¡æ¯ï¼›
- æ—¥å¿—æœ€ç»ˆä¼šå¸¦ä¸Šä»¥ä¸‹å­—æ®µï¼š

```json
{
  "log": "...",
  "kubernetes": {
    "container": {
      "name": "nginx-container"
    },
    "pod": {
      "name": "nginx-pod",
      "uid": "xxxxx",
      "namespace": "default"
    },
    "node": {
      "name": "node-1"
    }
  }
}
```

 

**2. `dnsPolicy: ClusterFirstWithHostNet` æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ**

é»˜è®¤æƒ…å†µä¸‹ï¼š

| `hostNetwork` | `dnsPolicy` é»˜è®¤å€¼ | èƒ½å¦è§£æ cluster.local åŸŸå                 |
| ------------- | ------------------ | ------------------------------------------- |
| false         | `ClusterFirst`     | âœ… å¯ä»¥è§£æï¼ˆä½¿ç”¨ kube-dnsï¼‰                 |
| true          | `Default`          | âŒ æ— æ³•è§£æï¼ˆä½¿ç”¨å®¿ä¸»æœº `/etc/resolv.conf`ï¼‰ |



**âœ… è§£å†³æ–¹å¼ï¼š**

å½“è®¾ç½®äº† `hostNetwork: true` åï¼Œå¦‚æœæƒ³ç»§ç»­ä½¿ç”¨ **Kubernetes å†…éƒ¨ DNSï¼ˆå¦‚ `elasticsearch.logging.svc.cluster.local`ï¼‰**ï¼Œå°±å¿…é¡»åŠ ä¸Šï¼š

```yaml
dnsPolicy: ClusterFirstWithHostNet
```

ğŸ“Œ **è¿™æ˜¯ä½ é…ç½® `elasticsearch.logging.svc.cluster.local` ä¸º hosts çš„å‰æ**ï¼Œå¦åˆ™ä¼šè§£æå¤±è´¥ã€‚



**3. `/var/lib/filebeat-data` æ˜¯ç”¨æ¥å¹²å˜›çš„ï¼ŸæŒ‚è½½åˆ° `/usr/share/filebeat/data` æœ‰ä»€ä¹ˆæ„ä¹‰ï¼Ÿ**

è¿™æ˜¯ Filebeat çš„ **çŠ¶æ€æ•°æ®ç›®å½•**ï¼Œé»˜è®¤ç”¨äºå­˜æ”¾ï¼š

- **offset ä¿¡æ¯**ï¼šå·²ç»è¯»å–åˆ°å“ªä¸ªä½ç½®ï¼›
- **æ³¨å†ŒçŠ¶æ€**ï¼šå“ªäº›æ–‡ä»¶è¢«é‡‡é›†ï¼›
- **harvester å…ƒä¿¡æ¯** ç­‰ã€‚

æŒ‚è½½å®¿ä¸»æœºçš„ `/var/lib/filebeat-data` çš„æ„ä¹‰ï¼š

| ç›®çš„         | è¯´æ˜                                          |
| ------------ | --------------------------------------------- |
| æŒä¹…åŒ–       | é˜²æ­¢ Pod é‡å¯åé‡æ–°é‡‡é›†å·²è¯»æ–‡ä»¶ï¼ˆé¿å…é‡å¤ï¼‰ï¼› |
| å•èŠ‚ç‚¹å¤š Pod | å®¹å™¨ä¹‹é—´å…±äº«é‡‡é›†çŠ¶æ€ï¼Œé¿å…é‡å¤ï¼›              |
| æ•…éšœæ¢å¤     | è®°å½•è¯»å–è¿›åº¦ï¼Œå®¹å™¨æŒ‚äº†é‡æ–°è¯»ä¸ä¼šé”™ä¹±          |



**4. ä¸ºä»€ä¹ˆå®¿ä¸»æœºä¼šæœ‰ `/var/lib/filebeat-data`ï¼Ÿéœ€è¦è‡ªå·±åˆ›å»ºå—ï¼Ÿ**

æ˜¯çš„ï¼Œéœ€è¦æ‰‹åŠ¨åˆ›å»ºï¼Œ**æˆ–è€…ä½¿ç”¨ `type: DirectoryOrCreate` è‡ªåŠ¨åˆ›å»º**ï¼š

```yaml
volumes:
  - name: data
    hostPath:
      path: /var/lib/filebeat-data
      type: DirectoryOrCreate
```

è¿™æ®µ YAML çš„æ„æ€æ˜¯ï¼š

- å¦‚æœç›®å½•ä¸å­˜åœ¨ï¼Œå°±è‡ªåŠ¨åˆ›å»ºä¸€ä¸ªç©ºç›®å½•ï¼›
- å¦åˆ™ä½¿ç”¨å·²æœ‰çš„ç›®å½•ï¼›

å®ƒè¢«æŒ‚è½½åˆ°å®¹å™¨å†…çš„ `/usr/share/filebeat/data`ï¼Œä½œä¸º Filebeat çš„çŠ¶æ€å­˜å‚¨ã€‚



##### åœ¨kibanaä¸­åˆ›å»ºç´¢å¼•

![image-20250507020745096](../markdown_img/image-20250507020745096.png)

![image-20250507021544891](../markdown_img/image-20250507021544891.png)

**å†æ¬¡åˆ›å»ºç´¢å¼•**

![image-20250507021744980](../markdown_img/image-20250507021744980.png)

![image-20250507021830986](../markdown_img/image-20250507021830986.png)

![image-20250507022117579](../markdown_img/image-20250507022117579.png)

ç›®å‰å·²ç»å¯ä»¥å®ç°ä¸€ä¸ªç´¢å¼•å¯¹åº”ä¸€ä¸ªå‘½åç©ºé—´ï¼Œä¸€ä¸ªç´¢å¼•å¯¹åº”ä¸€ä¸ªå½“å‰å‘½åç©ºé—´ä¸‹æ‰€æœ‰podæ—¥å¿—



#### Filebeaté‡‡é›†Javaæ—¥å¿—åˆ°ESé›†ç¾¤

æ€è·¯ï¼š

- åœ¨åˆ›å»ºjavaä¸šåŠ¡podæ—¶ï¼Œç»™podå‘½åå¸¦ä¸ŠæŒ‡å®šå­—çœ¼ï¼Œæ¯”å¦‚ï¼š"java"ã€‚
- åœ¨Filebeaté‡‡é›†å™¨ä¸­ï¼Œè¿‡æ»¤javaæ—¥å¿—



##### åˆ›å»ºtomcatå®¹å™¨

```bash
# åˆ›å»ºèµ„æºæ¸…å•æ–‡ä»¶
[root@master1 ~]# mkdir /root/logging/test-java
[root@master1 ~]# cd /root/logging/test-java/
[root@master1 ~/logging/test-java]# vim tomcat.yaml
[root@master1 ~/logging/test-java]# cat tomcat.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: tomcat-java    # åç»­å°±å¯ä»¥ä½¿ç”¨*java*æ¥åŒ¹é…è¿™ä¸ªpod
  namespace: default
  labels:
    app: tomcat
spec:
  containers:
  - name: tomcat-java
    ports:
    - containerPort: 8080
    image: tomcat:8.5-jre8-alpine
    imagePullPolicy: IfNotPresen
 
# æ›´æ–°èµ„æºæ¸…å•
[root@master1 ~/logging/test-java]# kubectl apply -f tomcat.yaml 
pod/tomcat-java created

# æŸ¥çœ‹
[root@master1 ~/logging/test-java]# kubectl get pod
NAME          READY   STATUS    RESTARTS   AGE
tomcat-java   1/1     Running   0          64s
```



##### ä¿®æ”¹Filebeatçš„configmap

```bash
[root@master1 ~/logging/filebeat]# cat filebeat-configmap.yaml 
apiVersion: v1
kind: ConfigMap
metadata:
  name: filebeat-config
  namespace: logging
  labels:
    k8s-app: filebeat
data:
  filebeat.yaml: |-
    filebeat.inputs:
    - type: container
      paths:
        - /var/log/containers/*logging*.log         # é‡‡é›†æºè·¯å¾„
      fields:
        index: logging
      processors:                                   # å¯¹é‡‡é›†çš„æ—¥å¿—è¿›è¡Œå¤„ç†çš„é…ç½®
        - add_kubernetes_metadata:                  # æ·»åŠ Kubernetesç›¸å…³çš„å…ƒæ•°æ®åˆ°é‡‡é›†çš„æ—¥å¿—
            default_indexers.enabled: true          # å¯ç”¨é»˜è®¤çš„ç´¢å¼•å™¨ï¼Œï¼Œç”¨åœ¨æ—¥å¿—ä¸­æ·»åŠ ç´¢å¼•ä¿¡æ¯
            default_matchers.enabled: true          # å¯ç”¨é»˜è®¤çš„åŒ¹é…å™¨ï¼Œï¼Œç”¨äºåŒ¹é…ç›¸å…³æ—¥å¿—
            host: ${NODE_NAME}                      # configmapé‡Œçš„å†…å®¹åªæ˜¯é™æ€æ¨¡ç‰ˆï¼Œéœ€è¦åç»­æŸç§æ–¹å¼åšæ¸²æŸ“
            matchers:                               # æŒ‡å®šåŒ¹é…è§„åˆ™
            - logs_path:
                logs_path: "/var/log/containers/"

    - type: container
      paths:
        - /var/log/containers/*kube-system*.log
      fields:
        index: kube-system
      processors:
        - add_kubernetes_metadata:
            default_indexers.enabled: true
            default_matchers.enabled: true
            hosts: ${NODE_NAME}
            matchers:
            - logs_path:
                logs_path: "/var/log/containers/"

####### æ·»åŠ ä¸‹é¢é…ç½®ï¼Œç”¨æ¥åŒ¹é…javaæœåŠ¡çš„æ—¥å¿—ï¼Œå¹¶å¯¹å…¶è¿›è¡Œè°ƒæ•´ ############################
    - type: container
      paths:
        - /var/log/containers/*java*.log
      multiline.pattern: '^\d{2}'
      multiline.negate: true
      multiline.match: after
      multiline.max_lines: 10000
      fields:
        index: java
      processors:
        - add_kubernetes_metadata:
            default_indexers.enabled: true
            default_matchers.enabled: true
            hosts: ${NODE_NAME}
            matchers:
            - logs_path:
                logs_path: "/var/log/containers/"
###################################################################

    output.elasticsearch:
      hosts: ['elasticsearch.logging.svc.cluster.local:9200']
      indices:
        - index: "logging-ns-%{+yyyy.MM.dd}"
          when.equals:
            fields:
              index: "logging"
        - index: "kube-system-ns-%{+yyyy.MM.dd}"
          when.equals:
            fields:
              index: "kube-system"
########## æ·»åŠ ä¸‹é¢çš„é…ç½®ï¼Œå°†javaæ”¾å…¥æŒ‡å®šç´¢å¼•ä¸­ ####################
        - index: "java-%{+yyyy.MM.dd}"
          when.equals:
            fields:
              index: "java
              
# æ›´æ–°æ¸…å•æ–‡ä»¶
[root@master1 ~/logging/filebeat]# kubectl apply -f filebeat-configmap.yaml 
configmap/filebeat-config configured

# é‡å¯filebeat
[root@master1 ~/logging/filebeat]# kubectl rollout restart -n logging daemonset filebeat
```



##### ä½¿ç”¨Sidecarå®Œæˆæ—¥å¿—é‡‡é›†

ä¸Šè¿°tomcatçš„æœåŠ¡çš„é—®é¢˜æ˜¯å®ƒçš„è®¿é—®ä¿¡æ¯å¹¶æ²¡æœ‰è¾“å‡ºåˆ°æ ‡å‡†è¾“å‡ºï¼Œä¹Ÿå°±æ˜¯è¯´å¿…é¡»ä½¿ç”¨sidecaræ‰èƒ½é‡‡é›†

```bash
# åœ¨tomcatçš„æ¸…å•æ–‡ä»¶ä¸­åŠ å…¥sidecar
# å‡†å¤‡tomcatçš„é…ç½®æ–‡ä»¶ï¼Œå°†æ—¥å¿—æ ¼å¼å˜ä¸ºjsonè¾“å‡º
[root@master1 ~/logging/test-java]# cat server.xml 
<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed to the Apache Software Foundation (ASF) under one or more
  contributor license agreements.  See the NOTICE file distributed with
  this work for additional information regarding copyright ownership.
  The ASF licenses this file to You under the Apache License, Version 2.0
  (the "License"); you may not use this file except in compliance with
  the License.  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!-- Note:  A "Server" is not itself a "Container", so you may not
     define subcomponents such as "Valves" at this level.
     Documentation at /docs/config/server.html
 -->
<Server port="8005" shutdown="SHUTDOWN">
  <Listener className="org.apache.catalina.startup.VersionLoggerListener" />
  <!-- Security listener. Documentation at /docs/config/listeners.html
  <Listener className="org.apache.catalina.security.SecurityListener" />
  -->
  <!--APR library loader. Documentation at /docs/apr.html -->
  <Listener className="org.apache.catalina.core.AprLifecycleListener" SSLEngine="on" />
  <!-- Prevent memory leaks due to use of particular java/javax APIs-->
  <Listener className="org.apache.catalina.core.JreMemoryLeakPreventionListener" />
  <Listener className="org.apache.catalina.mbeans.GlobalResourcesLifecycleListener" />
  <Listener className="org.apache.catalina.core.ThreadLocalLeakPreventionListener" />

  <!-- Global JNDI resources
       Documentation at /docs/jndi-resources-howto.html
  -->
  <GlobalNamingResources>
    <!-- Editable user database that can also be used by
         UserDatabaseRealm to authenticate users
    -->
    <Resource name="UserDatabase" auth="Container"
              type="org.apache.catalina.UserDatabase"
              description="User database that can be updated and saved"
              factory="org.apache.catalina.users.MemoryUserDatabaseFactory"
              pathname="conf/tomcat-users.xml" />
  </GlobalNamingResources>

  <!-- A "Service" is a collection of one or more "Connectors" that share
       a single "Container" Note:  A "Service" is not itself a "Container",
       so you may not define subcomponents such as "Valves" at this level.
       Documentation at /docs/config/service.html
   -->
  <Service name="Catalina">

    <!--The connectors can use a shared executor, you can define one or more named thread pools-->
    <!--
    <Executor name="tomcatThreadPool" namePrefix="catalina-exec-"
        maxThreads="150" minSpareThreads="4"/>
    -->


    <!-- A "Connector" represents an endpoint by which requests are received
         and responses are returned. Documentation at :
         Java HTTP Connector: /docs/config/http.html
         Java AJP  Connector: /docs/config/ajp.html
         APR (HTTP/AJP) Connector: /docs/apr.html
         Define a non-SSL/TLS HTTP/1.1 Connector on port 8080
    -->
    <Connector port="8080" protocol="HTTP/1.1"
               connectionTimeout="20000"
               redirectPort="8443" />
    <!-- A "Connector" using the shared thread pool-->
    <!--
    <Connector executor="tomcatThreadPool"
               port="8080" protocol="HTTP/1.1"
               connectionTimeout="20000"
               redirectPort="8443" />
    -->
    <!-- Define a SSL/TLS HTTP/1.1 Connector on port 8443
         This connector uses the NIO implementation. The default
         SSLImplementation will depend on the presence of the APR/native
         library and the useOpenSSL attribute of the
         AprLifecycleListener.
         Either JSSE or OpenSSL style configuration may be used regardless of
         the SSLImplementation selected. JSSE style configuration is used below.
    -->
    <!--
    <Connector port="8443" protocol="org.apache.coyote.http11.Http11NioProtocol"
               maxThreads="150" SSLEnabled="true">
        <SSLHostConfig>
            <Certificate certificateKeystoreFile="conf/localhost-rsa.jks"
                         type="RSA" />
        </SSLHostConfig>
    </Connector>
    -->
    <!-- Define a SSL/TLS HTTP/1.1 Connector on port 8443 with HTTP/2
         This connector uses the APR/native implementation which always uses
         OpenSSL for TLS.
         Either JSSE or OpenSSL style configuration may be used. OpenSSL style
         configuration is used below.
    -->
    <!--
    <Connector port="8443" protocol="org.apache.coyote.http11.Http11AprProtocol"
               maxThreads="150" SSLEnabled="true" >
        <UpgradeProtocol className="org.apache.coyote.http2.Http2Protocol" />
        <SSLHostConfig>
            <Certificate certificateKeyFile="conf/localhost-rsa-key.pem"
                         certificateFile="conf/localhost-rsa-cert.pem"
                         certificateChainFile="conf/localhost-rsa-chain.pem"
                         type="RSA" />
        </SSLHostConfig>
    </Connector>
    -->

    <!-- Define an AJP 1.3 Connector on port 8009 -->
    <Connector port="8009" protocol="AJP/1.3" redirectPort="8443" />


    <!-- An Engine represents the entry point (within Catalina) that processes
         every request.  The Engine implementation for Tomcat stand alone
         analyzes the HTTP headers included with the request, and passes them
         on to the appropriate Host (virtual host).
         Documentation at /docs/config/engine.html -->

    <!-- You should set jvmRoute to support load-balancing via AJP ie :
    <Engine name="Catalina" defaultHost="localhost" jvmRoute="jvm1">
    -->
    <Engine name="Catalina" defaultHost="localhost">

      <!--For clustering, please take a look at documentation at:
          /docs/cluster-howto.html  (simple how to)
          /docs/config/cluster.html (reference documentation) -->
      <!--
      <Cluster className="org.apache.catalina.ha.tcp.SimpleTcpCluster"/>
      -->

      <!-- Use the LockOutRealm to prevent attempts to guess user passwords
           via a brute-force attack -->
      <Realm className="org.apache.catalina.realm.LockOutRealm">
        <!-- This Realm uses the UserDatabase configured in the global JNDI
             resources under the key "UserDatabase".  Any edits
             that are performed against this UserDatabase are immediately
             available for use by the Realm.  -->
        <Realm className="org.apache.catalina.realm.UserDatabaseRealm"
               resourceName="UserDatabase"/>
      </Realm>

      <Host name="localhost"  appBase="webapps"
            unpackWARs="true" autoDeploy="true">

        <!-- SingleSignOn valve, share authentication between web applications
             Documentation at: /docs/config/valve.html -->
        <!--
        <Valve className="org.apache.catalina.authenticator.SingleSignOn" />
        -->

        <!-- Access log processes all example.
             Documentation at: /docs/config/valve.html
             Note: The pattern used is equivalent to using pattern="common" -->
        <!--<Valve className="org.apache.catalina.valves.AccessLogValve" directory="logs"
               prefix="localhost_access_log" suffix=".txt"
        pattern="%h %l %u %t &quot;%r&quot; %s %b" /> -->
#################### æ—¥å¿—æ ¼å¼æ”¹ä¸ºJsonæ ¼å¼ ########################################################
        <Valve className="org.apache.catalina.valves.AccessLogValve" directory="logs"
               prefix="localhost_access_log" suffix=".txt"
               pattern="{&quot;clientip&quot;:&quot;%h&quot;,&quot;ClientUser&quot;:&quot;%l&quot;,&quot;authenticated&quot;:&quot;%u&quot;,&quot;AccessTime&quot;:&quot;%t&quot;,&quot;method&quot;:&quot;%r&quot;,&quot;status&quot;:&quot;%s&quot;,&quot;SendBytes&quot;:&quot;%b&quot;,&quot;Query?string&quot;:&quot;%q&quot;,&quot;partner&quot;:&quot;%{Referer}i&quot;,&quot;AgentVersion&quot;:&quot;%{User-Agent}i&quot;}" />

      </Host>
    </Engine>
  </Service>
</Server>

# å°†é…ç½®æ–‡ä»¶ç”Ÿæˆconfigmap,åç»­æŒ‚è½½åˆ°å®¹å™¨ä¸­
[root@master1 ~/logging/test-java]# kubectl create cm tomcat-config --from-file=./server.xml 

# å‡†å¤‡filebeat.ymlçš„configmapèµ„æºæ¸…å•æ–‡ä»¶
[root@master1 ~/logging/test-java]# cat filebeat-configmap.yaml 
apiVersion: v1
kind: ConfigMap
metadata:
  name: filebeat-config
    #  namespace: logging
  labels:
    k8s-app: filebeat
data:
  filebeat.yml: |-
    filebeat.inputs:
    - type: log
      enabled: true
      paths:
        - /tomcat-logs/localhost_access_log.*
      json.keys_under_root: true
      json.overwrite_keys: false
      tags: ["tomcat-access"]
    - type: log
      enabled: true
      paths:
        - /tomcat-logs/catalina.*.log
      tags: ["tomcat-error"]
      multiline.pattern: '^\d{2}'
      multiline.negate: true
      multiline.match: after
      multiline.max_lines: 10000


    output.elasticsearch:
      hosts: ['elasticsearch.logging.svc.cluster.local:9200']
      indices:
        - index: "tomcat-access-%{[agent.version]}-%{+yyy.MM.dd}"
          when.contains:
            tags: "tomcat-access"
        - index: "tomcat-error-%{[agent.version]}-%{+yyy.MM.dd}"
          when.contains:
            tags: "tomcat-error"
    setup.ilm.enabled: false
    setup.template.name: "tomcat"
    setup.template.pattern: "tomcat-*"
    
# æ›´æ–°èµ„æºæ¸…å•æ–‡ä»¶
[root@master1 ~/logging/test-java]# kubectl apply -f filebeat-configmap.yaml

# å‡†å¤‡tomcatå’Œsidecarçš„èµ„æºæ¸…å•æ–‡ä»¶
[root@master1 ~/logging/test-java]# cat filebeat-configmap.yaml 
apiVersion: v1
kind: ConfigMap
metadata:
  name: filebeat-config
    #  namespace: logging
  labels:
    k8s-app: filebeat
data:
  filebeat.yml: |-
    filebeat.inputs:
    - type: log
      enabled: true
      paths:
        - /tomcat-logs/localhost_access_log.*
      json.keys_under_root: true
      json.overwrite_keys: false
[root@master1 ~/logging/test-java]# kubectl apply -f filebeat-configmap.yaml ^C
[root@master1 ~/logging/test-java]# cat tomcat.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: tomcat-java    # åç»­å°±å¯ä»¥ä½¿ç”¨*java*æ¥åŒ¹é…è¿™ä¸ªpod
  namespace: default
  labels:
    app: tomcat
spec:
  containers:
  - name: tomcat-java
    ports:
    - containerPort: 8080
    image: harbor.magedu.mysticalrecluse.com/k8simage/tomcat:8.5-jre8-alpine
    imagePullPolicy: IfNotPresent
    volumeMounts:
      - name: tomcat-logs
        mountPath: /usr/local/tomcat/logs
      - name: tomcat-config
        mountPath: /usr/local/tomcat/conf/server.xml
        subPath: server.xml
  - name: filebeat
    image: harbor.magedu.mysticalrecluse.com/k8simage/filebeat:7.17.12
    volumeMounts:
      - name: tomcat-logs
        mountPath: /tomcat-logs
      - name: config
        mountPath: /usr/share/filebeat/filebeat.yml
        subPath: filebeat.yml
        readOnly: true
  volumes:
  - name: config
    configMap:
      defaultMode: 0640
      name: filebeat-config
  - name: tomcat-config
    configMap:
      defaultMode: 0640
      name: tomcat-config
  - name: tomcat-logs
    emptyDir: {}
    
# æ›´æ–°èµ„æºæ¸…å•æ–‡ä»¶
[root@master1 ~/logging/test-java]# kubectl apply -f tomcat.yaml 

# æŸ¥çœ‹
[root@master1 ~/logging/test-java]# kubectl get pod
NAME          READY   STATUS    RESTARTS   AGE
tomcat-java   2/2     Running   0          10m
```

**æŸ¥çœ‹Elasticsearchçš„ç´¢å¼•ç”Ÿæˆæƒ…å†µ**

![image-20250507151320293](../markdown_img/image-20250507151320293.png)

**åç»­åœ¨Kibanaä¸Šå±•ç¤ºï¼ˆè¿‡ç¨‹å¦‚ä¸Šï¼‰**

![image-20250507151604289](../markdown_img/image-20250507151604289.png)



#### Filebeaté‡‡é›†æ—¥å¿—åˆ°Kafka

##### å®‰è£…Kafkaé›†ç¾¤

```bash
# åœ¨kubernetesçš„ç®¡ç†èŠ‚ç‚¹éƒ¨ç½²helm
[root@master1 ~]# wget -P /usr/local/src https://get.helm.sh/helm-v3.17.2-linux-amd64.tar.gz
[root@master1 ~]# tar xf /usr/local/src/helm-v3.17.2-linux-amd64.tar.gz -C /usr/local/
[root@master1 ~]# ls /usr/local/linux-amd64/
helm  LICENSE  README.md
[root@master1 ~]# ln -s /usr/local/linux-amd64/helm /usr/local/bin/

# helm-v3ç‰ˆæœ¬æ˜¾ç¤ºæ•ˆæœå¦‚ä¸‹
[root@master1 ~]#helm version
version.BuildInfo{Version:"v3.17.2", GitCommit:"cc0bbbd6d6276b83880042c1ecb34087e84d41eb", GitTreeState:"clean", GoVersion:"go1.23.7"}

# Helmå‘½ä»¤è¡¥ä¼š,é‡æ–°ç™»å½•ç”Ÿæ•ˆ
# æ–¹æ³•1
[root@master1 ~]# echo 'source <(helm completion bash)' >> .bashrc && exit

# æ·»åŠ bitnamiä»“åº“åˆ°helm
[root@master1 ~/logging/kafka]# helm repo add bitnami https://charts.bitnami.com/bitnami
"bitnami" has been added to your repositories

# åˆ›å»ºkafkaç›®å½•
[root@master1 ~/logging]# mkdir /root/logging/kafka
[root@master1 ~/logging]# cd /root/logging/kafka/

# ä¸ºæ¨¡æ‹Ÿå®é™…ç”Ÿäº§ç¯å¢ƒï¼Œéƒ¨ç½²ä¸€ä¸ªè€ä¸€ç‚¹çš„ç‰ˆæœ¬
[root@master1 ~/logging/kafka]# helm pull bitnami/kafka --version 23.0.5
[root@master1 ~/logging/kafka]# helm pull bitnami/zookeeper --version 11.4.7

[root@master1 ~/logging/kafka]# ls
kafka-23.0.5.tgz  zookeeper-11.4.7.tgz 

[root@master1 ~/logging/kafka]# tar xf kafka-23.0.5.tgz 
[root@master1 ~/logging/kafka]# tar xf zookeeper-11.4.7.tgz

[root@master1 ~/logging/kafka]# cd zookeeper/
[root@master1 ~/logging/kafka/zookeeper]# ls
Chart.lock  Chart.yaml  README.md  charts  templates  values.yaml

[root@master1 ~/logging/kafka/zookeeper]# vim values.yaml
# æ·»åŠ æ—¶åŒº
extraEnvVars:
  - name: TZ
    value: "Asia/Shanghai"
    
# è¿è¡Œä»»æ„ç”¨æˆ·è¿æ¥ï¼ˆé»˜è®¤ä¸å­˜åœ¨ï¼‰
allowAnonymousLogin: true

# å…³é—­è®¤è¯ï¼ˆé»˜è®¤å…³é—­ï¼‰
auth:
    enabled: false
    
# ä¿®æ”¹å‰¯æœ¬æ•°
replicaCount: 3

# é…ç½®æŒä¹…åŒ–ï¼ŒæŒ‰éœ€ä½¿ç”¨
persistence:
  enabled: true
  existingClaim: ""
  storageClass: "sc-nfs"
  accessModes:
    - ReadWriteOnce
  size: 10Gi
  annotations: {}
  
[root@master1 ~/logging/kafka/zookeeper]# helm install zookeeper -n logging .
NAME: zookeeper
LAST DEPLOYED: Wed May  7 16:51:34 2025
NAMESPACE: logging
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
CHART NAME: zookeeper
CHART VERSION: 11.4.7
APP VERSION: 3.8.1

** Please be patient while the chart is being deployed **

ZooKeeper can be accessed via port 2181 on the following DNS name from within your cluster:

    zookeeper.logging.svc.cluster.local

To connect to your ZooKeeper server run the following commands:

    export POD_NAME=$(kubectl get pods --namespace logging -l "app.kubernetes.io/name=zookeeper,app.kubernetes.io/instance=zookeeper,app.kubernetes.io/component=zookeeper" -o jsonpath="{.items[0].metadata.name}")
    kubectl exec -it $POD_NAME -- zkCli.sh

To connect to your ZooKeeper server from outside the cluster execute the following commands:

    kubectl port-forward --namespace logging svc/zookeeper 2181:2181 &
    zkCli.sh 127.0.0.1:2181
    
# æŸ¥çœ‹
[root@master1 ~/logging/kafka/zookeeper]# kubectl get pod -n logging 
NAME                       READY   STATUS    RESTARTS   AGE
cerebro-67d4d8bc85-ktwrr   1/1     Running   0          39h
es-cluster-0               1/1     Running   0          41h
es-cluster-1               1/1     Running   0          41h
es-cluster-2               1/1     Running   0          41h
filebeat-g6nzq             1/1     Running   0          5h11m
filebeat-v8h5f             1/1     Running   0          5h11m
filebeat-vqxbm             1/1     Running   0          5h12m
filebeat-xg2p5             1/1     Running   0          5h12m
kibana-7b5ff7fb95-8mrgj    1/1     Running   0          31h
zookeeper-0                1/1     Running   0          40s
zookeeper-1                1/1     Running   0          40s
zookeeper-2                1/1     Running   0          40s

# è¿›å…¥kafkaçš„ç›®å½•ï¼Œä¿®æ”¹values.yaml
[root@master1 ~/logging/kafka/zookeeper]# cd ../kafka/
[root@master1 ~/logging/kafka/kafka]# vim values.yaml 
# æ·»åŠ æ—¶åŒº
extraEnvVars: 
  - name: TZ
    value: "Asia/Shanghai"
    
# å…³é—­kraftæ¨¡å¼
# Kafkaä»ç‰ˆæœ¬2.8.0å¼€å§‹å¼•å…¥äº†ä¸€ç§æ–°çš„å­˜å‚¨æ¨¡å¼ï¼Œç§°ä¸ºKraftæ¨¡å¼ã€‚Kraftæ¨¡å¼æ˜¯ä¸€ç§å¯å¤åˆ¶ï¼Œé«˜å¯ç”¨çš„å­˜å‚¨æ¨¡å¼ï¼Œå®ƒæ›¿ä»£äº†ä¼ ç»Ÿçš„Zookeeperä¾èµ–ï¼Œå¹¶æä¾›äº†æ›´å¥½çš„å®¹é”™æ€§ï¼Œå®¹é‡å’Œå¯æ‰©å±•æ€§ã€‚è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨Zookeeperä¾èµ–ï¼Œæ‰€ä»¥éœ€è¦ç¦ç”¨kraftæ¨¡å¼
kraft:
  enabled: false
  
# ä¿®æ”¹å‰¯æœ¬æ•°
replicaCount: 3

# é…ç½®æŒä¹…åŒ–ï¼ŒæŒ‰éœ€ä½¿ç”¨
persistence:
 enabled:true
 storageClass: "sc-nfs" # storageClass
 accessModes:-ReadWriteOnce
 size: 20Gi
 annotations:{}

# ä½¿ç”¨Zookeeperå¤–éƒ¨è¿æ¥
externalZookeeper:
  servers: zookeeper
  
# é«˜å¯ç”¨é…ç½®
# é»˜è®¤åˆ†åŒºæ•°ï¼Œé»˜è®¤å‰¯æœ¬æ•°ï¼Œæ—¥å¿—è¿‡æœŸæ—¶é—´ã€‚ï¼ˆéœ€æ ¹æ®kafkaèŠ‚ç‚¹æ•°è®¾å®šï¼‰
# å…è®¸åˆ é™¤topicï¼ˆæŒ‰éœ€å¼€å¯ï¼‰
deleteTopicEnable: true

# æ—¥å¿—ä¿ç•™æ—¶é—´ï¼ˆé»˜è®¤ä¸€å‘¨ï¼‰
logRetentionHours: 168

# è‡ªåŠ¨åˆ›å»ºtopicæ—¶çš„é»˜è®¤å‰¯æœ¬æ•°
defaultReplicationFactor: 1

# ç”¨äºé…ç½®offsetè®°å½•çš„topicçš„partitionçš„å‰¯æœ¬ä¸ªæ•°
offsetsTopicReplicationFactor: 1

# äº‹åŠ¡ä¸»é¢˜çš„å¤åˆ¶å› å­
transactionStateLogReplicationFactor: 1

# æ–°å»ºTopicæ—¶é»˜è®¤çš„åˆ†åŒºæ•°
numPartitions: 1

# å¯ç”¨kafka
[root@master1 ~/logging/kafka/kafka]# helm install kafka -n logging .
NAME: kafka
LAST DEPLOYED: Wed May  7 17:27:18 2025
NAMESPACE: logging
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
CHART NAME: kafka
CHART VERSION: 23.0.5
APP VERSION: 3.5.0

** Please be patient while the chart is being deployed **

Kafka can be accessed by consumers via port 9092 on the following DNS name from within your cluster:

    kafka.logging.svc.cluster.local

Each Kafka broker can be accessed by producers via port 9092 on the following DNS name(s) from within your cluster:

    kafka-0.kafka-headless.logging.svc.cluster.local:9092
    kafka-1.kafka-headless.logging.svc.cluster.local:9092
    kafka-2.kafka-headless.logging.svc.cluster.local:9092

To create a pod that you can use as a Kafka client run the following commands:

    kubectl run kafka-client --restart='Never' --image harbor.magedu.mysticalrecluse.com/k8simage/kafka:3.5.0-debian-11-r21 --namespace logging --command -- sleep infinity
    kubectl exec --tty -i kafka-client --namespace logging -- bash

    PRODUCER:
        kafka-console-producer.sh \
            --broker-list kafka-0.kafka-headless.logging.svc.cluster.local:9092,kafka-1.kafka-headless.logging.svc.cluster.local:9092,kafka-2.kafka-headless.logging.svc.cluster.local:9092 \
            --topic test

    CONSUMER:
        kafka-console-consumer.sh \
            --bootstrap-server kafka.logging.svc.cluster.local:9092 \
            --topic test \
            --from-beginning

# æŸ¥çœ‹
[root@master1 ~]# kubectl get pod -n logging 
NAME                       READY   STATUS    RESTARTS   AGE
cerebro-67d4d8bc85-ktwrr   1/1     Running   0          39h
es-cluster-0               1/1     Running   0          42h
es-cluster-1               1/1     Running   0          42h
es-cluster-2               1/1     Running   0          42h
filebeat-g6nzq             1/1     Running   0          5h41m
filebeat-v8h5f             1/1     Running   0          5h41m
filebeat-vqxbm             1/1     Running   0          5h41m
filebeat-xg2p5             1/1     Running   0          5h41m
kafka-0                    1/1     Running   0          6m11s
kafka-1                    1/1     Running   0          6m11s
kafka-2                    1/1     Running   0          6m11s
kibana-7b5ff7fb95-8mrgj    1/1     Running   0          32h
zookeeper-0                1/1     Running   0          29m
zookeeper-1                1/1     Running   0          29m
zookeeper-2                1/1     Running   0          29m

# æµ‹è¯•kafka
# 1. åˆ›å»ºkafka-0å®¹å™¨
[root@master1 ~]# kubectl exec -it kafka-0 -n logging -- bash

# åˆ›å»ºä¸»é¢˜ï¼Œåˆ›å»ºä¸€ä¸ªåˆ†åŒºæ•°ä¸º1ï¼Œå‰¯æœ¬ä¸º1ï¼Œåç§°ä¸ºtestçš„ä¸»é¢˜
I have no name!@kafka-0:/# kafka-topics.sh --bootstrap-server localhost:9092 --create --topic test --partitions 1 --replication-factor 1
Created topic test.

# é€€å‡ºkafka-0å®¹å™¨
I have no name!@kafka-0:/# exit
exit

# è¿›å…¥kafka-1å®¹å™¨
[root@master1 ~]# kubectl exec -it kafka-1 -n logging -- bash

# æŸ¥çœ‹ä¸»é¢˜
I have no name!@kafka-1:/# kafka-topics.sh --list --bootstrap-server localhost:9092
test
```



#### Kafkaç›‘æ§ç®¡ç†å¹³å°ï¼ˆKafka-Eagleï¼‰

##### åŸºäºKuberneteséƒ¨ç½²kafka-eagle

**éƒ¨ç½²MySQL Pod**

```bash
# åˆ›å»ºèµ„æºæ¸…å•æ–‡ä»¶
[root@master1 ~/logging/kafka-eagle]# cat mysql.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pvc
  namespace: logging
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
  storageClassName: sc-nfs
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql
  namespace: logging
spec:
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
        - image: mysql:5.7
          name: mysql
          env:
            - name: MYSQL_ROOT_PASSWORD
              value: Magedu123..
          ports:
            - name: tcp-3306
              containerPort: 3306
          volumeMounts:
            - name: host-time
              readOnly: true
              mountPath: /etc/localtime
            - name: mysql-data
              mountPath: /var/lib/mysql
      volumes:
        - name: host-time
          hostPath:
            path: /etc/localtime
        - name: mysql-data
          persistentVolumeClaim:
            claimName: mysql-pvc
      restartPolicy: Always
---
apiVersion: v1
kind: Service
metadata:
  name: mysql-svc
  namespace: logging
spec:
  clusterIP: None
  selector:
    app: mysql
  ports:
    - name: tcp
      port: 3306
      targetPort: 3306
      
# æ›´æ–°èµ„æºæ¸…å•
[root@master1 ~/logging/kafka-eagle]# kubectl apply -f mysql.yaml

# è¿›å…¥mysqlå®¹å™¨ï¼Œåˆ›å»ºkafkaæ•°æ®è¡¨
[root@master1 ~/logging/kafka-eagle]# kubectl exec -it -n logging mysql-79f6cc4bb7-l4wjh -- bash
bash-4.2# mysql -uroot -pMagedu123..
mysql: [Warning] Using a password on the command line interface can be insecure.
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 2
Server version: 5.7.44 MySQL Community Server (GPL)

Copyright (c) 2000, 2023, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> create database kafka;
Query OK, 1 row affected (0.06 sec)

mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| kafka              |
| mysql              |
| performance_schema |
| sys                |
+--------------------+
5 rows in set (0.04 sec)

mysql> exit
Bye
```



##### åˆ›å»ºkafka-eagle configmap

```bash
# åˆ›å»ºsystem-config.propertiesæ–‡ä»¶
[root@master1 ~/logging/kafka-eagle]# cat system-config.properties 
 ######################################
 #multi zookeeper&kafkaclusterlist
 ######################################
 kafka.eagle.zk.cluster.alias=cluster1
 # zookeeper svc:Port
 cluster1.zk.list=zookeeper:2181

 ######################################
 #zk clientthreadlimit
 ######################################
 # å®¢æˆ·ç«¯çº¿ç¨‹çš„é™åˆ¶
 kafka.zk.limit.size=25

 ######################################
 #kafka eaglewebui port
 ######################################
 # Web UI çš„ç«¯å£
 kafka.eagle.webui.port=8048

 ######################################
 #kafka offsetstorage
 ######################################
 # kafkaçš„åé‡å­˜å‚¨
 cluster1.kafka.eagle.offset.storage=kafka

 ######################################
 #enable kafka metrics
 ######################################
 # å¯ç”¨KafkaæŒ‡æ ‡å›¾è¡¨ã€‚
 kafka.eagle.metrics.charts=true
 # å¯ç”¨SQLä¿®å¤é”™è¯¯åŠŸèƒ½ã€‚
 kafka.eagle.sql.fix.error=true

 ######################################
 #kafka sqltopicrecordsmax
 ######################################
 # kafkaæ³¨æ„è®°å½•æ•°ï¼Œæœ€å¤§5000
 kafka.eagle.sql.topic.records.max=5000

 ######################################
 #alarm emailconfigure
 ######################################
 kafka.eagle.mail.enable=false
 kafka.eagle.mail.sa=alert_sa@163.com
 kafka.eagle.mail.username=alert_sa@163.com
 kafka.eagle.mail.password=mqslimczkdqabbbh
 kafka.eagle.mail.server.host=smtp.163.com
 kafka.eagle.mail.server.port=25

 ######################################
 #delete kafkatopic token
 ######################################
 # ï¼šåˆ é™¤Kafkaä¸»é¢˜æ‰€éœ€çš„ä»¤ç‰Œè®¾ç½®ä¸ºâ€œkeadminâ€
 kafka.eagle.topic.token=keadmin

 ######################################
 #kafka saslauthenticate
 ######################################
 # è¿™è¡Œä»£ç è¡¨ç¤ºSASLè®¤è¯åœ¨â€œcluster1â€è¿™ä¸ªKafkaé›†ç¾¤ä¸Šæ˜¯ç¦ç”¨çš„ã€‚falseè¡¨ç¤ºä¸å¯ç”¨ã€‚
 cluster1.kafka.eagle.sasl.enable=false
 # ï¼šè¿™è¡Œä»£ç è®¾ç½®äº†SASLåè®®ä¸ºâ€œSASL_PLAINTEXTâ€ï¼Œè¿™æ„å‘³ç€ä½¿ç”¨æ˜æ–‡åè®®è¿›è¡ŒSASLè®¤è¯ã€‚
 cluster1.kafka.eagle.sasl.protocol=SASL_PLAINTEXT
 # è¿™è¡Œä»£ç è®¾ç½®äº†SASLæœºåˆ¶ä¸ºâ€œPLAINâ€ï¼ŒPlainSASLæœºåˆ¶æ˜¯å…¶ä¸­ä¸€ç§ç®€å•çš„è®¤è¯æœºåˆ¶ï¼Œé€šå¸¸ç”¨äºæµ‹è¯•å’Œå¼€å‘ç¯å¢ƒ
 cluster1.kafka.eagle.sasl.mechanism=PLAIN
 # è¿™è¡Œä»£ç è®¾ç½®äº†JASSé…ç½®æ–‡ä»¶ä¸ºâ€œkafka_client_jaas.confâ€ã€‚è¿™ä¸ªé…ç½®æ–‡ä»¶é€šå¸¸åŒ…å«ç”¨äºSASLè®¤è¯çš„ç”¨æˆ·åå’Œå¯†ç ç­‰ä¿¡æ¯ã€‚
 cluster1.kafka.eagle.sasl.jaas.config=kafka_client_jaas.conf
 
 # æ€»çš„æ¥è¯´ï¼Œè¿™æ®µä»£ç æ˜¯åœ¨é…ç½®Kafkaçš„SASLè®¤è¯ï¼Œä½†ç›®å‰å®ƒæ˜¯ç¦ç”¨çš„ï¼Œå¹¶ä¸”ä½¿ç”¨äº†æ˜æ–‡åè®®å’ŒPlainæœºåˆ¶è¿›è¡Œè®¤è¯ã€‚
 ######################################
 #kafka jdbcdriveraddress
 ######################################
 kafka.eagle.driver=com.mysql.jdbc.Driver
 # kafkaè¿æ¥æ•°æ®åº“
 kafka.eagle.url=jdbc:mysql://mysql-svc:3306/kafka
 kafka.eagle.username=root
 kafka.eagle.password=Magedu123..
 
# åˆ›å»ºkafka_client_jaas.conf æ–‡ä»¶
[root@master1 ~/logging/kafka-eagle]# cat kafka_client_jaas.conf 
KafkaClient {
 org.apache.kafka.common.security.plain.PlainLoginModule required
 username="admin"
 password="admin-secret";
};

# åˆ›å»ºconfigmap
[root@master1 ~/logging/kafka-eagle]# kubectl create cm kafka-eagle-config -n logging --from-file=./kafka_client_jaas.conf --from-file=system-config.properties 
configmap/kafka-eagle-config created
```



##### åˆ›å»ºkafka-eagle podå®¹å™¨

```bash
# åˆ›å»ºèµ„æºæ¸…å•æ–‡ä»¶
[root@master1 ~/logging/kafka-eagle]# cat kafka-eagle.yaml 
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kafka-eagle
  namespace: logging
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kafka-eagle
  template:
    metadata:
      labels:
        app: kafka-eagle
    spec:
      containers:
      - image: buzhiyun/kafka-eagle:latest
        name: kafka-eagle
        ports:
        - name: kafka-eagle
          protocol: TCP
          containerPort: 8048
        volumeMounts:
        - mountPath: /opt/kafka-eagle/conf
          name: conf
      restartPolicy: Always
      volumes:
      - name: conf
        configMap:
          name: kafka-eagle-config
---
apiVersion: v1
kind: Service
metadata:
  name: kafka-eagle
  namespace: logging
spec:
  type: NodePort
  ports:
    - port: 8048
      targetPort: 8048
      nodePort: 30048
  selector:
    app: kafka-eagle
    
# æ›´æ–°èµ„æºæ¸…å•
[root@master1 ~/logging/kafka-eagle]# kubectl apply -f kafka-eagle.yaml 
deployment.apps/kafka-eagle created
service/kafka-eagle created

#æµè§ˆå™¨è®¿é—®æµ‹è¯•kafka-eagleç®¡ç†å¹³å°
workerIP:30048/ke
```

![image-20250507205252264](../markdown_img/image-20250507205252264.png)

**ç™»å½•ç”¨æˆ·åï¼šadminï¼›ç™»å½•å¯†ç ï¼š123456**

![image-20250507205507990](../markdown_img/image-20250507205507990.png)

**ç‚¹å‡»å·¦ä¾§æ BScreen**

![image-20250507205623241](../markdown_img/image-20250507205623241.png)



#### Filebeaté‡‡é›†æ—¥å¿—åˆ°Kafka

```ABAP
æ³¨æ„ï¼šå…ˆåˆ é™¤filebeat podï¼Œåˆ é™¤ESï¼ŒKibanaä¸­åˆ›å»ºçš„ç´¢å¼•
```

```bash
# åˆ é™¤filebeat pod å’Œ configMap
[root@master1 ~/logging/filebeat]# kubectl delete -f filebeat-configmap.yaml 
configmap "filebeat-config" deleted
[root@master1 ~/logging/filebeat]# kubectl delete -f filebeat-daemonset.yaml 
daemonset.apps "filebeat" deleted
```

##### å°†ä¹‹å‰ESé‡Œçš„ç´¢å¼•å…¨éƒ¨åˆ é™¤

![image-20250507210540810](../markdown_img/image-20250507210540810.png)

åŸºæœ¬å…¨éƒ¨åˆ æ‰ï¼Œæ–¹ä¾¿åé¢çœ‹æ•ˆæœï¼Œå°†filebeatçš„dataç›®å½•ä¹Ÿåˆ é™¤ï¼Œåˆ°æ—¶å€™æ‰€æœ‰æ—¥å¿—é‡æ–°å¯¼å…¥

```bash
[root@work1]# rm -rf /var/lib/filebeat-data/*
[root@work2]# rm -rf /var/lib/filebeat-data/*
[root@work3]# rm -rf /var/lib/filebeat-data/*
```

å°†Kibanaä¸­ä¹‹å‰åˆ›å»ºçš„ç´¢å¼•æ¸…ç†æ‰

![image-20250507211247689](../markdown_img/image-20250507211247689.png)

![image-20250507211312740](../markdown_img/image-20250507211312740.png)



##### ä¿®æ”¹filebeat configmapæ–‡ä»¶ï¼Œéƒ¨ç½²filebeat

```bash
[root@master1 ~/logging/filebeat]# vim filebeat-configmap.yaml 
apiVersion: v1
kind: ConfigMap
metadata:
  name: filebeat-config
  namespace: logging
  labels:
    k8s-app: filebeat
data:
  filebeat.yml: |-
    filebeat.inputs:
    - type: container
      paths:
        - /var/log/containers/*logging*.log         # é‡‡é›†æºè·¯å¾„
      fields:
        index: logging
        topic: logging                              # æ·»åŠ è¿™è¡Œ
      processors:                                   # å¯¹é‡‡é›†çš„æ—¥å¿—è¿›è¡Œå¤„ç†çš„é…ç½®
        - add_kubernetes_metadata:                  # æ·»åŠ Kubernetesç›¸å…³çš„å…ƒæ•°æ®åˆ°é‡‡é›†çš„æ—¥å¿—
            default_indexers.enabled: true          # å¯ç”¨é»˜è®¤çš„ç´¢å¼•å™¨ï¼Œï¼Œç”¨åœ¨æ—¥å¿—ä¸­æ·»åŠ ç´¢å¼•ä¿¡æ¯
            default_matchers.enabled: true          # å¯ç”¨é»˜è®¤çš„åŒ¹é…å™¨ï¼Œï¼Œç”¨äºåŒ¹é…ç›¸å…³æ—¥å¿—
            host: ${NODE_NAME}                      # configmapé‡Œçš„å†…å®¹åªæ˜¯é™æ€æ¨¡ç‰ˆï¼Œéœ€è¦åç»­æŸç§æ–¹å¼åšæ¸²æŸ“
            matchers:                               # æŒ‡å®šåŒ¹é…è§„åˆ™
            - logs_path:
                logs_path: "/var/log/containers/"

    - type: container
      paths:
        - /var/log/containers/*kube-system*.log
      fields:
        index: kube-system
        topic: kube-system                          # æ·»åŠ è¿™è¡Œ
      processors:
        - add_kubernetes_metadata:
            default_indexers.enabled: true
            default_matchers.enabled: true
            hosts: ${NODE_NAME}
            matchers:
            - logs_path:
                logs_path: "/var/log/containers/"

    - type: container
      paths:
        - /var/log/containers/*java*.log
      multiline.pattern: '^\d{2}'
      multiline.negate: true
      multiline.match: after
      multiline.max_lines: 10000
      fields:
        index: java
        topic: java                                 # æ·»åŠ è¿™è¡Œ
      processors:
        - add_kubernetes_metadata:
            default_indexers.enabled: true
            default_matchers.enabled: true
            hosts: ${NODE_NAME}
            matchers:
            - logs_path:
                logs_path: "/var/log/containers/"


    output.kafka:
      hosts: ["kafka.logging.svc.cluster.local:9092"]
      topic: '%{[fields.topic]}'
      
# é‡æ–°éƒ¨ç½²filebeat configmapèµ„æº
[root@master1 ~/logging/filebeat]# kubectl apply -f .
configmap/filebeat-config created
daemonset.apps/filebeat created
clusterrolebinding.rbac.authorization.k8s.io/filebeat unchanged
rolebinding.rbac.authorization.k8s.io/filebeat unchanged
rolebinding.rbac.authorization.k8s.io/filebeat-kubeadm-config unchanged
clusterrole.rbac.authorization.k8s.io/filebeat unchanged
role.rbac.authorization.k8s.io/filebeat unchanged
role.rbac.authorization.k8s.io/filebeat-kubeadm-config unchanged
serviceaccount/filebeat unchanged
```

##### è¿›å…¥åˆ°kafkaå®¹å™¨å†…ï¼ŒæŸ¥è¯¢ä¸»é¢˜

```bash
# è¿›å…¥kafkaå®¹å™¨
I have no name!@kafka-0:/$ kafka-topics.sh --list --bootstrap-server localhost:9092
java
kube-system
logging
test

# æŸ¥çœ‹è¡¨ç›˜æ•°æ®
```

![image-20250507212548756](../markdown_img/image-20250507212548756.png)



#### LogStashæ¶ˆæ¯åˆ°ES

éƒ¨ç½²Logstashä»kafkaä¸­è¯»å–æ•°æ®ï¼Œç„¶åå‘é€åˆ°ESé›†ç¾¤

##### åˆ›å»ºLogstash-configmap

æ³¨æ„ï¼š

- topicsè¦ä¸å‰é¢è¾“å…¥åˆ°kafkaçš„topicsä¸€è‡´
- kafkaé›†ç¾¤åœ°å€å’ŒESé›†ç¾¤åœ°å€è¦å¡«å†™æ­£ç¡®ï¼Œå¦‚æœä¸é€šï¼Œé‚£ä¹ˆä¼šå‡ºç°è¯»å–å¤±è´¥æˆ–è€…å‘é€å¤±è´¥ã€‚å…·ä½“ä¿å­˜æŸ¥çœ‹Podæ—¥å¿—è¿›è¡Œæ’æŸ¥
- Logstashçš„ç‰ˆæœ¬è¦ä¸Elasticsearchç‰ˆæœ¬ä¸€è‡´

```bash
# åˆ›å»ºèµ„æºæ¸…å•å­˜æ”¾ä½ç½®
[root@master1 ~/logging/filebeat]# mkdir /root/logging/logstash
[root@master1 ~/logging/filebeat]# cd /root/logging/logstash/

# åˆ›å»ºlogstash configmapèµ„æºæ¸…å•æ–‡ä»¶
[root@master1 ~/logging/logstash]# vim logstash-configmap.ya# æ›´æ–°æ¸…å•æ–‡ä»¶

apiVersion: v1
kind: ConfigMap
metadata:
  name: logstash-k8s-config
  namespace: logging
data:
  containers.conf: |
    input {
      kafka {
        codec => "json"
        topics => ["logging"]
        bootstrap_servers => ["kafka.logging.svc.cluster.local:9092"]
        type => "logging"
      }
      kafka {
        codec => "json"
        topics => ["kube-system"]
        bootstrap_servers => ["kafka.logging.svc.cluster.local:9092"]
        type => "kube-system"
      }
      kafka {
        codec => "json"
        topics => ["java"]
        bootstrap_servers => ["kafka.logging.svc.cluster.local:9092"]
        type => "java"
      }
    }

    output {
      if [type] == "logging" {
        elasticsearch {
          hosts => ["elasticsearch.logging.svc.cluster.local:9200"]
          index => "logging-%{+YYYY.MM.dd}"
        }
      }
      if [type] == "kube-system" {
        elasticsearch {
          hosts => ["elasticsearch.logging.svc.cluster.local:9200"]
          index => "kube-system-%{+YYYY.MM.dd}"
        }
      }
      if [type] == "java" {
        elasticsearch {
          hosts => ["elasticsearch.logging.svc.cluster.local:9200"]
          index => "java-%{+YYYY.MM.dd}"
        }
      }
    }
    
# æ›´æ–°æ¸…å•æ–‡ä»¶
[root@master1 ~/logging/logstash]# kubectl apply -f logstash-configmap.yaml 
configmap/logstash-k8s-config created

# åˆ›å»ºlogstash deployment
[root@master1 ~/logging/logstash]# cat logstash-configmap.yaml 
apiVersion: v1
kind: ConfigMap
metadata:
  name: logstash-k8s-config
  namespace: logging
data:
  containers.conf: |
    input {
      kafka {
        codec => "json"
        topics => ["logging"]
        bootstrap_servers => ["kafka.logging.svc.cluster.local:9092"]
        type => "logging"
      }
      kafka {
        codec => "json"
        topics => ["kube-system"]
        bootstrap_servers => ["kafka.logging.svc.cluster.local:9092"]
        type => "kube-system"
      }
      kafka {
        codec => "json"
[root@master1 ~/logging/logstash]# ls
logstash-configmap.yaml
[root@master1 ~/logging/logstash]# kubectl apply -f logstash-configmap.yaml 
configmap/logstash-k8s-config created
[root@master1 ~/logging/logstash]# vim logstash-deployment.yaml
[root@master1 ~/logging/logstash]# cat logstash-deployment.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: logstash
  namespace: logging
spec:
  replicas: 1
  selector:
    matchLabels:
      app: logstash
  template:
    metadata:
      labels:
        app: logstash
    spec:
      containers:
      - name: logstash
        image: logstash:7.17.10
        volumeMounts:
        - name: config
          mountPath: /opt/logstash/config/containers:conf
          subPath: containers.conf
        command:
        - "/bin/sh"
        - "-c"
        - "/opt/logstash/bin/logstash -f /opt/logstash/config/containers.conf >/dev/null"
      volumes:
      - name: config
        configMap:
          name: logstash-k8s-config

# æ›´æ–°èµ„æºæ¸…å•
[root@master1 ~/logging/logstash]# kubectl apply -g logstash-deployment.yaml

# æŸ¥çœ‹
[root@master1 ~/logging/filebeat]# kubectl get pod -n logging 
NAME                           READY   STATUS    RESTARTS   AGE
cerebro-67d4d8bc85-ktwrr       1/1     Running   0          46h
es-cluster-0                   1/1     Running   0          2d
es-cluster-1                   1/1     Running   0          2d
es-cluster-2                   1/1     Running   0          2d
filebeat-4jk7q                 1/1     Running   0          7m40s
filebeat-6g59h                 1/1     Running   0          7m40s
filebeat-c9fd7                 1/1     Running   0          7m40s
filebeat-sgtbb                 1/1     Running   0          7m40s
kafka-0                        1/1     Running   0          6h38m
kafka-1                        1/1     Running   0          6h38m
kafka-2                        1/1     Running   0          6h38m
kafka-eagle-847db5d5bd-l6w2l   1/1     Running   0          3h16m
kibana-7b5ff7fb95-8mrgj        1/1     Running   0          38h
logstash-7f4c9b6bdc-5csqn      1/1     Running   0          17m
mysql-79f6cc4bb7-l4wjh         1/1     Running   0          5h53m
zookeeper-0                    1/1     Running   0          7h1m
zookeeper-1                    1/1     Running   0          7h1m
zookeeper-2                    1/1     Running   0          7h1m

# æŸ¥çœ‹Cerebroï¼Œå‡ºç°ç´¢å¼•ï¼Œåˆ™è¯æ˜æˆåŠŸï¼Œåç»­åœ¨Kibanaåˆ›å»ºç´¢å¼•
```

![image-20250508002000753](../markdown_img/image-20250508002000753.png)

![image-20250508001915166](../markdown_img/image-20250508001915166.png)
