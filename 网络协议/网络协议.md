## ARP

## Gentuitous ARP 

## STP（最小生成树）

## TCP
### TCP/IP协议发展
![alt text](images\image04.png)
- 最初在1973年，提出了TCPv1版本，此时还没有提出IP协议，IP协议所有的功能都融合在TCP协议中，可以查看(RFC675文档)，该文档描述了TCPv1所有的功能
- 1977年，推出了TCPv2
- 1978年，推出了TCPv3
  - 1978年，国际标准化组织ISO的与会专家提出来当下的TPC协议没有安装OSI进行分层，把网络层和传输层放在同一套协议中，他建议进行分层
- 所以，在1978年开始，历经2年，到1980年，TPC被分层，推出了TCPv4 + IPv4,这就是IPv4是怎么来的，为什么没有ipv1,ipv2..

TCP解决的是任意长度的数据包的可靠传输

### TCP的作用
![alt text](images\image05.png)

通常上图最常见的访问Web页面的场景了解TCP发挥了哪些作用
上图其实有3个网络
- 客户所在的客户端的网络Client Network
- 中间是一个广域网（骨干网），可能是光纤、海底电缆等等
- 企业IDC的内部网络

网络请求过程
- 首先在客户端发起了一个http request的get请求
- http request通过蓝色的线，先由数据链路层将数据包发送到路由器上
- 然后根据网络层也就是IP层，选择最短/最优的路径
- TCP层把不定长度的HTTP请求切分成TCP认为合适的segment(段)，然后将这些切分后的包发送到HTTP Server上，在中间的任意节点中，报文都有可能被丢掉，而且路径也可能发生变化，TCP来保证每一个Segment都能到达http Server
- HTTP Server接收到tcp segment后通常TCP层都是由操作系统内核实现的，操作系统上的kernel按照相同的顺序把http request发给web服务（nginx或者tomcat或者其他）处理，web服务器处理完数据后，将响应数据通过相同的路径将其发送给客户端

在整个过程中
- 如何选择跨越不同的网络是由IP层及其之下的数据链路层实现的
- 如何构造一条消息，如何生成一条响应，是由传输层之上的应用成解决的
- 该消息如何可靠的发送，如何保证顺序，都是有TCP层实现的

### TCP的定义
TCP: 面向连接的、可靠的、基于字节流的传输层通信协议
- 面向连接指的是
  - 面向连接的一定是一对一才能连接，TCP不能像UDP那样一个主机同时向多个主机发送同一条消息
- 可靠的指的是
  - 无论网络链路出现了怎样的链路变化，TCP可以保证报文一定能最终到达接收端
- 基于字节流：
  - 表示消息是没有边界的，无论消息有多大，都可以使用TCP传输
  - 其次，基于字节流，它是一个有序的，当前一个字节，接收端没有收到的时候，即使它先收到了后一个字节，也不能扔给应用层去处理

### TCP功能的实现
TCP众多功能的实现是基于层层嵌套的TCP协议的报文头部实现的

### TCP协议的特点
- 点对点（不能广播，多播），面向连接
- 双向传递 (全双工)
- 字节里：打包成报文段，保证有序接受，重复报文自动丢弃
  - 缺点：不维护应用报文的边界
    - 对比http,使用\r\n，content length来自定义维护报文边界
  - 优点：不强制要求应用必须离散的创建数据块，不限制数据块大小
- 流量缓冲：解决速度不匹配问题
- 可靠的传输服务（保证可达，丢包时通过重发进而增加时延实现可靠性）--- 滑动窗口
- 拥塞控制

### TCP协议的任务
- 主机内的进程寻址
- 创建、管理、终止连接
- 处理并将字节(8bit)流打包成报文段(如IP报文)
- 传输数据
- 保持可靠性与传输质量
  - 保证报文不失序，不乱序，不出错，不丢失
- 流控制与拥塞控制
  - 流控制：解决传输两端传输速度不匹配的问题
  - 拥塞控制：面向整个网络，防止网络中出现恶性拥塞

### TCP三次握手
#### 握手目标
- 同步Sequence序列号
  - 初始序列号ISN(Initial Sequence Number)
- 交换TCP通讯参数
  - 如MSS、窗口比例因子，选择性确认，指定校验和算法

#### 三次握手过程中的状态变迁
三次握手设计到的状态
- CLOSED
- LISTEN
- SYN-SENT
- SYN-RECEIVED
- ESTABLISHED

查看三次握手的状态
```shell
netstat -anp | grep tcp

# netstat用法

interval    # 重新显示选定的统计信息，各个显示间暂停的间隔秒数
-a          # 显示所有连接和侦听端口
-n          # 以数字形式(如IP地址)显示地址和端口号
-r          # 显示路由表
-s          # 显示每个协议的统计信息
-b(Windows)/-p(Linux)    # 显示对应的可执行程序名
```
![alt text](images\image06.png)


#### 补充概念TCB
TCB: Transmission Control Block：
保存连接使用的源端口，目的端口，目的IP，序号，应答序号，对方窗口大小，己方窗口大小，tcp状态，tcp输入/输出队列，应用层输出队列，tcp的重传有关变量等

#### 三次握手过程中的性能优化与安全问题
![alt text](images\image07.png)

- 客户端发送SYN网络分组 ---> 到达服务器（服务器内核中有对TCP层的实现）
- 在内核中，将SYN分组插入SYN队列中（tcp_max_syn_backlog）；同时会发出一个SYN/ACK网络分组；此时使用netstat查看连接状态，会发现SYN_RECEIVED的状态
- 当服务器收到客户端返回的ACK网络分组后，连接状态会进入ESTABLISHED状态
  - 同时在内核中
    - 将之前插入的SYN网络分组从SYN队列中移除
    - 将其塞入ACCEPT队列
    - 服务器上的应用程序，比如Nginx在调用accept()时，会从ACCEPT队列中，将刚刚的连接取出，在web服务器中处理使用

整个过程可以发现`SYN队列`和`ACCEPT队列`的长度和负载都是相关的，如果该服务器是一个面向每秒几十万请求的高负载的机器，那么上述两个队列的长度是一定要延长的

#### 关于内核处理TCP协议栈
在Linux内核中，处理TCP协议栈的模块是网络子系统(Networking Subsystem)，具体负责TCP协议栈的部分位于几个内核模块和代码路径中
1. `net/ipv4/`：主要负责IPv4协议及其相关的TCP协议的处理。TCP协议的核心实现文件是`tcp.c`，该文件包含了TCP协议的主要逻辑，包括连接管理，数据传输，拥塞控制等

2. `net/core/`：负责网络核心层，包括通用的socket接口，协议族的注册和网络设备接口的管理等

3. `include/ipv6/`：与`net/ipv4/`类似，但负责IPv6下的TCP协议处理

4. `kernel/`：虽然不是直接负责TCP，但与内存管理，调度等密切相关的部分也影响TCP协议栈的性能和行为

这些模块协同工作来处理TCP协议的各种功能，包括连接的建立与终止、数据包的收发、拥塞控制、流量控制等。


#### 高并发场景下的资源消耗
1. CPU资源消耗
1.1 任务调度和上下文切换
任务调度：
在高并发场景下，CPU需要频繁调度不同的线程或进程，以确保每个请求都能被处理。
调度开销：随着并发量增加，调度频率提升，导致CPU花费更多时间在任务切换上，而非实际计算。

上下文切换：
当CPU在多个任务之间切换时，必须保存当前任务的状态（如寄存器、程序计数器等），并恢复下一个任务的状态。
上下文切换开销：大量上下文切换会导致CPU时间片被大量消耗在保存和恢复任务状态上，而非处理实际业务逻辑。

1.2 锁竞争
同步与锁机制：
多线程环境下，多个线程可能需要访问共享资源，如内存数据结构、文件等，为避免竞态条件，使用锁来同步线程。

锁竞争：高并发场景下，大量线程同时争夺锁资源，会导致线程频繁进入等待状态，进而影响CPU的利用率。

自旋锁：
自旋锁让线程在获取锁前持续轮询，而不是进入睡眠状态，虽然避免了线程调度开销，但在高并发下可能导致CPU资源被大量消耗在无效的自旋操作上。

2. 内存资源消耗
2.1 堆内存消耗
对象创建与销毁：
在高并发场景下，每个请求可能会触发大量的对象创建，如请求参数解析、响应数据封装等。

垃圾回收：频繁的对象创建和销毁会导致垃圾回收器频繁运行，尤其是当对象生命周期短暂时，可能导致内存碎片化和垃圾回收暂停（GC Pause），影响系统性能。

2.2 内存泄漏
未释放资源：
在处理高并发请求时，如果程序未能及时释放不再使用的资源（如未关闭的文件句柄、数据库连接等），这些资源会继续占用内存，导致内存泄漏。

内存占用：随着时间推移，内存泄漏会逐渐占满系统内存，最终导致系统崩溃或性能严重下降。

2.3 内存缓存与分页
缓存机制：
系统或应用程序可能会使用内存缓存来提高访问速度。在高并发场景下，缓存大小可能增长迅速，占用大量内存。

分页：当内存不足时，操作系统会将部分内存页面交换到磁盘上（分页），频繁的分页操作会导致性能下降（因为磁盘IO比内存访问慢得多）。

3. IO资源消耗
3.1 磁盘IO
读写请求：
高并发请求可能涉及大量磁盘读写操作，如日志记录、文件存取、数据库查询等。

磁盘瓶颈：如果磁盘IO无法跟上请求速度，会导致请求阻塞，形成IO瓶颈。磁盘的机械性质（如磁头寻道时间）决定了它的并发处理能力有限。

磁盘竞争：
当多个线程或进程同时访问磁盘时，会出现磁盘竞争，导致IO操作排队和延迟。

3.2 网络IO
网络带宽消耗：
高并发的网络请求会消耗大量带宽，尤其是在处理大数据量传输时。
网络拥塞：带宽有限时，大量并发请求可能导致网络拥塞，增加数据包的丢失率和重传率，从而进一步降低系统性能。

网络延迟：
高并发下，网络设备（如路由器、交换机）处理大量数据包，可能导致延迟增加，影响响应时间。

4. 连接池资源消耗
4.1 连接池的工作机制
连接池简介：
连接池用于管理数据库连接、HTTP连接等资源，通过复用连接来减少连接建立和销毁的开销。
高并发场景下，连接池有助于降低连接创建和销毁的频率，但同时也带来了管理和资源占用问题。

4.2 连接耗尽与资源争用
连接耗尽：
在高并发场景下，如果请求数量超过了连接池的容量，新的请求将被阻塞，直到有空闲连接可用。这种情况下，可能导致连接耗尽，影响系统的处理能力。

连接超时：
高并发可能导致连接池中的连接频繁超时。连接超时会导致资源释放不及时，甚至可能导致连接池内连接资源的枯竭。

4.3 连接池的管理开销
连接的创建与销毁：
尽管连接池能够复用连接，但在高并发下，仍可能因为连接池达到上限而触发频繁的连接创建和销毁操作，增加了系统开销。

锁机制：
连接池内部通常需要使用锁来管理连接资源，保证并发访问的安全性。高并发下，锁的竞争会导致连接获取的延迟。

5. 总结
在高并发场景下，系统中的每种资源都会面临不同程度的消耗和压力：

CPU资源：主要被任务调度、上下文切换、锁竞争所消耗。优化策略包括减少上下文切换、使用无锁数据结构等。

内存资源：主要消耗在对象创建与垃圾回收、内存泄漏、缓存与分页上。可以通过优化内存管理、合理设置缓存大小来缓解。

IO资源：包括磁盘和网络IO的瓶颈，优化手段包括使用缓存、优化文件存储策略、使用更高效的网络协议等。

连接池资源：通过合理设置连接池大小、超时策略、连接复用等，可以有效缓解连接资源的枯竭问题。

了解和优化这些资源的消耗过程，是设计和开发高性能、高可用系统的关键。在实际应用中，常常需要结合具体的业务场景、系统瓶颈、性能测试结果来采取针对性的优化措施。

#### 高并发场景下解决方案
- 应用层connect超时时间调整
```shell
# connect timeout定义
connect timeout 是指应用程序在尝试与服务器建立连接时，等待连接成功的最大时间。如果在这个时间内连接未成功建立，连接操作就会超时并失败

# 调整connect timout的好处
## 快速失败机制
设置较短的 connect timeout 时间，意味着在连接请求未能快速建立时，应用程序能够更快地放弃该请求，并释放相关资源（如线程、连接池等）。这避免了连接长期占用资源，导致系统资源耗尽。

## 降低负载
连接失败的时间缩短时，服务器可以避免处理过多积压的连接请求，从而减少负载，并让系统更快地恢复到稳定状态。

## 改善用户体验
长时间的连接等待会导致用户感知到的延迟增加，这在用户体验上是不可接受的。通过缩短 connect timeout，可以更快地通知用户当前的网络或服务异常，从而提升用户体验。
```



### 通过实验深入了解TCP连接的建立和关闭
### 前置知识补充
#### tcpdump的使用
```shell
# tcpdump的用法
# 捕获指定接口的流量
tcpdump -i eth0

# 捕获指定数量的数据包
tcpdump -c 10

# 用于显示捕获的数据包的十六进制和ASCII格式内容
tcpdump -X
# 这个选项告诉"tcpdump"，在捕获并显示数据包时，不仅要显示数据包的头部信息，还要显示数据包的负载(payload)部分

# 过滤数据包
# 捕获特定协议数据包
tcpdump icmp     # 只捕获ICMP协议的数据包

# 捕获特定主机的数据包
tcpdump host 192.168.0.2  # 捕获来自或发往IP地址192.168.0.2的数据包

# 捕获特定端口的数据包
tcpdump port 80           # 只捕获TCP或UDP端口为80的数据包(常用于HTTP流量分析)

# 捕获来自指定源或目的数据包
tcpdump src port 22
tcpdump dst port 53

# 高级用法
# 显示更详细的包信息：
tcpdump -vv

# 显示链路层头信息：
tcpdump -e      # 显示数据链路层的头信息，如MAC地址

# 组合使用（组合过滤条件）
tcpdump tcp and port 80 and host 192.168.1.1
```

#### nc的使用
```shell
# -l选项(监听模式/listen mode)
# 这个选项使nc进入监听模式，充当一个服务器，等待传入的连接。通常用于创建一个简单的TCP或UDP服务器，监听指定的端口并接收连接请求
# 示例
nc -l 12345   # 监听当前机器的12345端口,等待其他客户端的连接

# -k选项（保持连接/keep-open）
# 这个选项通常与-l结合使用，告诉nc在当前连接关闭后继续保持监听状态，不退出，从而允许新的连接再次连接到同一个端口
# 在不使用"-k"时，nc在接收一个连接并关闭连接后会退出。如果你希望nc能持续接收多个连接而不退出，就需要使用-k选项
nc -l -k 12345
```

#### TCP连接的建立
- 开启抓包
```shell
# 在10.0.0.122主机上
# 抓取TCP端口为9527的数据包
tcpdump -s0 -X -nn "tcp port 9527" -w tcp.pcap --print

# 在10.0.0.122主机上使用nc监听9527端口
nc -k -l 10.0.0.122 9527

# 查看端口状态
[root@worker1 ~]#netstat -anpo|grep Recv-Q;netstat -anpo | grep 9527
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name     Timer
tcp        0      0 10.0.0.122:9527         0.0.0.0:*               LISTEN      152632/nc            关闭 (0.00/0/0)

# 在10.0.0.121上查看服务端和客户端的连接信息
[root@master1 ~]#netstat -anpo|grep Recv-Q;netstat -anpo | grep 9527
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name     Timer
tcp        0      0 10.0.0.121:38114        10.0.0.122:9527         ESTABLISHED 117704/nc            关闭 (0.00/0/0)

# 此时在10.0.0.122上用nc监听的上面可以看到3次握手的信息
13:38:34.306735 IP 10.0.0.121.38114 > 10.0.0.122.9527: Flags [S], seq 1420407109, win 64240, options [mss 1460,sackOK,TS val 2998988679 ecr 0,nop,wscale 7], length 0
	0x0000:  4500 003c 4f91 4000 4006 d638 0a00 0079  E..<O.@.@..8...y
	0x0010:  0a00 007a 94e2 2537 54a9 b145 0000 0000  ...z..%7T..E....
	0x0020:  a002 faf0 d5cb 0000 0204 05b4 0402 080a  ................
	0x0030:  b2c0 ef87 0000 0000 0103 0307            ............
13:38:34.306792 IP 10.0.0.122.9527 > 10.0.0.121.38114: Flags [S.], seq 2715825113, ack 1420407110, win 65160, options [mss 1460,sackOK,TS val 2679345721 ecr 2998988679,nop,wscale 7], length 0
	0x0000:  4500 003c 0000 4000 4006 25ca 0a00 007a  E..<..@.@.%....z
	0x0010:  0a00 0079 2537 94e2 a1e0 33d9 54a9 b146  ...y%7....3.T..F
	0x0020:  a012 fe88 1521 0000 0204 05b4 0402 080a  .....!..........
	0x0030:  9fb3 9239 b2c0 ef87 0103 0307            ...9........
13:38:34.307293 IP 10.0.0.121.38114 > 10.0.0.122.9527: Flags [.], ack 1, win 502, options [nop,nop,TS val 2998988680 ecr 2679345721], length 0
	0x0000:  4500 0034 4f92 4000 4006 d63f 0a00 0079  E..4O.@.@..?...y
	0x0010:  0a00 007a 94e2 2537 54a9 b146 a1e0 33da  ...z..%7T..F..3.
	0x0020:  8010 01f6 f5d9 0000 0101 080a b2c0 ef88  ................
	0x0030:  9fb3 9239                                ...9
```

使用wireshark打开tcp.pcap文件
![alt text](images\image01.png)

#### TCP协议头解析
![alt text](images\image02.png)

该报文的TCP协会头共40字节
- TCP头：20字节
- Options(20 bytes): 20字节

这个握手包恰好覆盖了 1、2、3、4、8 这五种常见的 options 字段：
![alt text](images\image03.png)

- Kind: Maximum Segment Size(2)
  - 告知对端自己的MSS值
  - MSS 选项传输的头是 MTU 减去 IP 基本头（v4 20 字节，v6 40 字节）和 TCP 基本头（20字节）的值，不考虑扩展头。从 ifconfig 的输出能看出来 MTU 是 1500，减去 IPv4 的 20 字节和 TCP 的 20 字节恰好是 1460 字节。
```shell
eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
      inet 10.0.0.122  netmask 255.255.255.0  broadcast 10.0.0.255
      inet6 fe80::250:56ff:fe21:355c  prefixlen 64  scopeid 0x20<link>
      ether 00:50:56:21:35:5c  txqueuelen 1000  (以太网)
      RX packets 105591  bytes 39257832 (39.2 MB)
      RX errors 0  dropped 0  overruns 0  frame 0
      TX packets 72146  bytes 6799365 (6.7 MB)
      TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

# MTU知识点补充
# MTU定义
# MTU是网络接口能处理的最大数据包大小，包含数据和协议头部
# 例如：MTU为1500表示这个网络接口能够处理的最大数据包大小为1500字节

# MTU作用：
# 数据包作用：如果一个数据包大于MTU，它会被分段(或分片)成多个较小的数据包进行传输。这种分段在传输过程中可能会增加延迟和降低效率
# 避免数据包分片：通过设置合理的MTU值，可以尽量避免数据包在传输过程中被分片
# 分片的数据包在传输时，如果其中一个片段丢失，整个数据包必须重新传输，这会增加网络负载
```

- Kind: SACK Permitted(4)
  - 该选项在后续“TCP 发送和接收数据”的讲解中在详细叙述
  - Linux 内核参数 net.ipv4.tcp_sack 可以控制这个特性的开关。
  ```shell
  $ sysctl net.ipv4.tcp_sack
  net.ipv4.tcp_sack = 1
  ```

- Kind: Time Stamp Option(8)
  - 时间戳标记
  - 内核使用 TCP 时间戳来更好地估算 TCP 连接中的往返时间 (RTTM, Round Trip Time Measurement)，可以更准确的进行 TCP 窗口和缓冲区的计算。
  - 这个特性的开关可以用内核参数 net.ipv4.tcp_timestamps 控制
  ```shell
  $ sysctl net.ipv4.tcp_timestamps
  net.ipv4.tcp_timestamps = 1 
  # 0 表示TCP 时间戳被禁用，1 表示 TCP 时间戳被启用（默认），2 表示 TCP 时间戳被启用，但没有随机偏移
  ```

- Kind：No-Operation(1)
  - NOP 一般是占位对齐用的。因为 TCP 包头 offset 字段的值是以 4 字节为单位的，所以 TCP 头的大小也只能是 4 的倍数。

- Kind: Window scale(3)
  - 表示 TCP 窗口缩放
  - TCP 协议头的窗口大小由 2 字节表示（最大 65535 字节），但实际上内核默认的 TCP 缓存区大小比这个值大很多：


### TCP头部解析
#### MSS

#### 重传与确认


### TCP三次握手
#### 握手的目标
- 同步Sequence序列号
  - 初始序列号ISN(Initial Sequence Number)

- 交换TCP通讯参数
  - 如MSS、窗口比例因子，选择性确认，指定校验和算法 

#### 三次握手详解
![alt text](images/image08.png)

SYN: synchronous 同步
ACK: acknowledge 确认

- Client首先把它的SYN发给Server
- Server如果确认收到了SYN，需要回一个ACK（确认号码是所确认序列号+1），那么Client就知道Server收到了自己发的SYN
- 同时Server也要发一个自己的SYN，给到Client，让Client知道我的序列号在哪里
- Client收到序列号也要回个ACK

抓包查看三次握手
```shell
tcpdump -i lo port 80 -c 3 -S 
# -S, 使用绝对序列号替换相对序列号
```

三次握手报文
![alt text](images/image09.png)

![alt text](images/image10.png)

![alt text](images/image11.png)

当执行三次握手的时候，客户端和服务器关于这条连接的状态都在发生变迁的，而我们了解了TCP连接的状态如何变迁的，我们才能更有效的定位一些困难的网路问题


#### 三次握手中TCP连接的状态变迁
当我们的服务器同时处理成千上万的TCP连接时，理解三次握手中的状态变迁，对我们定位复杂的网络问题是有帮助的

三次握手过程中涉及的5种状态
- CLOSE
- LISTEN
- SYN-SENT
- SYN-RECEIVED
- ESTABLISHED

![alt text](images/image12.png)

- 最初大家都是CLOSE状态，由于Server需要监听80或者443端口，因此Server处于Listen状态
- 在三次握手完成之后，大家都要处于ESTABLISHED的状态
- 状态的查看方法
```shell
# Linux上查看
netstat -anp | grep tcp;

# Windows查看
netstat - anop
```

- 首先我们的客户端发送一个SYN包，此时客户端会立即变为SYN-SENT状态（正常情况下，我们在netstat中很难看到SYN-SENT的状态，因为当我们收到了SYN+ACK包以后，就会从SYN-SENT进入到ESTABLISHED状态，这个过程非常短，大概只有几百ms）
- 当服务器收到SYN包后，就会从LISTEN状态进入SYN-RECEIVED状态，这个状态会延续到CLIENT返回ACK包，当Server端收到来自Client的ACK包，就会从SYN-RECEIVED状态进入ESTABLISHED

- TCB：Transmission Controller Block，保持连接使用的源端口，目的端口，目的ip，序号，应答序号，对方窗口大小，己方窗口大小，tcp状态，tcp输入/输出队列，应用层输出队列，tcp重传有关变量等


#### 服务器三次握手流程示例
![alt text](images/image13.png)

首先以服务器为例，看一下三次握手的流程
- 客户端发来SYN分组，到达了服务器，首先在服务器内核中（也就是我们对TCP层的实现），会把这个SYN分组插入到SYN队列中，同时会发送一个SYN+ACK数据分组给客户端，此时服务端处于SYN-RECEIVED状态
- 当客户端返回ACK分组后，服务端会进入ESTABLISHED状态
- 但实际上，在我们的kernel中，会把之前放入SYN队列中的SYN分组移出，将其放入ACCEPT队列，这样当我们的应用程序，比如Nginx，在调用accept方法时，就会从ACCEPT队列中，将刚刚的连接拿出来，交给相关的方法去使用
- 在上述过程中可以看到，在SYN队列长度和ACCEPT队列长度，跟我们的负载都是相关的，如果我们这是一个面向每秒服务几十万请求的高负载机器时，我们的SYN队列是一定要延长的，ACCEPT队列也是要延长的


#### 超时时间与缓冲队列
客户端调整
- 应用层Connect超时时间调整：当然我们的客户端也可以调整，比如客户端调用connect方法时，通常会有一个connect_timeout,可以设置超时时间

服务器端设置(操作系统内核限制调整)
- 服务端SYN_RCV状态
  - net.ipv4.tcp.max.syn.backlog: SYN_RCVD状态链接的最大个数（即调整SYN队列大小）
  - net.ipv4.tcp_synack_retries：被动建立连接时，发SYN/ACK的重试次数(当我们发送了SYN/ACK后，一定时间内，没有收到ACK后的重试次数)

- 客户端SYN_SENT状态
  - 客户端在处于SYN_SENT状态时，如果对linux这样一个负载均衡，或者需要向其他服务进行大量的TCP连接时，可以更改下述内核参数
  - net.ipv4.tcp_syn_retries=6 主动建立连接时，发SYN的重试次数
  - net.ipv4.ip_local_port_rang=32768 60999 建立连接时的本地端口可用范围

- ACCEPT队列设置
  - 设置ACCEPT队列长度(backlog)


#### Fast Open 降低时延
前提概念补充：
在TCP连接中，RTT（Round-Trip Time，往返时间）是指从发送一个数据包到接收到该数据包的确认所需的时间。具体来说，RTT包括以下几个阶段：

- 发送数据包：客户端发送一个数据包到服务器。
- 服务器处理：服务器接收到数据包并处理，然后发送一个确认包（ACK）回客户端。
- 接收确认包：客户端接收到服务器发送的确认包。

RTT时间是这整个过程所花费的时间。RTT是网络性能的重要指标之一，它反映了网络的延迟情况。较低的RTT表示网络延迟较小，数据传输速度较快；较高的RTT则表示网络延迟较大，数据传输速度较慢。

- TCP Fast Open (net.ipv4.tcp_fastopen)

![alt text](images/image14.png)

在我们发送一个http-get请求时，我们需要三次握手，收到一个SYN，还会收到一个ACK，在ACK中携带一个GET请求，这里一共用了两个RTT的时间（在TCP连接中，RTT,round-trip-Time,往返时间，是指从发送一个数据包到接受到该数据包的确认所需的时间），TCP提供了FAST OPen的功能，它所做的功能是
- 第一次建立连接的时候，Server端会生成一个Cookie，在发送SYN+ACK的时候，将Cookie发送给Client，Client会缓存这个Cookie，所以第一个发起连接的时候，还是需要2个RTT时间
- 但在第二次发送请求的时候，我们就跳过了3次握手，因为我们的Cookie中维护了我们上次建立连接的时候的相关信息，比如窗口，timestamp，TCP各种options等，我们直接把Cookie在SYN的时候发给Sever，同时将HTTP-GET请求也发给Server，此时只需要一个RTT时间就可以，相当与消除了三次握手的RTT的时延

#### Linux上打开TCP Fast Open
- `net.ipv4.tcp_fastopen`: 系统开启TFO功能
  - 0：关闭
  - 1：作为客户端时可以使用TFO
  - 2：作为服务器时可以使用TFO
  - 3：无论作为客户端还是服务端都可以使用TFO


注意：某些网络设备或防火墙可能不支持TFO，此外，在某些情况下，TFO可能会引入安全隐患，因此需要根据具体情况谨慎使用


#### 如何应对SYN攻击

攻击者短时间伪造不同IP地址的SYN报文，快速占满backlog队列，使服务器不能为正常用户服务

使用`tcp_syncookies`应对攻击

![alt text](images/image15.png)


#### TCP_DEFER_ACCEPT
正常来说，服务器在收到ACK分组后，会将SYN分组从SYN队列转移到ACCEPT队列，然后应用程序从ACCEPT队列中，获取该分组进行处理
但启动了TCP_DEFER_ACCEPT之后，SYN分组到达ACCEPT后，并不会触发应用程序，而是直到后续data数据包到达后，再进行统一处理，这样效率更高


### 数据传输与MSS分段
TCP是一个面向字节流的协议，它不限制应用层传输消息的长度，但实际上在TCP之下的网络层和链路层由于它们在发送报文时所使用的内存是有限的，所以它一定会限制报文的长度，因此TCP必须把它从应用层那里接收到的任意长度的字节流，切分成许多个报文段，那么切分报文段的依据是什么？又是怎样进行拆分的？

#### 如何基于MSS最大报文段大小，来进行TCP报文段的拆分

![alt text](images/image16.png)

- 流分段的依据
  - MSS: 防止IP层分段
    - 如果TCP不分层，IP层一定会分层，我们要避免IP层分层，因为IP层分层是很低效的
  - 流控：接收端的能力
    - 比如接收端一次只能接受4个字节，因为我的服务器此时可能非常繁忙，内存可能已经不够用了，或者应用程序没有及时将缓存区的内容读取出来，所以需要进行流控，此时也会影响分段大小

MSS: MAX Segment Size
- 定义：仅指TCP承载数据，不包含TCP头部的大小，参见RFC879
- MSS选择目的
  - 尽量每个Segment报文段携带更多的数据，以减少头部空间占用比率
  - 防止Segment被某个设备的IP基于MTU拆分,ip层拆分报文很低效，如果出现报文丢失，所有报文都要重传
- 默认MSS：536字节（默认MTU576字节,20字节IP头部，20字节TCP头部）
- 握手阶段协商MSS
- MSS分类
  - 发送方最大报文段SMSS：Sender Maximum segment size
  - 接收端最大报文段RMSS：Receiver Maximum segment size

![alt text](images/image17.png)
客户端发送MSS,告诉服务端，我使用1460字节

![alt text](images/image18.png)
服务端也会发送MSS，给客户端，表示我的MSS是1328字节

将这些报文段发送给对方的时候，如何确定的对方是否收到
如果没有收到，是否需要重传

### 重传与确认
TCP通过重传与确认来保证每个Segment段一定会到达对方

- 理解：重传与确认演变为滑动窗口的过程
- 演化为滑动窗口后，定义TCP的序号，以及确认序列号

#### 序列号的设计理念
这里介绍序列号的设计理念

![alt text](images/image19.png)

如图，这里发送了4条消息，当第3条消息丢了以后，如何解决

解决方法1：简单的使用定时器来重传
![alt text](images/image20.png)

PAR:  `Positive Acknowledgment with Retransmission`
比如发送第一个消息的时候，启动一个定时器，第一个消息没发送完之前，无法发送第二个消息，直到在超时时间内，客户端接收到服务器发送的确认后，客户端发送第二个消息，发送第二个消息的时候，再启用一个定时器，如果在第二个超时时间内，没有接收到第二次的确认，就重新发送第二个消息，重发的时候，重启定时器，由此完成一个简单的重发确认功能

- 问题：
  - 效率低下：因为第一个消息没发送完之前，是不能发送第二个消息的，必须一个消息一个消息的串行发送

解决方案2：提升并发能力的PAR改进版
![alt text](images/image21.png)

- 接收缓冲区的管理
  - Limit限制发送方

在发送消息的时候，添加#1的标识，返回响应的时候，也发送我响应的是哪个消息的标识ACK #1,从而可以实现并发的发送消息，但是这里有个问题，就是我的服务端的内存和处理能力是有限的，必须限制设备A，不能无限制的发送，所以需要添加一个Limit字段，告诉客户端，现在还有几个缓冲区，如果服务端现在还有2个缓冲区，则客户端只能向服务端发送两个消息，而不能是第三个消息，从而实现流控

解决方案3（真实的TCP实现）:Sequence序列号/Ack序列号
TCP中的序列号是针对每个字节的，而不像上面的那样是仅针对报文的，

- 设计目的：解决应用层字节流的可靠发送
  - 跟踪应用层的发送端数据是否送达
  - 确定接收端有序的接收到字节流

- 序列号的值针对的是字节而不是报文
发送SYN报文段
![alt text](images/image22.png)
确认报文，确认SYN=1897的序列号的报文 
![alt text](images/image23.png)
Windows size value = 1032，后续还可以发送1032字节
![alt text](images/image24.png)

TCP序列号由于是32位，导致如果我们发送2^32，约4G数据后，还在连接状态数据传输，我们就需要复用最初的SYN序列号，此时就会出现问题，该问题就是PAWS

PAWS (Protect Against Wrapped Sequence number)
防止序列号回绕
![alt text](images/image25.png)

比如在A时间点发送0G到1G字节，此时相对序列号就是0G到1G，B时间点发送1G到2G，但是此时B时间点的数据包丢失了，在后续F时间点重传，在后面E时间点，此时已经发生了SYN复用问题，发生了回绕，在F时间点，就需要发送1G到2G的序列号，但是B时间点重发的1G到2G和F时间点发的新的1G到2G同时发送到接收端，此时接收端就无法判断到底哪个才是真实的消息，就会把消息覆盖掉

这个问题通过TCP Timestamp可以解决，我们发生消息的时候，带上时间戳，

TCP Timestamp（TCP timestamp在TCP options中使用）
- 更精准的计算RTO
- PAWS

#### RTO重传定时器的计算
当指定时间内，我们没有收到对方的确认，我们需要重传这个Segment，这个时间究竟是多长

一共有两种重发的场景，在第一种情况，以RTT2为准，在第二种情况，以RTT1为准
![alt text](images/image26.png)

所以没有办法很好的同时照顾两种场景，所以需要采用另一种RTT的测量方法

RTT测量的第2种方法
- 发送时间
  - 数据包中Timestamp选项的回显时间
  ![alt text](images/image27.png)

Timestamp上有发送的时间，也有重发的时间，这样就可以更精准的测量


RTO应该设置多大
- RTO应当略大于RTT，但是由于网络状态和链路时常会变动，因此RTO也需要响应改变，因此RTO应该有平滑性

- 平滑RTO：RFC793，降低瞬时变化
  - 具体计算方法（略）

### 滑动窗口：发送窗口与接收窗口

#### 发生窗口
1. 已发送并收到Ack确认的数据：1-31字节
2. 已发送未收到Ack确认的数据：32-45字节
3. 未发送但总大小在接收方处理范围内：46-51字节
4. 未发送但总大小超出接收方处理范围：52-字节

![alt text](images/image28.png)

#### 可用窗口/发送窗口
- 可用窗口：46-51字节
- 发送窗口：32-51字节
![alt text](images/image29.png)

当46-51字节已发送，此时可用窗口耗尽
![alt text](images/image30.png)

当32-36字节已确认
- 发送窗口移动
![alt text](images/image31.png)

因为我收到了5个字节，这样，发送窗口右移5个字节，此时，新的可用窗口（52-56），此时又可以发送5个字节，这就是所谓的滑动窗口

发送窗口
- SND.WND（Send Window）：发送窗口(20字节)
- SND.UNA：指向已发送未收到ACK的首部
- SND.NXT：指向可用窗口的第一个字节的指针

所以：Usable Window Size = SND.UNA + SND.WND - SND.NXT


#### 接受窗口
通常接收窗口约等于对端发送窗口的接收窗口
- RCV.WND
- RCV.NXT
![alt text](images/image32.png)

### 滑动窗口实例：
前提：MSS不产生影响，窗口不变（实际情况中，发送和接收窗口的大小与我们的缓冲区相关，也与进程读取缓冲区的速度有关）
![alt text](images/image33.png)

- 首先客户端发送一个http-get请求,该请求大小140字节，此时SND.NXT=141
- 服务器接收到该请求后，要返回一个响应，该响应有一个80字节的头部+280字节的body，首先先发送80字节的头部，并且告诉客户端140字节已收到，发送一个ACK报文
  - 此时服务器的RCV.NXT变为141，SND.NXT=241+80=321
- 客户端收到80字节后，接收窗口RCV.NXT=321，同时收到了ACK，因此SND.UNA=141，并向服务端发送ACK(第3步发送的Ack，第6步才到)
- 服务器发送body部分的280字节，但是由于此时可用窗口为241+200-321，只有120字节，因此此时流控发生作用，这里只能发送120字节，SND.NXT变为441
- 客户端收到响应文件的第1部分，此时接收窗口变为321+120=441，同时发送ACK给服务端，
- 服务端此时先收到第3步发送的ack，知晓第一个响应头部的80字节已经被确认了，SDN.UNA=241+80=321(理论上此时服务端的可用窗口应该向后滑动80字节)
- 紧接着收到第四步，也就是第二个响应文件120字节的ACK包，SDN.UNA=321+120=441,又腾出了120字节，此时服务器的可用窗口大小应该为200字节
- 第8步，因为此时可用窗口为200字节，满足剩下的160字节，因此将剩下的全部发送给客户端，此时可用窗口还剩40字节，SND.NXT=441+160=601字节
- 第9步中，接收到文件第2部分，发送ack，RCV.NXT=441+160=601,
- 在第10步，服务器收到了第160字节的响应报文的ACK，此时SND.UNA=441+160,变为601，此时可用窗口再次变为200字

#### 客户端发送消息的角度
![alt text](images/image34.png)

- 客户端发送消息，最初发送窗口是360字节，可用窗口也是360字节，当发送140字节的请求后
- 可用窗口减少为220字节，所以此时的SDN.UNA不变，SDN.NXT指向141
- 当后续收到Server发回来的ACK后，SDN.UNA向后移动140字节，此时可用窗口又变成了360字节

#### 服务器发送消息的角度
![alt text](images/image35.png)

- 最初服务端的发送窗口的可用窗口是200字节，但它的发送窗口也是200字节，然后它发送了一个80字节的header，响应包的头部信息，发送完后，可用窗口变为120字节，
- 紧接着它要发送一个280字节的文件，但是此时它的可用窗口只有120字节，所以它只能把文件拆成两部分，一个120字节，一个160字节，发送完120字节后，它的可用窗口变为0（即窗口关闭）
- 当在 后续收到客户端发过来的80字节的ACK包后，Server此时又空出了80字节，但是此时80字节还是不够，后续等到又收到客户端发过来的120字节的ACK包后，此时可用窗口变为200字节，大于160字节，所以在第8步发送了剩余160字节的数据，此时可用窗口还剩40字节
- 当最后收到客户端发过来的160字节的ACK包后，可用窗口变为了200字节


### 操作系统的缓冲区和滑动窗口之间的关系
在上述示例，假定MSS不产生影响，窗口不变，但是实际上，发送窗口和接收窗口所存放的字节数都是放在操作系统的缓冲区中的

而操作系统的缓冲区会被操作系统调整，当我们的应用进程无法及时读取缓冲区中的内容时，也会对缓冲区造成影响

#### 窗口与缓存
- 应用层没有及时读取缓存
![alt text](images/image36.png)

- 起初，客户端的发送窗口是360字节，它的可用窗口也是360字节；而服务器端的接收窗口也是360字节
- 第一步，客户端向服务端发送140字节给服务器，所以客户端的发送窗口仍然是360字节，但是它的可用窗口变为360-140=220字节，
- 服务器收到140字节后，它的接收窗口却变为了260字节，因为此时应用进程将140字节读取了40字节，还有100字节，进程没有读取，因此我们本身窗口是360字节，但是因为缓存里有100字节没有被读取，被占用了，导致接收窗口变为了360-100=260字节（接收窗口的第一次收缩）
- 服务端的接收窗口收缩后，通过TCP报文的Window字段（此时Window=260），带给客户端，客户端此时就会把发送窗口也从360减为260
- 第四步，客户端向服务端发送180字节，因为180字节小于当前可用窗口260字节，所以发送了180字节后，可用窗口变为了260-180=80字节，但是此时发送窗口仍然是260字节
- 第五步，服务端收到180字节后，我们假定我们的应用进程仍然没有读取新来的180字节，所以我们的接收窗口又收缩为260-180=80字节
- 服务端在向客户端发送ACK报文的时候，又会携带Window=80字段
- 客户端收到后，发送窗口变为80字节
- 第七步，客户端向服务端发送80字节后，可用窗口为0
- 服务器收到客户端发送的80字节后，本身的接收窗口收缩为0，因为应用进程仍然没有读取这个数据，而是占用了缓存，
- 此时服务端在向客户端发送ACK包时，携带window=0，告诉客户端
- 客户端的可用窗口收缩为0

#### 收缩窗口导致丢包
- 先收缩窗口，再减少缓存
- 窗口关闭后，定时探测窗口大小

#### Linux中对TCP缓冲区的调整方式
- net.ipv4.tcp_rmem=4096 87380 6291456
  - 读缓存最小值，默认值，最大值，单位字节，覆盖net.core.remem_max
- net.ipv4.tcp_wmem=4096 16384 4194304
  - 写缓存最小值，默认值，最大值，单位字节，覆盖net.core.wmem_max
- net.ipv4.tcp_mem=1541646 2055528 3083292
  - 系统无内存压力，启动压力模式阈值，最大值，单位为页的数量
- net.ipv4.tcp_moderate_rcvbuf=1
  - 开启自动调整缓存模式

在linux中，当我们的操作系统现在压力非常大， 比如说从100个进程变到了1000个进程时，此时内存非常紧张，此时我们可以将缓存像小调一些

当我们进程少以后，可以在调大一些

### 如何减少小报文提高网络效率
当我们的TCP segment报文段中承载的信息数据非常少的时候，例如只有几个字节，整个网络的效率是很低的，因为每个TCP segment中都会有固定的20个字节的TCP头部，也会有固定的20个字节的IP头部，这样在整个TCP segment中，有效信息所占的比重就会非常的低。我们应该尽量在合理范围内，避免大量传输小报文

#### SWS(Silly Window syndrome)糊涂窗口综合征
场景：Server当前非常繁忙
![alt text](images/image37.png)

- 初始客户端的发送窗口和可用窗口都是360字节，而Server的接收窗口也是360字节
- 客户端向服务端发送一个大报文，直接用光360字节，而此时Server端由于程序进程繁忙，可能只能处理120字节的数据，而还有240字节会缓存在缓冲区，因此此时我们的发送窗口从360字节降低为120字节，我们把120字节的窗口通过ack传递给客户端
- 此时客户端的发送窗口变为120字节，虽然客户端有大量数据要发送，但此时它只能发送120字节了
- 这120字节发送给server后，Server本来进程处理的速度已经很慢了，此时Server只能再次把40字节传递给应用进程，此时接收窗口就从120字节降低为80字节，并通过ACK包传递给客户端
- 客户端发现可用窗口变为80字节，于是，只能继续发送80字节数据给Server
- Server此时可能只处理了13个字节，Server只能将可用窗口降低为67字节，有告诉了客户端
- 以此往复，服务端的接收窗口会越来越少，整个网络中的效率会越来越低

实际上更好的解决方法是：应该等等Server让它有时间把缓冲区中的数据处理掉，实际上就不会出现反复传输效率低下的TCP Segment


#### SWS避免算法
- 接收端
  - David D Clark算法：窗口边界移动值小于min(MSS,缓存/2)时，通知窗口为0

- 发送方：
  - Nagle算法：TCP_NODELAY用于关闭Nagle算法
    - 没有已发送未确认报文段时，立刻发送数据
    - 存在未确认报文段时，直到：1-没有已发送未确认报文段，或2-数据长度达到MSS时再发送

#### TCP delayed acknowledgment 延迟确认
实时上，当没有携带数据的ACK它的网络效率也很低，因此衍生出一个TCP delayed acknowledgment的延迟确认

- 当有响应数据要发送时，ack会随着响应数据立即发送给对方
- 如果没有响应数据，ack的发送将会有一个延迟，以等待看是否有响应数据可以一起发送
- 如果在等待发送ack期间，对方的第二个数据段又到达了，这时要立即发送ack

延迟时间算法(HZ大小和时钟频率有关，每个操作系统可能都不一样)
```C
#define TCP_DELACK_MAX ((unsigned)(HZ/5))
#if HZ >= 100
#define TCP_DELACK_MIN ((unsigned)(HZ/25))
#define TCP_ATO_MIN ((unsigned(HZ/25)))
#else
#define TCP_DELACK_MIN 4U
#define TCP_ATO_MIN 4U
#endif 
```

HZ的查询方法
```shell
cat /boot/config-`uname -r` | grep '^CONFIG_HZ='
```

![alt text](images/image38.png)

Nagle算法和 Delay之间会出现问题，因为我们一开始发送一个小数据包，客户端等不到ACK包就不会发送下一个包，而服务端等不到对方第二个数据段，就会有会延迟产生

如何解决Nagle算法下，一定会有一个小报文在网络中发送，后续的报文都在等这个小报文的ACK，才能继续发送，而延迟确认又会导致ACK Delay一定会发生

解决方案：
- 关闭delayed ACK: TCP_QUICKACK
- 关闭Nagle：TCP_NODELAY`（关闭发送端）

#### 扩展：零拷贝技术
我们本身在linux上需要把一个磁盘中的文件，通过TCP发给客户端，我们需要先把这个文件读取到用户态的内存中，在从用户态的内存中，发给linux Kernel的缓冲区，经过发送窗口，再发送给客户端

但是有了sendfile，可以直接由内核把磁盘中的文件读入到TCP发送缓冲区中直接发送，这样就减少了两次拷贝

### 拥塞控制
由于TCP协议向应用层提供不定长的字节流发送方法，使得TCP协议先天性的就有意愿去占满网络中的每个带宽，但是当网络中许多TCP来来连接同时试图去占满整个带宽时，就有可能出现恶行拥塞事件，所以TCP的拥塞算法是十分必要的
